<!DOCTYPE html>
<html lang="en-us">

<!--
It's bad to import d3 in every post separately (https://groups.google.com/forum/#!topic/d3-js/bwdNirt2uEU).
Importing it globally here.
Putting this up at the top, according to this controversial stack overflow answer
http://stackoverflow.com/questions/7169370/d3-js-and-document-onready
 -->
<script src="https://d3js.org/d3.v5.min.js"></script>

<link rel="stylesheet" href="/public/font-awesome-4.4.0/css/font-awesome.min.css">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
    <meta name="description" content="My name is Chris Said and I am a data scientist at Stitch Fix. This blog is mostly about tech, stats, and science.">
  

  <!-- To get a link preview image, just set the image attribute in your _post -->
  

  <!-- twitter standard -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@chris_said" />
  <meta name="twitter:title" content="Home" />
  <meta name="twitter:description" content="" />


  <title>
    
      The File Drawer &middot; A blog by Chris Said
    
  </title>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-67746868-1', 'auto');
    ga('send', 'pageview');
  </script>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

<!--   <body class="theme-base-cps">
 -->
  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          The File Drawer
        </a>
      </h1>
      <p class="lead">My name is Chris Said and I am a data scientist at Stitch Fix. This blog is mostly about tech, stats, and science.</p>

    </div>

    <nav class="sidebar-nav">

      
        <a class="sidebar-nav-item active" href="/">Home</a>
      
        <a class="sidebar-nav-item" href="/archive">Archive</a>
      
        <a class="sidebar-nav-item" href="/atom.xml">Feed</a>
      

    </nav>

    <div class="wrapper">
      <div class="inner">
        <a href = "http://www.twitter.com/Chris_Said" class="contact-button"><i class="fa fa-twitter fa-2x"></i></a>
        <a href = "https://www.linkedin.com/pub/chris-said/6b/86b/979" class="contact-button"><i class="fa fa-linkedin-square fa-2x"></i></a>
        <a href = "mailto:chris.said@gmail.com" class="contact-button"><i class="fa fa-envelope fa-2x"></i></a>
      </div>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/02/08/the-shower-problem/">
        The shower problem
      </a>
    </h1>

    <span class="post-date">08 Feb 2020</span>

    <p><em>Attention mathematicians and computer scientists:</em> I’ve got a problem for you, and I don’t know the solution.</p>

<p>Here’s the setup: You’re at your friend’s place and you need to take a shower. The shower knob is unlabeled. One direction is hot and the other direction is cold, and you don’t know which is which.</p>

<div class="wrapper">
  <img src="/assets/2020_shower_problem/shower_knob.png" class="inner" style="position:relative border: #222 2px solid; max-width:40%;" />
</div>

<p>You turn it to the left. It’s cold. You wait.</p>

<p>At what point do you switch over to the right?</p>

<h3 id="the-baseline-shower-problem">The baseline shower problem</h3>
<p>Let’s make this more explicit.</p>

<ul>
  <li>Your goal is to find a policy that minimizes the expected amount of time it takes to get hot water flowing out of the shower head. To simplify things, assume that the water coming out of the head is either hot or cold, and that the lukewarm transition time is effectively zero.</li>
  <li>You know that the shower has a Time-To-Hot constant called <script type="math/tex">\tau</script>. This value is defined as the time it takes for hot water to arrive, assuming you have turned the knob to the hot direction and keep it there.</li>
  <li>The <script type="math/tex">\tau</script> constant is a fixed property of the shower and is sampled once from a known distribution. You have certain knowledge of the distribution, but you don’t know <script type="math/tex">\tau</script>.</li>
  <li>The shower is memoryless, such that every time you turn the knob to the hot direction, it will take <script type="math/tex">\tau</script> seconds until the hot water arrives, regardless of your prior actions. Every time you turn it to the cold direction, only cold water will come out.</li>
</ul>

<div class="wrapper">
  <img src="/assets/2020_shower_problem/distributions.gif" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 1.</strong> Unbeknownst to the user, the hot water direction is to the right, with a time constant τ of 60 seconds. The user knows the probability distribution over τ, and follows a strategy of eliminating segments of that distribution. They initially guess the incorrect left direction. They spend 40 seconds on that direction and eliminate that segment of the distribution. They then switch to the right direction, waiting 60 seconds until the hot water arrives.
  </div>
</div>
<p><br /></p>

<p>I don’t know how to solve this problem. But as a starting point I realize it’s possible to keep track of the probability that the hot direction is to the left or to the right. In the animation above, the probability that the hot direction is to the right is just the unexplored white area under the right curve, divided by the total unexplored white area of both curves.</p>

<p>But how do you turn that into a policy for exploring the space? Does anybody know?</p>

<h3 id="bonus-problem-plumbing-realities-and-the-elusive-middle-solution">Bonus problem: Plumbing realities and the elusive “Middle Solution”</h3>
<p>The baseline shower problem assumes a simplified version of reality, where the shower is memoryless and there is only a single pipe. If you want a harder problem, I have written a comment below that describes some of the plumbing realities, including lag and the existence of separate hot and cold pipes. The comment explores the tantalizing possibility that we’ve all been fiddling with our showers completely wrong this whole time. Instead of swinging the knob between one extreme and the other, what if the optimal solution is to start by putting the knob in the middle? To read more, see the comment below.</p>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2020/02/08/the-shower-problem/#disqus_thread" data-disqus-identifier="http://localhost:4000/2020/02/08/the-shower-problem/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2020/02/08/the-shower-problem/"
              data-via="Chris_Said"
              data-count="none"
              data-text="The shower problem">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/">
        Optimizing sample sizes in A/B testing, Part III&#58; Aggregate time-discounted expected lift
      </a>
    </h1>

    <span class="post-date">10 Jan 2020</span>

    <div class="caption">
This is Part III of a three part blog post on how to optimize your sample size in A/B testing. Make sure to read <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/">Part I </a> and <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/">Part II</a> if you haven't already.
</div>
<hr />

<p>In <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/">Part II</a>, we learned how before the experiment starts we can estimate $\hat{L}$, the expected post-experiment lift, a probability weighted average of outcomes.</p>

<p>In Part III, we’ll discuss how to estimate what is perhaps the most important per-unit cost of experimentation: the forfeited benefits that are lost by delayed shipment. This leads to something I think is incredibly cool: A formula for the <em>aggregate time-discounted expected post-experiment lift</em> as a function of sample size. We call this quantity $\hat{L}_a$. The formula for $\hat{L}_a$ allows you to pick optimal sample sizes specific to your business circumstances. We’ll cover two examples in Python, one where you are testing a continuous variable, and one where you are testing a binary variable (as in conversion rate experiments).</p>

<p>As usual, the focus will be on choosing a sample size at the beginning of the experiment and committing to it, not on dynamically updating the sample size as the experiment proceeds.</p>

<h2 id="a-quick-modification-from-part-ii">A quick modification from Part II</h2>

<p>In <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/">Part II</a>, we saw that if you ship whichever version (A or B) does best in the experiment, your business will on average experience a post-experiment per-user lift of</p>

<script type="math/tex; mode=display">\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{n})}}</script>

<p>where $\sigma_\Delta^2$ is the variance on your normally distributed zero-mean prior for $\mu_B - \mu_A$, $\sigma_X^2$ is the within-group variance, and $n$ is the per-bucket sample size.</p>

<p>Because Part III is primarily concerned with the duration of the experiment, we’re going to modify the formula to be time-dependent. As a simplifying assumption we’re going to make <em>sessions</em>, rather then <em>users</em>, the unit of analysis. We’ll also assume that you have a constant number of sessions per day. This changes the definition of $\hat{L}$ to a <em>post-experiment per-session lift</em>, and the formula becomes</p>

<script type="math/tex; mode=display">\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{m\tau})}}</script>

<p>where $m$ is the sessions per day for each bucket, and $\tau$ is duration of the experiment in days.</p>

<h2 id="time-discounting">Time Discounting</h2>

<p>The formula above shows that larger sample sizes result in higher $ \hat{L} $, since larger samples make it more likely you will ship the better version. But as with all things in life, there are costs to increasing your sample size. In particular, the larger your sample size, the longer you have to wait to ship the winning bucket. This is bad because lift today is much more valuable than the same lift a year from now.</p>

<p>How much more valuable is lift today versus lift a year from now? A common way to quantify this is with exponential discounting, such that weights (or “discount factors”) on future lift follow the form:</p>

<script type="math/tex; mode=display">w = e^{-rt}</script>

<p>where $ r $ is a discount rate. For startup teams, the annual discount rate might be quite large, like 0.5 or even 1.0, which would correspond to a daily discount rate $r$ of 0.5/365 or 1.0/365, respectively. Figure 1 shows an example of a discount rate of 1.0/365</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/discount_function.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 1</strong>.
  </div>
</div>
<p><br /></p>

<h2 id="aggregate-time-discounted-expected-lift-visual-intuition">Aggregate time-discounted expected lift: Visual Intuition</h2>

<p>Take a look at Figure 2, below. It shows an experiment that is planned to run for $\tau = 60$ days. The top panel shows $\hat{L}$, which we have now defined as the expected per-session lift. While the experiment is running, $\hat{L} = 0$, since our prior is that $\Delta$ is sampled from a normal distribution with mean zero. But once the experiment finishes and we launch the winning bucket, we should begin to reap our expected per-session lift.</p>

<p>The middle panel shows our discount function.</p>

<p>The bottom panel shows our time-discounted lift, defined as the product of the lift in the top panel and the time discount in the middle panel. (We can also multiply it by $M$, the number of post-experiment sessions per day, which for simplicity we set to 1 here.) The aggregate time-discounted expected lift, $\hat{L}_a$, is the area under the curve.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/discounted_lift_static.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 2</strong>.
  </div>
</div>
<p><br /></p>

<p>Now let’s see what happens with different experiment durations. Figure 3 shows that the longer you plan to run your experiment, the higher $\hat{L}$ will be (top panel). But due to time discounting, (middle panel), the area under the time-discounted lift curve (bottom panel) is low for overly large sample sizes. There is an optimal duration of the experiment (in this case, $\tau = 24$ days), that maximizes $\hat{L}_a$, the area under the curve.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/discounted_lift_dynamic.gif" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 3</strong>.
  </div>
</div>
<p><br /></p>

<h2 id="aggregate-time-discounted-expected-lift-formula">Aggregate time-discounted expected lift: Formula</h2>
<p>The aggregate time-discounted expected lift $\hat{L}_a$, i.e. the area under the curve in the bottom panel of Figure 3, is:</p>

<script type="math/tex; mode=display">\hat{L}_a = \frac{\sigma_\Delta^2 M e^{-r\tau}}{r \sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{m\tau})}}</script>

<p>where $ \tau $ is the duration of the experiment and $M$ is the number of post-experiment sessions per day. See the Appendix for a derivation.</p>

<p>There’s two things to note about this formula.</p>
<ol>
  <li>Increasing the number of bucketed sessions per day, $m$, always increases $\hat{L}_a$.</li>
  <li>Increasing the duration of the experiment, $\tau$, may or may not help. Its impact is controlled by competing forces in the numerator and denominator. In the numerator, higher $\tau$ decreases $\hat{L}_a$ by delaying shipment. In the denominator, higher $\tau$ increases $\hat{L}_a$ by making it more likely you will ship the superior version.</li>
</ol>

<h2 id="optimizing-sample-size">Optimizing sample size</h2>

<p>At long last, we can answer the question, “How long should we run this experiment?”. A nice way to do it is to plot $\hat{L}_a$ as a function of $\tau$. Below we see what this looks like for one set of parameters. Here the optimal duration is 38 days.</p>
<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/time_aggregated_lift_by_tau.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 4</strong>.
  </div>
</div>
<p><br />
Note also that a set of simulated experiment and post-experiment periods (in blue) confirm the predictions of the closed form solution (in gray). See the <a href="https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb">notebook</a> for details.</p>

<h2 id="examples-in-python">Examples in Python</h2>
<h3 id="example-1-continuous-variable-metric">Example 1: Continuous variable metric</h3>
<p>Let’s say you want to run an experiment comparing two different versions of a website, and your main metric is revenue per session. You know in advance that the within-group variance of this metric is $\sigma_X^2 = 100$. You don’t know which version is better but you have a prior that the true difference in means is normally distributed with variance $\sigma_\Delta^2 = 1$. You have 200 sessions per day and plan to bucket 100 sessions into Version A and 100 sessions into Version B, running the experiment for $\tau=20$ days. Your discount rate is fairly aggressive at 1.0 annually, or $r = 1/365$ per day. Using the function in the <a href="https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb">notebook</a>, you can find $\hat{L}_a$ with this command:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">get_agg_lift_via_closed_form</span><span class="p">(</span><span class="n">var_D</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">var_X</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">365</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="c1"># returns 26298
</span></code></pre></div></div>

<p>You can also use the <code class="language-plaintext highlighter-rouge">find_optimal_tau</code> function to determine the optimal duration, which in this case is $\tau=18$.</p>

<h3 id="example-2-conversion-rates">Example 2: Conversion rates</h3>
<p>Let’s say your main metric is conversion rate. You think that on average conversion rates will be about 10%, and that the difference in conversion rates between buckets will be normally distributed with variance 1%. Using the normal approximation of the binomial distribution, you can use <code class="language-plaintext highlighter-rouge">p*(1-p)</code> for <code class="language-plaintext highlighter-rouge">var_X</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">get_agg_lift_via_closed_form</span><span class="p">(</span><span class="n">var_D</span><span class="o">=</span><span class="mf">0.01</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">var_X</span><span class="o">=</span><span class="n">p</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">),</span> <span class="n">m</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">365</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="c1"># returns 207
</span></code></pre></div></div>

<p>You can also use the <code class="language-plaintext highlighter-rouge">find_optimal_tau</code> function to determine the optimal duration, which in this case is $\tau=49$.</p>

<h2 id="faq">FAQ</h2>
<p><strong>Q:</strong> Has there been any similar work on this?</p>

<p><strong>A:</strong> As I was writing this, I came across a <a href="https://arxiv.org/pdf/1811.00457.pdf">fantastic in-press paper</a> by <a href="https://drexel.edu/now/experts/Overview/Feit-Elea/">Elea Feit</a> and <a href="https://www.ron-berman.com/">Ron Berman</a>. The paper is exceptionally clear and I would recommend reading it. Like this blog post, Feit and Berman argue that it doesn’t make any sense to pick sample sizes based on statistical significance and power thresholds. Instead they recommend profit-maximizing sample sizes. They independently come to the same formula for $ \hat{L} $ as I do (see right addend in their Equation 9, making sure to substitute my $\frac{\sigma_\Delta^2}{2}$ for their $\sigma^2)$. Where they differ is that they assume there is a fixed pool of $N$ users that can only experience the product once. In their setup, you can allocate $n_1$ users to Bucket A and $n_2$ users to Bucket B. Once you have identified the winning bucket, you ship that version to the remaining $N-n_1-n_2$ users. Your expected profit is determined by the total expected lift from those users. My experience in industry differs from this setup. In my experience there is no constraint that you can only show the product once to a fixed set of users. Instead, there is often an indefinitely increasing pool of new users, and once you ship the winning bucket you can ship it to everyone, including users who already participated in the experiment. To me, the main constraint in industry is therefore time discounting, rather than a finite pool of users.</p>

<p><strong>Q:</strong> In addition to the lift from shipping a winning bucket, doesn’t experimentation also help inform us about the types of products that might work in the future? And if so, doesn’t this mean we should run experiments longer than recommended by your formula for $\hat{L}_a$?</p>

<p><strong>A:</strong> Yes, experimentation can teach lessons that are generalizable beyond the particular product being tested. This is an advantage of high powered experimentation not included in my framework.</p>

<p><strong>Q:</strong> What about <a href="/2016/02/28/four-pitfalls-of-hill-climbing/">novelty effects</a>?</p>

<p><strong>A:</strong> Yup, that’s a real concern not covered by my framework. You probably want to know a somewhat long term impact of your product, which means you should probably run the experiment for longer than recommended by my framework.</p>

<p><strong>Q:</strong> If some users can show up in multiple sessions, doesn’t bucketing by session violate independence assumptions?</p>

<p><strong>A:</strong> Yeah, so this is tricky. For many companies, there is a distribution of user activity, where some users come for many sessions per week and other users come for only one session at most. Modeling this would make the framework significantly more complicated, so I tried to simplify things by making sessions the unit of analysis.</p>

<p><strong>Q:</strong> Is there anything else on your blog vaguely related to this topic?</p>

<p><strong>A:</strong> I’m glad you asked!</p>
<ul>
  <li><a href="/2016/02/28/four-pitfalls-of-hill-climbing/">Four pitfalls of hill climbing</a> discusses some product-focused issues in A/B testing</li>
  <li><a href="/2018/02/04/hyperbolic-discounting/">Hyperbolic discounting — The irrational behavior that might be rational after all</a> is about time discounting, although not in the context of experimentation.</li>
</ul>

<h2 id="appendix">Appendix</h2>
<p>The aggregate time-discounted expected lift $\hat{L}_a$ is</p>

<script type="math/tex; mode=display">\hat{L}_a = \int_{\tau}^{\infty} \hat{L} M e^{-rt} \,dt</script>

<p>where $\hat{L}$ is the expected per-session lift, $M$ is the number of post-experiment sessions per day, $r$ is the discount rate, and $ \tau $ is the duration of the experiment. Solving the integral gives:</p>

<script type="math/tex; mode=display">\hat{L}_a = \frac{\hat{L} M e^{-r\tau}}{r}</script>

<p>Plugging in our previously solved value of $\hat{L}$ gives</p>

<script type="math/tex; mode=display">\hat{L}_a = \frac{\sigma_\Delta^2 M e^{-r\tau}}{r \sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{m\tau})}}</script>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/#disqus_thread" data-disqus-identifier="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Optimizing sample sizes in A/B testing, Part III&#58; Aggregate time-discounted expected lift">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/">
        Optimizing sample sizes in A/B testing, Part II&#58; Expected lift
      </a>
    </h1>

    <span class="post-date">10 Jan 2020</span>

    <div class="caption">
This is Part II of a three-part blog post on how to optimize your sample size in A/B testing. Make sure to read <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I">Part I</a> if you haven't already.
</div>
<hr />

<p>In this blog post (Part II), I describe what I think is an incredibly cool business-focused formula that quantifies how much you can benefit from increasing your sample size. It is, in short, an <em>average of the value of all possible outcomes of the experiment, weighted by their probabilities</em>. This post starts off kind of dry, but if you can make it through the first section, it gets a lot easier.</p>

<h2 id="outcome-probabilities">Outcome probabilities</h2>
<p>Imagine you are comparing two versions of a website. You currently are on version A, but you would like to compare it to version B. Imagine you are measuring some random variable $X$, which might represent something like clicks per user or page views per user. The goal of the experiment is to determine which version of the website has a higher mean value of $X$.</p>

<p>This blog post aims to quantify the benefit of experimentation as an average of the value of all possible outcomes, weighted by their probabilities. To do that, we first need to describe the probabilities of all the different outcomes. An outcome consists of two parts: A <em>true</em> difference in means, $\Delta$, defined as</p>

<script type="math/tex; mode=display">\Delta = \mu_B - \mu_A</script>

<p>and an experimentally <em>observed</em> difference in means $\delta$, defined as</p>

<script type="math/tex; mode=display">\delta = \overline{X}_B - \overline{X}_A</script>

<p>Let’s start with $\Delta$. While you don’t yet know which version of the website is better (that’s what the experiment is for!), you have a sense for how important the product change is. You can therefore create a normally distributed prior on $\Delta$ with mean zero and variance $ \sigma_\Delta^2 $.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/univariate_normal.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 1</strong>.
  </div>
</div>
<p><br /></p>

<p>Next, let’s consider $\delta$, your experimentally observed difference in means. It will be a noisy estimate of $\Delta$. Let’s assume you have previously measured the variance of $X$ to be $ \sigma_X^2 $. It is reasonable to assume that within each group in the experiment, and for any particular $\Delta$, the variance of $X$ will still be $ \sigma_X^2$. You should therefore believe that for any particular $\Delta$, the observed difference in means $\delta$ will be sampled from a normal distribution $\mathcal{N}(\Delta, \sigma_c^2)$, where</p>

<script type="math/tex; mode=display">\sigma_c^2 = \frac{2\sigma_X^2}{n}</script>

<p>and where $n$ is the sample size in each of the two buckets. If that doesn’t make sense, check out <a href="https://www.khanacademy.org/math/statistics-probability/significance-tests-confidence-intervals-two-samples/comparing-two-means/v/difference-of-sample-means-distribution">this video</a>.</p>

<p>Collectively, this all forms a bivariate normal distribution of outcomes, shown below.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/bivariate_normal.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 2</strong>. Probabilities of possible outcomes, based on your prior beliefs. The horizontal axis is the true difference in means, and the vertical axis is the observed difference in means.
  </div>
</div>
<p><br /></p>

<p>To gain some more intuition about this, take a look at Figure 3. As sample size increases, $ \sigma^2_c $ decreases.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/three_bivariate_normals.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 3</strong>.
  </div>
</div>
<p><br /></p>

<h2 id="outcome-lifts">Outcome lifts </h2>
<p>Now that we know the probabilities of all the different outcomes, we next need to estimate how much per-user lift, $l$, we will gain from each possible outcome, assuming we follow a policy of shipping whichever bucket (A or B) looked better in the experiment.</p>

<ul>
  <li>In cases where $\delta &gt; 0$ and $\Delta &gt; 0$, you would ship B and your post-experiment per-user lift will be positively valued at $l = \Delta$.</li>
  <li>In cases where $\delta &gt; 0$ and $\Delta &lt; 0$, you would ship B, but unfortunately your post-experiment per-user lift will be negatively valued at $l = \Delta$, since $\Delta$ is negative.</li>
  <li>In cases where $\delta &lt; 0$, you would keep A in production, and your post-experiment lift would be zero.</li>
</ul>

<p>A heatmap of the per-user lifts ($l$) for each outcome is shown in the plot below. Good outcomes, where shipping B was the right decision, are shown in blue. Bad outcomes, where shipping B was the wrong decision, are shown in red. There are two main ways to get a neutral outcomes, shown in white. Either you keep A (bottom segment), in which case there is zero lift, or you ship B where B is only negligibly different than A (vertical white stripe).</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/lift_matrix.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 4</strong>. Heatmap of possible outcomes, where the color scale represents the lift, $l$. The horizontal axis is the true difference in means, and the vertical axis is the observed difference in means
  </div>
</div>
<p><br /></p>

<h2 id="probability-weighted-outcome-lifts">Probability-weighted Outcome Lifts</h2>
<p>At this point, we know the probability of each outcome, and we know the post-experiment per-user lift of each outcome. To determine how much lift we can expect, on average, by shipping the winning bucket of an experiment, we need to compute a probability-weighted average of the outcome lifts. Let’s start by looking at this visually and then later we’ll get into the math.</p>

<p>As shown in Figure 5, if we multiply the bivariate normal distribution (left) by the lift map (center), we can obtain the probability-weighted lift of each outcome (right).</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/product_stages.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 5</strong>.
  </div>
</div>
<p><br /></p>

<p>The good outcomes contribute more than the bad outcomes, simply because a good outcome is more likely than a bad outcome. To put it differently, experimentation will on average give you useful information.</p>

<p>To gain some more intuition on this, it is helpful to see this plot for different sample sizes. As sample size increases, the probability-weighted contribution of bad outcomes gets smaller and smaller. </p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/three_products.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 6</strong>.
  </div>
</div>
<p><br /></p>

<h3 id="computing-the-expected-post-experiment-per-user-lift">Computing the expected post-experiment per-user lift</h3>
<p>We’re almost there! To determine the expected post-experiment lift from shipping the winning bucket, we need to compute a probability-weighted average of all the post-experiment lifts. In other words, we need to sum up all the probability-weighted post-experiment lifts on the right panel of Figure 5. The formula for doing this is shown below. A derivation can be found in the Appendix.</p>

<script type="math/tex; mode=display">\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{n})}}</script>

<p>There’s three things to notice about this formula.</p>
<ul>
  <li>As $n$ increases, $\hat{L}$ increases. This makes sense. The larger the sample size, the more likely it is that you’ll ship the winning bucket.</li>
  <li>As the within-group variance $\sigma_X^2$ increases, $\hat{L}$ decreases. That’s because a high within-group variance makes experiments less informative – they’re more likely to give you the wrong answer.</li>
  <li>As the variance prior on $\Delta$ increases, $\hat{L}$ increases. This also make sense. The more impactful (positive or negative) you think the product change might be, the more value you will get from experimentation.</li>
</ul>

<p>You can try this out using the <code class="language-plaintext highlighter-rouge">get_lift_via_closed_form</code> formula in the <a href="https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb">Notebook</a>.</p>

<h2 id="demonstration-via-simulation">Demonstration via simulation</h2>
<p>In the previous section, we derived a formula for $\hat{L}$. Should you trust a formula you found on a random internet blog? Yes! Let’s put the formula to the test, by comparing its predictions to actual simulations.</p>

<p>First, let’s consider the case where the outcome is a continuous variable, such as the number of clicks. Let’s set $ \sigma_D^2 = 2 $ and $ \sigma_X^2 = 100 $. We then measure $\hat{L}$ for a range of sample sizes, using both the closed-form solution and simulations. To see how we determine $\hat{L}$ for simulations, refer to the box below.</p>

<div class="box">
<strong>Procedure for finding $\hat{L}$ with simulations</strong><br /><br />
Loop through thousands of simulated experiments. On each each experiment doing the following:<br /><br />
<ol>
<li>Sample a true group difference $\Delta$ from $\mathcal{N}(0, \sigma_D^2)$</li>
<li>Sample an $X$ for each of the $n$ users in each bucket A and B, using Normal distributions $\mathcal{N}(\frac{\Delta}{2}, \sigma_X^2)$ and $\mathcal{N}(-\frac{\Delta}{2}, \sigma_X^2)$, respectively.</li>
<li>Compute $ \delta = \overline{X}_B - \overline{X}_A $.</li>
<li>If $\delta &lt;= 0$, stick with A and accrue zero lift.</li>
<li>If $\delta &gt; 0$, ship B and accrue the per-user lift of $\Delta$, which will probably, but not necessarily, be positive.</li>
</ol>
We run these experiments thousands of times, each time computing the per-user lift. Finally, we average all the per-user lifts together to get $\hat{L}$. See the get_lift_via_simulations_continuous function in the <a href="https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb">notebook</a> for an implementation.
</div>
<p>As seen in Figure 7, below, the results of the simulation closely match the closed-form solution.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/lift_by_n_continuous.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 7</strong>.
  </div>
</div>
<p><br /></p>

<p>Second, let’s consider the case where the variable is binary, as in conversion rates. For reasonably large values of $ n $, we can safely assume that the error variance is normally distributed with variance $ \sigma_X^2 = p(1-p) $, where $ p $ is the baseline conversion rate. For this example, let’s set the baseline conversion rate $p = 0.1$, and let’s set $ \sigma_\Delta^2 = 0.01^2 $. The results of the simulation closely match the closed-form solution.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/lift_by_n_binary.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 8</strong>.
  </div>
</div>
<p><br /></p>

<h2 id="thinking-about-costs-and-a-preview-of-part-iii">Thinking about costs, and a preview of Part III</h2>

<p>In this blog post, we saw how increasing the sample size improves the expected post-experiment per-user lift, $\hat{L}$. But to determine the <em>optimal</em> sample size, we need to think about costs.</p>

<p>The cost in dollars of an experiment can be described as $f + vn$, where $f$ is a fixed cost and $ v $ is the variable cost per participant. If you already know these costs, and if you already know the revenue increase $ u $ from each unit increase in lift, you can calculate the net revenue $R$ as</p>

<script type="math/tex; mode=display">R = u\hat{L}  - f - vn</script>

<p>and then find the sample size $ n $ that maximizes $ R $.</p>

<p>Unfortunately, these costs aren’t always readily available. The good news is that there is a really nice way to calculate the most important cost: the forfeited benefit that comes from prolonging your experiment. To read about that, and about how to optimize your sample size, please continue to <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/">Part III</a>.</p>

<h2 id="appendix">Appendix</h2>
<p>To determine $\hat{L}$, we start with the probability-weighted lifts on the right panel of Figure 5. This is a bivariate normal distribution over $ \Delta $ and $ \delta $, multiplied by $ \Delta $.</p>

<script type="math/tex; mode=display">f(\Delta, \delta) = \frac{\Delta}{2 \pi \sigma_\Delta \sigma_\delta \sqrt{1-\rho^2}} e^{-\frac{
\frac{\Delta^2}{\sigma_\Delta^2} - \frac{2 \rho \Delta \delta}{\sigma_\Delta \sigma_\delta} + \frac{\delta^2}{\sigma_\delta^2}
}{2(1-\rho^2)}
}</script>

<p>where the correlation coefficient $ \rho $, is <a href="http://athenasc.com/Bivariate-Normal.pdf">defined</a> as:</p>

<script type="math/tex; mode=display">\rho = \sqrt{1 - \frac{\sigma_c^2}{\sigma_\Delta^2 + \sigma_c^2}}</script>

<p>and $\sigma_\delta^2$ is the variance on $\delta$. By the <a href="/2019/05/18/variance_after_scaling_and_summing/">variance addition rules</a>, $\sigma_\delta^2$ is defined as</p>

<script type="math/tex; mode=display">\sigma_\delta^2 = \sigma_\Delta^2 + \sigma_c^2</script>

<p>We next need to sum up the probability-weighted values in $f(\Delta, \delta)$. To obtain a closed form solution, we can use integration.</p>

<script type="math/tex; mode=display">\hat{L} = \int_{0}^{\infty} \int_{-\infty}^{\infty} {\frac{\Delta}{2 \pi \sigma_\Delta \sigma_\delta \sqrt{1-\rho^2}} e^{-\frac{

\frac{\Delta^2}{\sigma_\Delta^2} - \frac{2 \rho \Delta \delta}{\sigma_\Delta \sigma_\delta} + \frac{\delta^2}{\sigma_\delta^2}

}{2(1-\rho^2)}

}
\,d\Delta\,d\delta
}</script>

<p>The integration limits on $ \delta $ start at zero because the lift will always be zero if $ \delta &lt; 0 $ (i.e if the status quo bucket A wins the experiment).</p>

<p>Thanks to my 15-day free trial of <a href="https://www.wolfram.com/mathematica/">Mathematica</a>, I determined that this integral comes out to the surprisingly simple </p>

<script type="math/tex; mode=display">\hat{L} = \rho \frac{\sigma_\Delta}{\sqrt{2\pi}}</script>

<p>The command I used was:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Integrate[(t / (2*\[Pi]*s1*s2*Sqrt[1 - p^2]))*Exp[-((t^2/s1^2 - \
(2*p*t*d)/(s1*s2) + d^2/s2^2)/(2*(1 - p^2)))], {d, 0, \[Infinity]}, \
{t, -\[Infinity], \[Infinity]}, Assumptions -&gt; p &gt; 0 &amp;&amp; p &lt; 1 &amp;&amp; s1 &gt; \
0 &amp;&amp; s2 &gt; 0]
</code></pre></div></div>

<p>If we then substitute in previously defined formulas for $ \rho $ and $ \sigma_c^2 $, we can produce a formula that accepts more readily-available inputs.</p>

<script type="math/tex; mode=display">\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{n})}}</script>

<p>Continue to <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/">Part III</a>.</p>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/#disqus_thread" data-disqus-identifier="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Optimizing sample sizes in A/B testing, Part II&#58; Expected lift">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/">
        Optimizing sample sizes in A/B testing, Part I&#58; General summary
      </a>
    </h1>

    <span class="post-date">10 Jan 2020</span>

    <div class="caption">
A special thanks to <a href="https://www.linkedin.com/in/john-mcdonnell-65833233/">John McDonnell</a>, who came up with the idea for this post. Thanks also to <a href="https://www.linkedin.com/in/marika-inhoff-92087313a/">Marika Inhoff</a> and <a href="https://www.linkedin.com/in/nelson-ray-b180641b/">Nelson Ray</a> for comments on an earlier draft.
</div>
<hr />

<p>If you’re a data scientist, you’ve surely encountered the question, “How big should this A/B test be?”</p>

<p>The standard answer is to do a power analysis, typically aiming for 80% power at $\alpha$=5%. But if you think about it, this advice is pretty weird. Why is 80% power the best choice for your business? And doesn’t a 5% significance cutoff seem pretty arbitrary?</p>

<p>In most business decisions, you want to choose a policy that maximizes your benefits minus your costs. In experimentation, the benefit comes from learning information to drive future decisions, and the cost comes from the experiment itself. The optimal sample size will therefore depend on the unique circumstances of your business, not on arbitrary statistical significance thresholds.</p>

<p>In this three-part blog post, I’ll present a new way of determining optimal sample sizes that completely abandons the notion of statistical significance.</p>
<ul>
  <li><a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/">Part I: General Overview</a>. Starts with a mostly non-technical overview and ends with a section called “Three lessons for practitioners”.</li>
  <li><a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/">Part II: Expected lift</a>. A more technical section that quantifies the benefits of experimentation as a function of sample size.</li>
  <li><a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/">Part III: Aggregate time-discounted lift</a>. A more technical section that quantifies the costs of experimentation as a function of sample size. It then combines costs and benefits into a closed-form expression that can be optimized. Ends with an FAQ.</li>
</ul>

<p>Throughout Parts I-III, the focus will be on choosing a sample size at the beginning of the experiment and committing to it, not on dynamically updating the sample size as the experiment proceeds.</p>

<p>With that out of the way, let’s get started!</p>

<h3 id="benefits-of-large-samples">Benefits of large samples</h3>
<p>The bigger your sample size, the more likely it is that you’ll ship the right bucket. Since there is a gain to shipping the right bucket and a loss to shipping the wrong bucket, the averages benefit of the experiment is a probability-weighted average of these outcomes. We call this the <em>expected post-experiment lift</em>, $\hat{L}$, which we cover in more detail in Part II.</p>

<h3 id="costs-of-large-samples">Costs of large samples</h3>
<p>For most businesses, increasing your sample size requires you to run your experiment longer. This brings us to the main per-unit cost of experimentation: the forfeited benefits that could come from shipping the winning bucket earlier. In a fast moving startup, there’s often good reason to accrue your wins as soon as possible. The advantage of shipping earlier can be quantified with a <em>discount rate</em>, which describes how much you value the near future over the distant future. If you have a high discount rate, it’s critical to ship as soon as possible. If you have a low discount rate, you can afford to wait longer. This is described in more detail in Part III.</p>

<h3 id="combining-costs-and-benefits-into-an-optimization-function">Combining costs and benefits into an optimization function</h3>
<p>You should run your experiment long enough that you’ll likely ship the winning bucket, but not so long that you waste time not having shipped your product. The optimal duration depends on the unique circumstances of your business. The overall benefit of running an experiment, as a function of duration and other parameters, is defined as the <em>aggregate time-discounted expected post-experiment lift</em>, or $\hat{L}_a$.</p>

<p>Figure 1 shows $\hat{L}_a$ as a function of experiment duration in days ($\tau$) for one particular set of business parameters. The gray curve shows the result of a closed form solution presented in Part III. The blue curve shows the results of simulated experiments. As you can see, the optimal duration for this experiment should be about 38 days. Simulations match the closed-form predictions.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/time_aggregated_lift_by_tau_descriptive_labels.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 1.</strong> Aggregate time-discounted expected post-experiment lift ($\hat{L}_a$) as a function of experiment duration in days ($\tau$), for a fairly typical set of business parameters.
  </div>
</div>
<p><br /></p>

<h3 id="three-lessons-for-practitioners">Three lessons for practitioners</h3>
<p>I played around with the formula for $\hat{L}_a$ and came across three lessons that should be of interest to practitioners.</p>

<h4 id="1-you-should-run-underpowered-experiments-if-you-have-a-very-high-discount-rate">1. You should run “underpowered” experiments if you have a very high discount rate</h4>
<p>Take a look at Figure 2, which shows some recommendations for a fairly typical two-bucket conversion rate experiment with 1000 sessions per bucket per day. On the left panel we plot the optimal duration as a function of the annual discount rate. If you have a high discount rate, you care a lot more about the near future than the distant future. It is therefore critical that you ship any potentially winning version as soon as possible. In this scenario, the optimal duration is low (left panel). Power, the probability you will find a statistically significant result, is also low (right panel). For many of these cases, the optimal duration would traditionally be considered “underpowered”.</p>
<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/optimal_tau_and_power_by_r.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 2</strong>.
  </div>
</div>
<p><br /></p>

<h4 id="2-you-should-run-underpowered-experiments-if-you-have-a-small-user-base">2. You should run “underpowered” experiments if you have a small user base</h4>
<p>Now let’s plot these curves as a function of $m$, our daily sessions per bucket. If you only have a small number of daily sessions to work with, you’ll need to run the experiment for longer (left panel). So far, that’s not surprising. But here’s where it gets interesting: Even though optimal duration increases as $m$ decreases, it doesn’t increase fast enough to maintain constant power (right panel). In fact, for low $m$ scenarios where you don’t have a lot of users, the optimal duration results in power that can drop below 50%, well into what would traditionally be considered “underpowered” territory. In these situations, waiting to get a large number of sessions causes too much time-discounting loss.</p>
<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/optimal_tau_and_power_by_m.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 3</strong>.
  </div>
</div>
<p><br /></p>

<h4 id="3-that-said-its-far-better-to-run-your-experiment-too-long-than-too-short">3. That said, it’s far better to run your experiment too long than too short</h4>
<p>Let’s take another look at $\hat{L}_a$ as a function of duration. As shown in Figure 4 below, the left shoulder is steeper than the right shoulder. This means that it’s really bad if your experiment is shorter than optimal, but it’s kind of ok if your experiment is longer than optimal.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/time_aggregated_lift_by_tau_shoulders.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 4.</strong> Aggregate time-discounted expected post-experiment lift ($\hat{L}_a$) as a function of experiment duration in days ($\tau$), for a fairly typical set of business parameters.
  </div>
</div>
<p><br /></p>

<p>Is this true in general? Yes. Below we plot $\hat{L}_a$ as a function of duration for various combinations of $m$ and the discount rate, $r$. For all of these parameter combinations, it’s better to run a bit longer than optimal than a bit shorter than optimal. The only exception is if you have an insanely high discount rate (not shown).</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/L_a_by_tau_for_m_and_r.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 5</strong>.
  </div>
</div>
<p><br /></p>

<h3 id="upcoming-posts-and-python-notebook">Upcoming posts and Python notebook</h3>
<p>You probably have a lot of questions about where this framework comes from and how it is justified. <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/">Part II</a> and <a href="/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/">Part III</a> dive more deeply into the math and visual intuition behind it. They also contain some example uses of the <code class="language-plaintext highlighter-rouge">get_lift_via_closed_form</code> and <code class="language-plaintext highlighter-rouge">get_agg_lift_via_closed_form</code> functions available in the accompanying <a href="https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb">Python Notebook</a>.</p>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/#disqus_thread" data-disqus-identifier="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Optimizing sample sizes in A/B testing, Part I&#58; General summary">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/05/18/variance_after_scaling_and_summing/">
        Variance after scaling and summing&#58; One of the most useful facts from statistics
      </a>
    </h1>

    <span class="post-date">18 May 2019</span>

    <p>What do $ R^2 $, laboratory error analysis, ensemble learning, meta-analysis, and financial portfolio risk all have in common? The answer is that they all depend on a fundamental principle of statistics that is not as widely known as it should be. Once this principle is understood, a lot of stuff starts to make more sense.</p>

<p>Here’s a sneak peek at what the principle is.</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij}</script>

<p>Don’t worry if the formula doesn’t yet make sense! We’ll work our way up to it slowly, taking pit stops along the way at simpler formulas are that useful on their own. As we work through these principles, we’ll encounter lots of neat applications and explainers.</p>

<p>This post consists of three parts:</p>

<ul>
  <li><strong>Part 1</strong>: Sums of uncorrelated random variables: Applications to social science and laboratory error analysis</li>
  <li><strong>Part 2</strong>: Weighted sums of uncorrelated random variables: Applications to machine learning and scientific meta-analysis</li>
  <li><strong>Part 3</strong>: Correlated variables and Modern Portfolio Theory</li>
</ul>

<h2 id="part-1-sums-of-uncorrelated-random-variables-applications-to-social-science-and-laboratory-error-analysis">Part 1: Sums of uncorrelated random variables: Applications to social science and laboratory error analysis</h2>

<p>Let’s start with some simplifying conditions and assume that we are dealing with <em>uncorrelated</em> random variables. If you take two of them and add them together, the variance of their sum will equal the sum of their variances. This is amazing!</p>

<p>To demonstrate this, I’ve written some Python code that generates three arrays, each of length 1 million. The first two arrays contain samples from two normal distributions with variances 9 and 16, respectively. The third array is the sum of the first two arrays. As shown in the simulation, its variance is 25, which is equal to the sum of the variances of the first two arrays (9 + 16).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># 1M samples from normal distribution with variance=9
</span><span class="k">print</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 9
</span><span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># 1M samples from normal distribution with variance=16
</span><span class="k">print</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 16
</span><span class="n">xp</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span>
<span class="k">print</span><span class="p">(</span><span class="n">xp</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 25
</span></code></pre></div></div>

<p>This <a href="https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)">fact</a> was first discovered in 1853 and is known as <a href="https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)">Bienaymé’s Formula</a>. While the code example above shows the sum of two random variables, the formula can be extended to multiple random variables as follows:</p>

<div class="box">
If $ X_p $ is a sum of uncorrelated random variables $ X_1 .. X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum{\sigma^2_i} $$

where each $ X_i $ has variance $ \sigma_i^2 $.
</div>

<p>What does the $ p $ stand for in $ X_p $? It stands for <em>portfolio</em>, which is just one of the many applications we’ll see later in this post.</p>

<h3 id="why-this-is-useful">Why this is useful</h3>
<p>Bienaymé’s result is surprising and unintuitive. But since it’s such a simple formula, it is worth committing to memory, especially because it sheds light on so many other principles. Let’s look at two of them.</p>

<h4 id="understanding--r2--and-variance-explained">Understanding $ R^2 $ and “variance explained”</h4>
<p>Psychologists often talk about “within-group variance”, “between-group variance”, and “variance explained”. What do these terms mean?</p>

<p>Imagine a hypothetical study that measured the extraversion of 10 boys and 10 girls, where extraversion is measured on a 10-point scale (<em>Figure 1</em>. Orange bars). The boys have a mean extraversion of 4.4 and the girls have a mean extraversion 5.0. In addition, the overall variance of the data is 2.5.  We can decompose this variance into two parts:</p>

<ul>
  <li><strong>Between-group variance</strong>: Create a 20-element array where every boy is assigned to the mean boy extraversion of 4.4, and every girl is assigned to the mean girl extraversion of 5.0. The variance of this array is 0.9. (<em>Figure 1</em>. Blue bars).</li>
  <li><strong>Within-group variance</strong>: Create a 20-element array of the amount each child’s extraversion deviates from the mean value for their sex. Some of these values will be negative and some will be positive. The variance of this array is 1.6. (<em>Figure 1</em>. Pink bars).</li>
</ul>

<div class="wrapper">
  <img src="/assets/2019_variance/boy_girl_extraversion.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 1</strong>: Decomposition of extraversion scores (orange) into between-group variance (blue) and within-group variance (pink).
  </div>
</div>
<p><br /></p>

<p>If you add these arrays together, the resulting array will represent the observed data (<em>Figure 1</em>. Orange bars). The variance of the observed array is 2.5, which is exactly what is predicted by Bienaymé’s Formula. It is the sum of the variances of the two component arrays (0.9 + 1.6). Psychologists might say that sex “explains” 0.9/2.5 = 36% of the extraversion variance. Equivalently, a model of extraversion that uses sex as the only predictor would have an <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">$ R^2 $</a> of 0.36.</p>

<h4 id="error-propagation-in-laboratories">Error propagation in laboratories</h4>
<p>If you ever took a physics lab or chemistry lab back in college, you may remember having to perform <a href="http://ipl.physics.harvard.edu/wp-uploads/2013/03/PS3_Error_Propagation_sp13.pdf">error analysis</a>, in which you calculated how errors would propagate through one noisy measurement after another.</p>

<p>Physics textbooks often say that standard deviations add in “quadrature”, which just means that if you are trying to estimate some quantity that is the sum of two other measurements, and if each measurement has some error with standard deviation <script type="math/tex">\sigma_1</script> and <script type="math/tex">\sigma_2</script> respectively, the final standard deviation would be  <script type="math/tex">\sigma_{p} = \sqrt{\sigma^2_1 + \sigma^2_2}</script>. I think it’s probably easier to just use variances, as in the Bienaymé Formula, with <script type="math/tex">\sigma^2_{p} = \sigma^2_1 + \sigma^2_2</script>.</p>

<p>For example, imagine you are trying to estimate the height of two boxes stacked on top of each other (<em>Figure 2</em>). One box has a height of 1 meter with variance $ \sigma^2_1 $ = 0.01, and the other has a height of 2 meters with variance $ \sigma^2_2 $ = 0.01. Let’s further assume, perhaps optimistically, that these errors are independent. That is, if the measurement of the first box is too high, it’s not any more likely that the measurement of the second box will also be too high. If we can make these assumptions, then the total height of the two boxes will be 3 meters with variance $ \sigma^2_p $ = 0.02.</p>

<div class="wrapper">
  <img src="/assets/2019_variance/stacked_boxes.png" class="inner" style="position:relative border: #222 2px solid; max-width:50%;" />
  <div class="caption"><strong>Figure 2</strong>: Two boxes stacked on top of each other. The height of each box is measured with some variance (uncertainty). The total height is the sum of the individual heights, and the total variance (uncertainty) is the sum of the individual variances.
  </div>
</div>
<p><br /></p>

<p>There is a key difference between the extraversion example and the stacked boxes example. In the extraversion example, we added two <em>arrays</em> that each had an observed sample variance. In the stacked boxes example, we added two <em>scalar measurements</em>, where the variance of these measurements refers to our measurement uncertainty. Since both cases have a meaningful concept of ‘variance’, the Bienaymé Formula applies to both.</p>

<h2 id="part-2-weighted-sums-of-uncorrelated-random-variables-applications-to-machine-learning-and-scientific-meta-analysis">Part 2: Weighted sums of uncorrelated random variables: Applications to machine learning and scientific meta-analysis</h2>

<p>Let’s now move on to the case of <em>weighted</em> sums of uncorrelated random variables. But before we get there, we first need to understand what happens to variance when a random variable is scaled.</p>

<div class="box">
If $ X_p $ is defined as $ X $ scaled by a factor of $ w $, then the variance $ X_p $ will be

$$ \sigma_{p}^{2} = w^2 \sigma^2 $$

where $ \sigma^2 $ is the variance of $ X $.
</div>

<p>This means that if a random variable is scaled, the scale factor on the variance will change <em>quadratically</em>. Let’s see this in code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">baseline_var</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">w</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">baseline_var</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># Array of 1M samples from normal distribution with variance=10
</span><span class="k">print</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 10
</span><span class="n">xp</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x1</span> <span class="c1"># Scale this by w=0.7
</span><span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">baseline_var</span><span class="p">)</span> <span class="c1"># 4.9 (predicted variance)
</span><span class="k">print</span><span class="p">(</span><span class="n">xp</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 4.9 (empirical variance) 
</span></code></pre></div></div>
<p>To gain some intuition for this rule, it’s helpful to think about outliers. We know that outliers have a huge effect on variance. That’s because the formula used to compute variance, $ \sum{\frac{(x_i - \bar{x})^2}{n-1}} $, squares all the deviations, and so we get really big variances when we square large deviations. With that as background, let’s think about what happens if we scale our data by 2. The outliers will spread out twice as far, which means they will have even more than twice as much impact on the variance. Similarly, if we multiply our data by 0.5, we will squash the most “damaging” part of the outliers, and so we will reduce our variance by more than a factor of two.</p>

<p>While the above principle is pretty simple, things start to get interesting when you combine it with the Bienaymé Formula in Part I:</p>

<div class="box">
If $ X_p $ is a weighted sum of uncorrelated random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be 

$$ \sigma_{p}^{2} = \sum{w^2_i \sigma^2_i} $$

where each $ w_i $ is a weight on $ X_i $, and each $ X_i $ has its own variance $ \sigma_i^2 $.
</div>

<p>The above formula shows what happens when you scale and then sum random variables. The final variance is the weighted sum of the original variances, where the weights are squares of the original weights. Let’s see how this can be applied to machine learning.</p>

<h3 id="an-ensemble-model-with-equal-weights">An ensemble model with equal weights</h3>

<p>Imagine that you have built two separate models to predict car prices. While the models are unbiased, they have variance in their errors. That is, sometimes a model prediction will be too high, and sometimes a model prediction will be too low. Model 1 has a mean squared error (MSE) of \$1,000 and Model 2 has an MSE of \$2,000.</p>

<p>A valuable insight from machine learning is that you can often create a better model by simply averaging the predictions of other models. Let’s demonstrate this with simulations below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">actual</span> <span class="o">=</span> <span class="mi">20000</span> <span class="o">+</span> <span class="mi">5000</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">errors1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">errors1</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 1000
</span><span class="n">errors2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">errors2</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 2000
</span>
<span class="c1"># Note that this section could be replaced with 
# errors_ensemble = 0.5 * errors1 + 0.5 * errors2
</span><span class="n">preds1</span> <span class="o">=</span> <span class="n">actual</span> <span class="o">+</span> <span class="n">errors1</span>
<span class="n">preds2</span> <span class="o">=</span> <span class="n">actual</span> <span class="o">+</span> <span class="n">errors2</span>
<span class="n">preds_ensemble</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">preds1</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">preds2</span>
<span class="n">errors_ensemble</span> <span class="o">=</span> <span class="n">preds_ensemble</span> <span class="o">-</span> <span class="n">actual</span>

<span class="k">print</span><span class="p">(</span><span class="n">errors_ensemble</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 750. Lower than variance of component models!
</span></code></pre></div></div>

<p>As shown in the code above, even though a good model (Model 1) was averaged with an inferior model (Model 2), the resulting Ensemble model’s MSE of \$750 is better than either of the models individually.</p>

<p>The benefits of ensembling follow directly from the weighted sum formula we saw above, <script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>. To understand why, it’s helpful to think of models not as generating predictions, but rather as generating errors. Since averaging the predictions of a model corresponds to averaging the errors of the model, we can treat each model’s array of errors as samples of a random variable whose variance can be plugged in to the formula. Assuming the models are unbiased (i.e. the errors average to about zero), the formula tells us the expected MSE of the ensemble predictions. In the example above, the MSE would be</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = 0.5^2 \times 1000 + 0.5^2 \times 2000 = 750</script>

<p>which is exactly what we observed in the simulations.</p>

<p>(For a totally different intuition of why ensembling works, see <a href="https://www.opendoor.com/w/blog/why-ensembling-works-the-intuition-behind-opendoors-home-pricing">this blog post</a> that I co-wrote for my company, Opendoor.)</p>

<h3 id="an-ensemble-model-with-inverse-variance-weighting">An ensemble model with Inverse Variance Weighting</h3>

<p>In the example above, we obtained good results by using an equally-weighted average of the two models. But can we do better?</p>

<p>Yes we can! Since Model 1 was better than Model 2, we should probably put more weight on Model 1. But of course we shouldn’t put all our weight on it, because then we would throw away the demonstrably useful information from Model 2. The optimal weight must be somewhere in between 50% and 100%.</p>

<p>An effective way to find the optimal weight is to <a href="http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/">build another model on top of these models</a>. However, if you can make certain assumptions (unbiased and uncorrelated errors), there’s an even simpler approach that is great for back-of-the envelope calculations and great for understanding the principles behind ensembling.</p>

<p>To find the optimal weights (assuming unbiased and uncorrelated errors), we need to minimize the variance of the ensemble errors
<script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>
with the constraint that 
<script type="math/tex">\sum{w_i} = 1</script>.</p>

<p>It <a href="https://en.wikipedia.org/wiki/Inverse-variance_weighting">turns out</a> that the variance-minimizing weight for a model should be proportional to the inverse of its variance.</p>

<script type="math/tex; mode=display">w_k = \frac{\frac{1}{\sigma^2_k}}{\sum{\frac{1}{\sigma^2_i}}}</script>

<p>When we apply this method, we obtain optimal weights of <script type="math/tex">w_1</script> = 0.67 and <script type="math/tex">w_2</script> = 0.33. These weights give us an ensemble error variance of</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = 0.67^2 \times 1000 + 0.33^2 \times 2000 = 666</script>

<p>which is significantly better than the $750 variance we were getting with equal weighting.</p>

<p>This method is called <a href="https://en.wikipedia.org/wiki/Inverse-variance_weightinghttps://en.wikipedia.org/wiki/Inverse-variance_weighting">Inverse Variance Weighting</a>, and allows you to assign the right amount of weight to each model, depending on its error.</p>

<p>Inverse Variance Weighting is not just useful as a way to understand Machine Learning ensembles. It is also one of the core principles in scientific <a href="https://en.wikipedia.org/wiki/Meta-analysis">meta-analysis</a>, which is popular in medicine and the social sciences. When multiple scientific studies attempt to estimate some quantity, and each study has a different sample size (and hence variance of their estimate), a meta-analysis should weight the high sample size studies more. Inverse Variance Weighting is used to determine those weights.</p>

<h2 id="part-3-correlated-variables-and-modern-portfolio-theory">Part 3: Correlated variables and Modern Portfolio Theory</h2>

<p>Let’s imagine we now have three unbiased models with the following MSEs:</p>

<ul>
  <li>Model 1: MSE = 1000</li>
  <li>Model 2: MSE = 1000</li>
  <li>Model 3: MSE = 2000</li>
</ul>

<p>By Inverse Variance Weighting, we should assign more weight to the first two models, with <script type="math/tex">w_1=0.4, w_2=0.4, w_3=0.2</script>.</p>

<p>But what happens if Model 1 and Model 2 have correlated errors? For example, whenever Model 2’s predictions are too high, Model 3’s predictions tend to also be too high. In that case, maybe we don’t want to give so much weight to Models 1 and 2, since they provide somewhat redundant information. Instead we might want to <em>diversify</em> our ensemble by increasing the weight on Model 3, since it provides new independent information.</p>

<p>To determine how much weight to put on each model, we first need to determine how much total variance there will be if the errors are correlated. To do this, we need to borrow a <a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory">formula</a> from the financial literature, which extends the formulas we’ve worked with before. This is the formula we’ve been waiting for.</p>

<div class="box">
If $ X_p $ is a weighted sum of (correlated or uncorrelated) random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij} $$

where each $ w_i $ and $ w_j $ are weights assigned to $ X_i $ and $ X_j $, where each $ X_i $ and $ X_j $ have standard deviations $ \sigma_i $ and $ \sigma_j $, and where the correlation between $ X_i $ and $ X_j $ is $ \rho_{ij} $.
</div>

<p>There’s a lot to unpack here, so let’s take this step by step.</p>

<ul>
  <li><script type="math/tex">\sigma_i \sigma_j \rho_{ij}</script> is a scalar quantity representing the covariance between <script type="math/tex">X_i</script> and <script type="math/tex">X_j</script>.</li>
  <li>If none of the variables are correlated with each other, then all the cases where $ i \neq j $ will go to zero, and the formula reduces to <script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>, which we have seen before.</li>
  <li>The more that two variables <script type="math/tex">X_i</script> and <script type="math/tex">X_j</script> are correlated, the more the total variance <script type="math/tex">\sigma_{p}^{2}</script> increases.</li>
  <li>If two variables <script type="math/tex">X_i</script> and <script type="math/tex">X_j</script> are anti-correlated, then the total variance decreases, since <script type="math/tex">\sigma_i \sigma_j \rho_{ij}</script> is negative.</li>
  <li>This formula can be rewritten in more compact notation as <script type="math/tex">\sigma_{p}^{2} = \vec{w}^T\Sigma \vec{w}</script>, where <script type="math/tex">\vec{w}</script> is the weight vector, and <script type="math/tex">\Sigma</script> is the covariance matrix (not a summation sign!)</li>
</ul>

<p>If you skimmed the bullet points above, go back and re-read them! They are super important.</p>

<p>To find the set of weights that minimize variance in the errors, you must minimize the above formula, with the constraint that <script type="math/tex">\sum{w_i} = 1</script>. One way to do this is to use a <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize">numerical optimization method</a>. In practice, however, it is more common to just find weights by <a href="http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/">building another model on top of the base models</a></p>

<p>Regardless of how the weights are found, it will usually be the case that if Models 1 and 2 are correlated, the optimal weights will reduce redundancy and put lower weight on these models than simple Inverse Variance Weighting would suggest.</p>

<h3 id="applications-to-financial-portfolios">Applications to financial portfolios</h3>

<p>The formula above was discovered by economist <a href="https://en.wikipedia.org/wiki/Harry_Markowitz">Harry Markowitz</a> in his <a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory">Modern Portfolio Theory</a>, which describes how an investor can optimally trade off between expected returns and expected risk, often measured as variance. In particular, the theory shows how to maximize expected return given a fixed variance, or minimize variance given a fixed expected return. We’ll focus on the latter.</p>

<p>Imagine you have three stocks to put in your portfolio. You plan to sell them at time $ T $, at which point you expect that Stock 1 will have gone up by 5%, with some uncertainty. You can describe your uncertainty as variance, and in the case of Stock 1, let’s say <script type="math/tex">\sigma_1^2</script> = 1. This stock, as well Stocks 2 and 3, are summarized in the table below:</p>

<table>
  <thead>
    <tr>
      <th>Stock ID</th>
      <th>Expected Return</th>
      <th>Expected Risk (<script type="math/tex">\sigma^2</script>)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>5.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>5.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>5.0</td>
      <td>2.0</td>
    </tr>
  </tbody>
</table>

<p>This financial example should remind you of ensembling in machine learning. In the case of ensembling, we wanted to minimize variance of the weighted sum of error arrays. In the case of financial portfolios, we want to minimize the variance of the weighted sum of scalar financial returns.</p>

<p>As before, if there are no correlations between the expected returns (i.e. if Stock 1 exceeding 5% return does not imply that Stock 2 or Stock 3 will exceed 5% return), then the total variance in the portfolio will be
<script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>
and we can use Inverse Variance Weighting to obtain weights $ w_1=0.4, w_2=0.4, w_3=0.2 $.</p>

<p>However, sometimes stocks have correlated expected returns. For example, if two of the stocks are in oil companies, then one stock exceeding 5% implies the other is also likely to exceed 5%. When this happens, the total variance becomes</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij}</script>

<p>as we saw before in the ensemble example. Since this includes an additional positive term for <script type="math/tex">w_1 w_2 \sigma_1 \sigma_2 \rho_{1,2}</script>, the expected variance is higher than in the uncorrelated case, assuming the correlations are positive. To reduce this variance, we should put less weight on Stocks 1 and 2 than we would otherwise.</p>

<p>While the example above focused on minimizing the variance of a financial portfolio, you might also be interested in having a portfolio with high return. Modern Portfolio Theory describes how a portfolio can reach any abitrary point on the <a href="https://en.wikipedia.org/wiki/Efficient_frontier">efficient frontier</a> of variance and return, but that’s outside the scope of this blog post. And as you might expect, financial markets can be more complicated than Modern Portfolio Theory suggests, but that’s also outside scope.</p>

<h2 id="summary">Summary</h2>

<p>That was a long post, but I hope that the principles described have been informative. It may be helpful to summarize them in <em>backwards</em> order, starting with the most general principle.</p>

<div class="box">

If $ X_p $ is a weighted sum of (correlated or uncorrelated) random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij} $$

where each $ w_i $ and $ w_j $ are weights assigned to $ X_i $ and $ X_j $, where each $ X_i $ and $ X_j $ have standard deviations $ \sigma_i $ and $ \sigma_j $, and where the correlation between $ X_i $ and $ X_j $ is $ \rho_{ij} $. The term $ \sigma_i \sigma_j \rho_{ij} $ is a scalar quantity representing the covariance between $ X_i $ and $ X_j $.

<br /><br />If none of the variables are correlated, then all the cases where $ i \neq j $ go to zero, and the formula reduces to 

$$ \sigma_{p}^{2} = \sum{w^2_i \sigma^2_i} $$

And finally, if we are computing a simple sum of random variables where all the weights are 1, then the formula reduces to

$$ \sigma_{p}^{2} = \sum{\sigma^2_i} $$
</div>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2019/05/18/variance_after_scaling_and_summing/#disqus_thread" data-disqus-identifier="http://localhost:4000/2019/05/18/variance_after_scaling_and_summing/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2019/05/18/variance_after_scaling_and_summing/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Variance after scaling and summing&#58; One of the most useful facts from statistics">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>

<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'csaid81'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function () {
  var s = document.createElement('script'); s.async = true;
  s.type = 'text/javascript';
  s.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
  }());
</script>

    </div>

  </body>
</html>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


<script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
