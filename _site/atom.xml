<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Chris Said</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2020-07-30T13:50:52-07:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Chris Said</name>
   <email></email>
 </author>

 
 <entry>
   <title>Why low sensitivity antigen tests are better than slow PCR tests</title>
   <link href="http://localhost:4000/2020/07/15/why-inaccurate-antigen-tests-are-better-than-slow-pcr-tests/"/>
   <updated>2020-07-15T00:00:00-07:00</updated>
   <id>http://localhost:4000/2020/07/15/why-inaccurate--antigen-tests-are-better-than-slow-pcr-tests</id>
   <content type="html">&lt;p&gt;&lt;em&gt;[Disclaimer: I am not an epidemiologist. The models presented here make simplifying assumptions.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[Update: After I wrote this post, I got confirmation that antigen tests have low sensitivity &lt;a href=&quot;https://www.medrxiv.org/content/10.1101/2020.06.22.20136309v2.full.pdf&quot;&gt;mostly when viral load is low and you aren’t infectious&lt;/a&gt;, which further stengthens the case for antigen tests.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If we want to reopen schools and offices, we need to do so safely. According to Nobel Prize winning economist Paul Romer, the best way is with &lt;a href=&quot;https://schools.paulromer.net/&quot;&gt;frequent&lt;/a&gt; and &lt;a href=&quot;https://paulromer.net/faqs-on-virus-tests-in-schools/&quot;&gt;regular&lt;/a&gt; testing of asymptomatic students and employees. Catching infections early can stop the spread before it starts.&lt;/p&gt;

&lt;p&gt;Much of the discussion about testing has centered on &lt;a href=&quot;https://www.npr.org/sections/health-shots/2020/05/01/847368012/how-reliable-are-covid-19-tests-depends-which-one-you-mean&quot;&gt;PCR&lt;/a&gt;, a relatively expensive method that is vulnerable to &lt;a href=&quot;https://www.npr.org/2020/05/28/863558750/coronavirus-testing-machines-are-latest-bottleneck-in-troubled-supply-chain&quot;&gt;multiple&lt;/a&gt; &lt;a href=&quot;https://www.latimes.com/california/story/2020-07-12/california-fail-coronavirus-testing-covid-start&quot;&gt;supply chain&lt;/a&gt; bottlenecks and is generally &lt;a href=&quot;https://www.technologyreview.com/2020/05/06/1001150/podcast-covid-19-testing-bottleneck/&quot;&gt;hard to scale up&lt;/a&gt;. Test samples must be sent to a central laboratory and then passed through expensive machines, a process that is currently taking 5-7 days, and in some places as high as &lt;a href=&quot;https://twitter.com/SFCovidTestWait/status/1283122098714693633&quot;&gt;10-13 days&lt;/a&gt;. Efforts are being made to speed this up via pooling, but the operational complexity of PCR makes it likely that there will always be at least a few days of delay.&lt;/p&gt;

&lt;p&gt;A less talked about alternative to PCR is called &lt;em&gt;&lt;a href=&quot;https://www.sciencemag.org/news/2020/05/coronavirus-antigen-tests-quick-and-cheap-too-often-wrong&quot;&gt;antigen tests&lt;/a&gt;&lt;/em&gt;. These tests are much cheaper than PCR, with estimates ranging between &lt;a href=&quot;https://www.sciencemag.org/news/2020/05/coronavirus-antigen-tests-quick-and-cheap-too-often-wrong&quot;&gt;$1&lt;/a&gt; and &lt;a href=&quot;https://www.technologyreview.com/2020/04/24/1000486/antigen-testing-could-faster-cheaper-diagnose-covid-19-coronavirus/&quot;&gt;$10&lt;/a&gt; per test, compared to &lt;a href=&quot;https://www.technologyreview.com/2020/04/24/1000486/antigen-testing-could-faster-cheaper-diagnose-covid-19-coronavirus/&quot;&gt;$50&lt;/a&gt; for PCR. More importantly, they can return results in just &lt;a href=&quot;https://www.cdc.gov/flu/professionals/diagnosis/clinician_guidance_ridt.htm&quot;&gt;15 minutes&lt;/a&gt;, without any need to send samples to a central lab.&lt;/p&gt;

&lt;p&gt;Since antigen tests are far cheaper and faster than PCR, why are we not massively investing in them?&lt;/p&gt;

&lt;p&gt;The usual answer is test sensitivity. SARS-CoV-2 antigen tests have a sensitivity of about &lt;a href=&quot;https://www.nytimes.com/2020/07/06/health/fast-coronavirus-tests.html&quot;&gt;85-90%&lt;/a&gt;, meaning that only 85%-90% of true cases will be detected as such. And those are optimistic estimates reported by test manufacturers. For influenza, the CDC reports that antigen tests have a sensitivity of &lt;a href=&quot;https://www.cdc.gov/flu/professionals/diagnosis/clinician_guidance_ridt.htm&quot;&gt;50-70%&lt;/a&gt;. This is much lower than PCR tests, which have a sensitivity as high at &lt;a href=&quot;https://www.medrxiv.org/content/10.1101/2020.05.26.20112565v1.full.pdf&quot;&gt;98%&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some doctors view the low sensitivity as an insurmountable hurdle. “It won’t work”, &lt;a href=&quot;https://www.technologyreview.com/2020/04/24/1000486/antigen-testing-could-faster-cheaper-diagnose-covid-19-coronavirus/&quot;&gt;said&lt;/a&gt; one doctor. “A misdiagnosis is worse than no diagnosis”, &lt;a href=&quot;https://www.sciencemag.org/news/2020/05/coronavirus-antigen-tests-quick-and-cheap-too-often-wrong&quot;&gt;said&lt;/a&gt; another.&lt;/p&gt;

&lt;p&gt;I am not sure I agree, at least when it comes to mass testing of asymptomatic individuals. In this blog post I will show, via both simulations and analytical arguments, that the low sensitivity of antigen tests is a much better problem to have than the long delay of a PCR test.&lt;/p&gt;

&lt;h3 id=&quot;simulations&quot;&gt;Simulations&lt;/h3&gt;

&lt;p&gt;To test this hypothesis, I built an &lt;a href=&quot;https://github.com/csaid/covid_model_with_testing/blob/master/SIR%20model%20with%20testing.ipynb&quot;&gt;agent-based SIR model&lt;/a&gt; that includes testing and quarantining. I make the following key assumptions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Population of 200 students&lt;/li&gt;
  &lt;li&gt;Using a rotation system, each student is tested once every K days.&lt;/li&gt;
  &lt;li&gt;Students go into a 14 day quarantine if they get positive test results.&lt;/li&gt;
  &lt;li&gt;In addition to intra-school infections, each student has a chance of getting infected by someone outside the population.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I &lt;a href=&quot;https://github.com/csaid/covid_model_with_testing/blob/master/SIR%20model%20with%20testing.ipynb&quot;&gt;simulated&lt;/a&gt; two different scenarios.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Regular PCR testing (sensitivity of 98%, testing delay of 5 days)&lt;/li&gt;
  &lt;li&gt;Regular antigen testing (sensitivity of 75%, no testing delay)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For comparison, I also modeled a “best case scenario”, where no intra-school infections occur, and a “worst case scenario”, where there is no testing or quarantining at all.&lt;/p&gt;

&lt;p&gt;Assuming that each student is tested once every 4 days, I find that the cumulative number of infections is far lower if we do regular antigen testing than if we do regular PCR testing.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_antigen_testing/fig_test_every_4_days.png&quot; class=&quot;inner&quot; style=&quot;position:relative border:#222 2px solid; max-width:95%;&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If we change the frequency of testing to once every day, antigen testing is able to achieve close to best case results, with almost no intra-school infections. Meanwhile, daily PCR testing still results in a large number of infections.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_antigen_testing/fig_test_every_1_day.png&quot; class=&quot;inner&quot; style=&quot;position:relative border:#222 2px solid; max-width:95%;&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This seems very important. &lt;em&gt;Not only are antigen tests cheaper than PCR, they are also more effective in a mass testing regime.&lt;/em&gt; Further simulations in my &lt;a href=&quot;https://github.com/csaid/covid_model_with_testing/blob/master/SIR%20model%20with%20testing.ipynb&quot;&gt;notebook&lt;/a&gt; show that the superiority of antigen tests is maintained even for much lower sensitivities.&lt;/p&gt;

&lt;h3 id=&quot;why-is-low-sensitivity-better-than-slowness&quot;&gt;Why is low sensitivity better than slowness?&lt;/h3&gt;

&lt;p&gt;Maybe you don’t believe my simulations, so here’s an analytical argument instead. Imagine you test people every day using a 75% sensitive antigen test. If someone is infected, they will probably find out immediately. There is a smallish chance they will first find out after a day, and a very small chance it will take longer than that. On average they will find out after 0.33 days, which is much faster than a typical PCR test. In general, for a sensitivity of &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;, daily antigen test participants will find out after an average of &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; days, where &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d = \sum_{t=0}^{\infty}{(1-s)^t s t}&lt;/script&gt;

&lt;p&gt;which converges to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d = \frac{1-s}{s}&lt;/script&gt;

&lt;p&gt;For any reasonable value of &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; will be quite low. Here’s a striking example. If you test people every day, a test with 50% sensitivity and no delay is equivalent to a test with 100% sensitivity and a 1-day delay!&lt;/p&gt;

&lt;p&gt;This analysis assumes that &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; is fixed across time and individuals, an assumption I’ll address in the FAQ, below.&lt;/p&gt;

&lt;h3 id=&quot;faq&quot;&gt;FAQ:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;What is your main conclusion from this analysis?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; For mass testing of asymptomatic individuals in schools and offices, antigen tests are cheaper and more effective than PCR tests.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Have other people come to this conclusion?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Yes. After writing this blog post, I came across two recent pre-prints.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.medrxiv.org/content/10.1101/2020.06.22.20136309v2.full.pdf&quot;&gt;Larremore et al.&lt;/a&gt; shows that sensitivity is less important than testing delays. It comes with a wonderful &lt;a href=&quot;https://larremorelab.github.io/covid-calculator3&quot;&gt;interactive calculator&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.medrxiv.org/content/10.1101/2020.07.06.20147702v1.full.pdf&quot;&gt;Paltiel et al.&lt;/a&gt; show that sensitivity is less important than test frequency.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Are you arguing that antigen tests should always replace PCR?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; No, I’m only suggesting it for mass testing of asymptomatic individuals. If a symptomatic person comes into a doctor’s office a PCR test would make sense, since sensitivity becomes more important and cost becomes less important.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;What if the antigen test has a much lower sensitivity for certain individuals, who will therefore remain undetected?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Low sensitivity for these individuals is &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S138665321000106X&quot;&gt;likely due to low viral load&lt;/a&gt;, which most researchers believe means they will not be very infectious. That said, if would be really good if we could get more research on this, since it’s an important question.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;What if the sensitivity of the antigen test is especially low at the start of infection?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Another good question. As with the previous answer, this period of low sensitivity would likely be to due low viral load, when the individual is less infectious. Furthermore, the period where PCR tests can detect the virus while antigen tests cannot is &lt;a href=&quot;https://www.medrxiv.org/content/10.1101/2020.06.22.20136309v2.full.pdf&quot;&gt;probably only 24 hours&lt;/a&gt;. But &lt;em&gt;even if&lt;/em&gt; we assume the individual is just as infectious during this early period, the antigen test still remains superior. To estimate this, I ran simulations where antigen tests were completely ineffective for the first two days of infection, a conservative assumption. Even with this conservative constraint, and even assuming the individuals are fully infectious during this period, antigen tests still outperformed PCR.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_antigen_testing/fig_test_every_1_day_delayed_detection.png&quot; class=&quot;inner&quot; style=&quot;position:relative border:#222 2px solid; max-width:95%;&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;What if the false negatives cause riskier behavior?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; It’s possible that people might exhibit more risk after a false negative antigen test than after taking a PCR test and having to wait. However, this risk can be minimized by (a) educating people about sensitivity and (b) pairing the testing regime with other measures like masks and social distancing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Do antigen tests require those invasive pharyngeal swabs that go deep up your nose?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Not necessarily. While &lt;a href=&quot;https://www.nytimes.com/2020/05/09/health/antigen-testing-fda-coronavirus.html&quot;&gt;Quidel&lt;/a&gt; and &lt;a href=&quot;https://www.nytimes.com/reuters/2020/07/06/us/06reuters-health-coronavirus-becton-dickinson.html&quot;&gt;Becton Dickinson&lt;/a&gt; are producing pharyngeal swab antigen tests, another company called &lt;a href=&quot;https://www.inquirer.com/news/spit-test-covid-coronavirus-orasure-fda-hiv--20200706.html&quot;&gt;OraSure&lt;/a&gt; is producing a saliva test. Will a saliva test be as accurate as a pharyngeal swab test? The evidence is mixed, with some studies suggesting it will be less accurate and others suggesting it will be more accurate (&lt;a href=&quot;https://www.medrxiv.org/content/10.1101/2020.04.16.20067835v1&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://www.medrxiv.org/content/10.1101/2020.05.26.20112565v1.full.pdf&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://jcm.asm.org/content/jcm/55/1/226.full.pdf&quot;&gt;3&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;I heard that &lt;a href=&quot;https://m.canadianinsider.com/sona-nanotech-announces-validation-results-for-its-covid-19-antigen-test&quot;&gt;SONA&lt;/a&gt; has an antigen test with 96% sensitivity. This sounds super promising!&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; This statistic is a bit misleading. The positive samples were generated in a laboratory. When antigen tests are done in real world clinical settings, the sensitivity is &lt;a href=&quot;https://www.technologyreview.com/2020/04/24/1000486/antigen-testing-could-faster-cheaper-diagnose-covid-19-coronavirus/&quot;&gt;lower&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Will we ever get a test that is both fast and sensitive?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; The New York Times had a great &lt;a href=&quot;https://www.nytimes.com/2020/07/06/health/fast-coronavirus-tests.html&quot;&gt;article&lt;/a&gt; last week on new types of point-of-care tests that are being developed, including &lt;a href=&quot;https://www.medrxiv.org/content/10.1101/2020.06.13.20129841v1.full.pdf&quot;&gt;HP-LAMP&lt;/a&gt; and &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2020.05.28.119131v1.full.pdf&quot;&gt;SHINE&lt;/a&gt;. These seem promising, although they are in an earlier stage of development than antigen tests, which already have several companies producing tests.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;A special thanks to &lt;a href=&quot;https://twitter.com/antonioregalado&quot;&gt;Antonio Regalado&lt;/a&gt; and &lt;a href=&quot;https://path.upmc.edu/personnel/faculty/Wells.htm&quot;&gt;Alan Wells&lt;/a&gt; for help answering some questions during my research.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Everything I've learned about solar storm risk and EMP attacks</title>
   <link href="http://localhost:4000/2020/06/18/everything-ive-learned-about-solar-storm-risk-and-emp-attacks/"/>
   <updated>2020-06-18T00:00:00-07:00</updated>
   <id>http://localhost:4000/2020/06/18/everything-ive-learned-about-solar-storm-risk-and-emp-attacks</id>
   <content type="html">&lt;p&gt;A few months ago, I &lt;a href=&quot;https://www.themoneyillusion.com/sunday-morning-quarterbacking/&quot;&gt;came across&lt;/a&gt; one of the &lt;a href=&quot;https://link.springer.com/article/10.1186/s13705-019-0199-y&quot;&gt;most extraordinary papers I have ever read&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In testimony before a Congressional Committee, it has been asserted that a prolonged collapse of this nation’s electrical grid—through starvation, disease, and societal collapse—could result in the death of up to 90% of the American population.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;According to the paper, the grid could be knocked out either by solar storms or by Electromagnetic Pulse (EMP) attacks. Solar storms are common. Serious storms like the &lt;a href=&quot;https://en.wikipedia.org/wiki/September_1859_geomagnetic_storm#:~:text=The%20September%201859%20geomagnetic%20storm,September%201%E2%80%932%2C%201859.&quot;&gt;1859 Carrington Event&lt;/a&gt; are expected to happen about once every 150 years.&lt;/p&gt;

&lt;p&gt;The paper continues:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;HV transformers are the weak link in the system, and the Federal Energy Regulatory Commission (FERC) has identified 30 of these as being critical. The simultaneous loss of just 9, in various combinations, could cripple the network and lead to a cascading failure, resulting in a “coast-to coast blackout”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If the HV transformers are irreparably damaged, they might not get replaced for 1-2 years, which would be devastating.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The great majority of these units are custom built. The lead time between order and delivery for a domestically manufactured HV transformer is between 12 and 24  months, and this is under benign, low demand conditions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To manage this risk, we could stockpile these transformers. An order of 30 of them would cost only $300M. But due to regulatory failure and the financial incentives of the utility industry, the paper claims, we are not stockpiling these transformers.&lt;/p&gt;

&lt;p&gt;When I read this paper, I was stunned. Is the risk of prolonged grid collapse really that high? And is it true that, just as the CDC &lt;a href=&quot;https://www.usatoday.com/story/news/factcheck/2020/04/03/fact-check-did-obama-administration-deplete-n-95-mask-stockpile/5114319002/&quot;&gt;failed to stockpile masks&lt;/a&gt; for a pandemic that we were all warned about, we are equally unprepared for a grid failure that could lead to societal collapse and mass starvation?&lt;/p&gt;

&lt;p&gt;To answer these questions, I did some homework. I read congressional testimony, think tank technical reports, a book, academic papers, insurance company assessments, several industry technical reports, and multiple reports in the trade media. What I found was at times contradictory. Somewhat troublingly, both sides of the issue accused each other of bias from financial incentives. Overall, my view is that while some of the EMP and solar storm risk is overhyped, it remains a serious issue, and one of the main tail risks we should be preparing for.&lt;/p&gt;

&lt;p&gt;I summarize my conclusions as a dialogue.&lt;/p&gt;

&lt;h2 id=&quot;dialogue&quot;&gt;Dialogue&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;If the power goes out for 12-24 months, will it really be so bad?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Yes, if the power goes out for that long — and that is a big “if” — the results would be catastrophic. Almost everything our modern society depends on, from water pumps to gasoline pumps to ATM machines, would all stop working. This means no water, no car, and no cash.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Jesus. What could cause these outages?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; The biggest risks are Solar Storms and EMP attacks.&lt;/p&gt;

&lt;h3 id=&quot;solar-storms&quot;&gt;Solar Storms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Let’s start with solar storms. First question: What is a solar storm?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; A solar storm is a temporary disturbance in the earth’s magnetic field, typically caused by coronal mass ejections (CMEs) from the sun.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;How can these storms disrupt the grid?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; The electromagnetic pulse can induce a quasi-direct current in power transmission lines, which can cause transformers to &lt;a href=&quot;http://www.firstempcommission.org/uploads/1/1/9/5/119571849/executive_report_on_assessing_the_threat_from_emp_-_final_april2018.pdf&quot;&gt;overheat&lt;/a&gt; and be permanently damaged. If enough of these transformers go down, we could be in serious long term trouble.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Are dangerous solar storms common?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Yes. The &lt;a href=&quot;https://en.wikipedia.org/wiki/March_1989_geomagnetic_storm&quot;&gt;March 1989 geomagnetic storm&lt;/a&gt; caused a nine-hour complete outage of the Hyro-Quebec grid. The &lt;a href=&quot;https://en.wikipedia.org/wiki/May_1921_geomagnetic_storm&quot;&gt;1921 Railroad Storm&lt;/a&gt; was &lt;a href=&quot;https://www.thespacereview.com/article/1553/1&quot;&gt;10 times stronger&lt;/a&gt; than the Quebec storm. And the &lt;a href=&quot;https://en.wikipedia.org/wiki/September_1859_geomagnetic_storm#:~:text=The%20September%201859%20geomagnetic%20storm,September%201%E2%80%932%2C%201859.&quot;&gt;1859 Carrington Event&lt;/a&gt; is estimated to be &lt;a href=&quot;https://republicans-oversight.house.gov/wp-content/uploads/2015/05/Pry-Statement-5-13-EMP.pdf&quot;&gt;10 times stronger&lt;/a&gt; than the 1921 storm. That is, the Carrington Event was 100 times stronger than the storm that took down the Quebec grid. Storms as large as the Carrington Event have a &lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/2016SW001470%4010.1002/%28ISSN%291542-7390.swqv14i1&quot;&gt;10% chance of happening every decade&lt;/a&gt;. Larger storms may be possible.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;This is terrifying! What hope do we have?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; The good news is that we will probably have advance warning. A team at the &lt;a href=&quot;https://www.swpc.noaa.gov/&quot;&gt;Space Weather Prediction Center&lt;/a&gt; (SWPC) monitors activity on the sun and activity in the earth’s magnetosphere. They can alert utilities of a CME with several hours to days notice, depending on its severity. This puts the utilities on high alert to turn off their transformers if needed.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_solar_storms/fig_swpc.png&quot; class=&quot;inner&quot; style=&quot;position:relative border:#222 2px solid; max-width:95%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Monitoring solar storms at the Space Weather Prediction Center 
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;That makes me feel better. But how reliable is this system?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Not as reliable as you might like. &lt;a href=&quot;https://www.thespacereview.com/article/1553/2&quot;&gt;One-third of major storms arrive unexpectedly&lt;/a&gt;, according to the SWPC’s own 2010 analysis. And that’s not just the small storms. According to a news article in &lt;a href=&quot;https://science.sciencemag.org.sci-hub.tw/content/324/5935/1640&quot;&gt;Science&lt;/a&gt;, the SWPC might be also be poor at identifying the characteristics of severe storms, since they are so rare.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Wait, what? Did you say we miss one-third of the storms? And that we might even miss the big ones? We’re screwed!&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Well, the good news is that even if the storm goes undetected, the transformers won’t fry up immediately. It will take several minutes for them to heat up. The utility companies can monitor them in realtime and turn them off if they get too hot (see &lt;a href=&quot;https://www.nerc.com/comm/PC/Geomagnetic%20Disturbance%20Task%20Force%20GMDTF%202013/Template_TOP.pdf&quot;&gt;NERC guidelines&lt;/a&gt; and &lt;a href=&quot;https://www.pjm.com/~/media/documents/manuals/m13.ashx&quot;&gt;Section 3.8.2 of this manual&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;I dunno, I watched that documentary on Chernobyl, and it seems like in emergency situations people aren’t great at following instructions in the manual.&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; I have my doubts as well. But from what I can tell, the utility companies will probably be able to execute if there is another Carrington Event, resulting in some unpleasant but short-term disruptions, but not a catastrophic long-term collapse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;I’m still not convinced.&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Well, even if the utility companies completely fail to react, many engineers &lt;a href=&quot;https://othjournal.com/wp-content/uploads/2019/08/EMP-Threats-to-Americas-Electric-Grid.pdf&quot;&gt;believe&lt;/a&gt; that the voltage collapse caused by a severe solar storm will ultimately save the transformers, since the circuit breakers will open automatically. But this is &lt;a href=&quot;https://othjournal.com/wp-content/uploads/2019/08/EMP-Threats-to-Americas-Electric-Grid.pdf&quot;&gt;disputed&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Overall, the combination of human intervention and automatic grid self-protection makes me feel that catastrophic long-term collapse from a solar storm is unlikely. But we should still take preparedness extremely seriously.&lt;/p&gt;

&lt;p&gt;Should we move on to EMP attacks?&lt;/p&gt;

&lt;h3 id=&quot;emp-attacks&quot;&gt;EMP Attacks&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Sure, what is an EMP attack?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; EMP stands for “electromagnetic pulse”. Just as EMPs can be emitted during a solar storm, they can also be man-made, in what is know as an “EMP attack”. There are a variety of ways to generate an EMP attack, but the one that worries people the most is a nuclear weapon detonated at high altitude. The pulse generated from such an attack could reach almost everywhere in the continental United States. Major nuclear powers like Russia and China have the ability to launch such an attack. More recently, potentially irrational adversaries like North Korea have pursued this capability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Should I be more worried about EMP Attacks or solar storms?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Opinions differ. But the EMP Commission is &lt;a href=&quot;https://republicans-oversight.house.gov/wp-content/uploads/2015/05/Pry-Statement-5-13-EMP.pdf&quot;&gt;more worried&lt;/a&gt; about EMP attacks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Why?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; There are two reasons. The first is that EMP attacks are unpredictable. Unlike with solar storms, where we will have several hours to days of advance notice, EMP attacks will happen immediately, giving us no time to prepare.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;That makes sense. What’s the second reason?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; The second reason is that &lt;a href=&quot;https://republicans-oversight.house.gov/wp-content/uploads/2015/05/Pry-Statement-5-13-EMP.pdf&quot;&gt;EMP attacks contain a high frequency component called an E1 field&lt;/a&gt;, which can potentially cause permanent damage to smaller electronics. Both EMP attacks and solar storms have an E3 field, which can heat up transformers. But only an EMP attack contains an E1 field.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;That sound scary. We should prepare for an EMP attack.&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; I agree. And the nice thing is that much of what we do to prepare for an EMP attack, like making our transformers more resilient, will also help us with a solar storm.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Given that this could potentially kill 90% of Americans and send the rest of us back to the Stone Age, why isn’t this our most important national priority?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Well, not everyone agrees that it’s that big a threat. Whereas the hawkish EMP Commission has been warning of prolonged national grid collapse, other people — often affiliated with the utility industry — have claimed that an EMP attack would only cause brief disruptions limited to a few states. The EMP Commission accuses the utility industry of &lt;a href=&quot;https://www.govinfo.gov/content/pkg/CHRG-114hhrg96952/html/CHRG-114hhrg96952.htm&quot;&gt;minimizing the risk to avoid liability&lt;/a&gt;. But EMP skeptics speculate that the EMP hawks are themselves &lt;a href=&quot;https://medium.com/war-is-boring/the-overrated-threat-from-electromagnetic-pulses-46e92c3efeb9&quot;&gt;motivated&lt;/a&gt; to maximize the &lt;a href=&quot;https://slate.com/technology/2019/05/emp-weapons-conservatives-trump-gingrich-huckabee.html&quot;&gt;profits&lt;/a&gt; of their own EMP books and protection companies. I tend to believe both sides are motivated by genuine conviction, but that’s still a relevant backdrop to the debate.&lt;/p&gt;

&lt;h3 id=&quot;the-2019-epri-report-and-its-critics&quot;&gt;The 2019 EPRI report and its critics&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;It’s terrifying that the predictions are so contradictory! How do we know who is right?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; We don’t know for sure. But in an attempt to settle the issue, the Electric Power Research Institute (EPRI) conducted a &lt;a href=&quot;https://www.epri.com/research/products/3002014979&quot;&gt;careful study&lt;/a&gt; over three years from 2016-2019. They concluded that while an EMP attack could do some damage to a few states, there was no risk of prolonged country-wide collapse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Wait a second… isn’t &lt;a href=&quot;https://www.epri.com/&quot;&gt;EPRI&lt;/a&gt; funded by the utility industry?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Indeed it is, a fact that EMP hawks are quick to point out. Veterans of the EMP Commission call the EPRI report “&lt;a href=&quot;https://www.newsmax.com/peterpry/electromagnetic-pulse-attack-defense/2019/12/04/id/944503/&quot;&gt;junk science&lt;/a&gt;”.&lt;/p&gt;

&lt;p&gt;But I am not so sure. While the &lt;a href=&quot;https://www.epri.com/research/products/3002014979&quot;&gt;EPRI report&lt;/a&gt; does leave some unanswered questions, it was still the most sophisticated and careful report I have read on the topic. And that’s not just my opinion. Sharon Burke, a former assistant secretary of defense for operational energy in the Obama administration, &lt;a href=&quot;https://www.wired.com/story/the-grid-might-survive-an-electromagnetic-pulse-just-fine/&quot;&gt;speaks highly of the report&lt;/a&gt;. “When you are doing documented research on physical systems, it is still solid evidence, no matter who paid for it. This is not someone’s opinion.” As for accusations of bias, it’s worth pointing out that the report was done in close consultation with leading experts from the DOE and national labs. Much of the data came directly from the government.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;So it sounds like the industry put together a high quality report. I’m still a little skeptical of it though. Could you give me some more detail about what they found?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Sure, I’ll start with their simplest and most optimistic finding, but then I’ll get into some of the more mixed (and interesting!) results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;OK, what was their simplest and most optimistic finding?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Remember how, unlike activity from solar storms, EMP attacks create something called an E1 field? One of risks of an E1 field is that key electrical devices known as “digital protective relays” (DPRs) can be damaged, which could cause major outages. To test this hypothesis, the EPRI report authors subjected 17 different types of DPRs to E1 fields of different intensities, ranging from 0 kV/m to 50 kV/m. They found that none of the DPRs were permanently damaged under any of these intensities. Some of the devices became temporarily disabled, but they could be easily restarted with a power cycle.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_solar_storms/fig_testing.png&quot; class=&quot;inner&quot; style=&quot;position:relative border:#222 2px solid; max-width:95%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Setup for EPRI's free field tests on DPRs. [&lt;a href=&quot;https://www.epri.com/research/products/3002014979&quot;&gt;EPRI&lt;/a&gt;]
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;OK, so these DPR devices could withstand EMPs up to 50 kV/m, but how does this compare to the intensities that could come from an actual EMP attack?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Even with a giant 1000 kT nuclear weapon, the peak field once it reaches the ground is only 25 kV/m. The EPRI report tested a field twice as high.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;What were some of the other findings in the EPRI report?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; So here’s where it gets a little bit more pessimistic, but still nothing close to prolonged country-level grid collapse. EMP pulses are risky not only because they can do direct damage to DPRs. They can also induce voltage surges, which can in turn cause damage to electric equipment.&lt;/p&gt;

&lt;p&gt;To test the amount of damage caused by surges, the authors injected direct voltage into the inputs of the devices…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Wait, why did they inject the voltage rather than using an EMP to naturally induce the voltage?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; That would require radiating the EMP over a large area, which they said was “not practical”. I’m not an expert, but this seems to be a standard practice for EMP testing.&lt;/p&gt;

&lt;p&gt;Anyway, in their voltage injection tests they found that under sufficient voltage, the inputs to the DPRs could become permanently damaged. Some devices required voltages as high as 80 kV to be damaged. Others required voltages as low as 5 kV.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Did you say kV? Weren’t you earlier talking about kV/m?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Yes, for these direct voltage injection experiments, the relevant unit is the voltage measured in kV. For the earlier experiment, which measured the impact of the free field pulse on the DPR, the relevant unit is kV/m.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;If these device inputs can be permanently damaged by a 5 kV surge, does this mean they will be damaged by a nuclear EMP attack?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; It depends! This brings us to the coolest part of the paper. The amount of voltage induced in a wire depends on a lot of things. First, it depends on the strength of the electromagnetic field. This in turn depends on the distance from the detonation site. So even if a giant nuclear weapon is able to create a peak field of 50 kV/m, the field will be weaker at more distant locations.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_solar_storms/fig_detonation_field.png&quot; class=&quot;inner&quot; style=&quot;position:relative border:#222 2px solid; max-width:95%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: The magnitude of the E1 field decreases with distance from the detonation site. [&lt;a href=&quot;https://www.epri.com/research/products/3002014979&quot;&gt;EPRI&lt;/a&gt;]
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;But even more interestingly, the induced voltage also depends on the polarization of the field, the incidence angle (psi), and the azimuthal angle (phi). Depending on where the input power line is located and how it is oriented, the actual induced voltage might be much lower than one would expect under maximal conditions.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_solar_storms/fig_orientation.png&quot; class=&quot;inner&quot; style=&quot;position:relative border:#222 2px solid; max-width:95%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: The induced voltage in a wire depends on its orientation relative to the incoming electromagnetic field. [&lt;a href=&quot;https://www.epri.com/research/products/3002014979&quot;&gt;EPRI&lt;/a&gt;]
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Based on the known locations of substations throughout the US (and assuming random orientations), the authors were able to estimate the distribution of induced voltages in these substations, assuming a very strong nuclear attack. In most cases the induced voltages would be less than 10 kV, but some would be even higher.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;10 kV? Isn’t that above the 5 kV damage threshold for some device inputs?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Yes, for some of the devices.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;So given that different devices will experience different voltage surges and given that different devices will have different voltages thresholds, is there a way we can calculate the percentage of device inputs that would be damaged?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Yes! With simulations. While the authors don’t know the type and orientation of each device out in the world, they can make reasonable assumptions that these are uniformly distributed over known substation locations. For each device the authors sampled an induced voltage from the distribution of induced voltage (this depends on the device’s location and on a randomly sampled orientation). They also sampled a damage threshold from a distribution they constructed from their empirical voltage injection tests. For each device in the simulation, if the sampled induced voltage was greater than the damage threshold, they marked the device as damaged.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_solar_storms/fig_simulations.png&quot; class=&quot;inner&quot; style=&quot;position:relative border:#222 2px solid; max-width:95%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: To estimate how many DPRs would be damaged by voltage surges, the authors drew independent samples from their simulated distribution of voltage surges (Stress PDF) and their empirically measured distribution of damage thresholds (Strength PDF). [&lt;a href=&quot;https://www.epri.com/research/products/3002014979&quot;&gt;EPRI&lt;/a&gt;]
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;That seems vaguely reasonable. What were the results?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; It depended on how much protection they assumed and the intensity of the attack, but the fraction of damaged devices ranged from 1% to 20%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Is that good or bad?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; I can’t get a clear answer on that. The tone of the EPRI report implied that this wasn’t a big deal, but a &lt;a href=&quot;https://othjournal.com/wp-content/uploads/2019/08/EMP-Threats-to-Americas-Electric-Grid.pdf&quot;&gt;rebuttal&lt;/a&gt; from the Electromagnetic Defense Task Force (EDTF) said this was quite serious:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Relay malfunction during a HEMP attack would likely cause other electric grid systems to fail, resulting in large-scale cascading blackouts and widespread equipment damage. Notably, E1 effects on protective relays are likely to interrupt substation self-protection processes needed to interrupt E3 current flow through transformers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Interesting. That reminds me, you’ve talked a lot about EPRI’s analysis of the E1 field, both directly via free field and indirectly via voltage surges. But what about the E3 field? If I recall, that was the thing that could heat up and damage transformers, right?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Yes, the authors looked into this as well. While they didn’t physically test any transformers, they were able to simulate the effects of E3 fields on about a thousand transformers, using information they had about the age and quality of each transformer. Of the thousand transformers they simulated, only 3 to 21 would experience damage. These damaged transformers would be geographically dispersed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;3 to 21 transformers would experience damage? Couldn’t that be quite bad? Didn’t you say earlier that &lt;a href=&quot;https://fas.org/sgp/crs/homesec/R43604.pdf&quot;&gt;9 critical transformers could take down the whole grid&lt;/a&gt;?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; I can’t get a clear answer on this either, but I think it’s ok. The EPRI report made this sound like not a big deal, and the &lt;a href=&quot;https://othjournal.com/wp-content/uploads/2019/08/EMP-Threats-to-Americas-Electric-Grid.pdf&quot;&gt;most comprehensive critique of the EPRI report&lt;/a&gt; did not bring it up in its list of concerns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;What are some criticisms of the EPRI Report?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; &lt;a href=&quot;https://othjournal.com/wp-content/uploads/2019/08/EMP-Threats-to-Americas-Electric-Grid.pdf&quot;&gt;The Electromagnetic Defense Task Force (EDTF)&lt;/a&gt; includes dozens of critiques, but a few that jump out are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The EPRI report did not do any physical tests on transformers, relying only on simulations.&lt;/li&gt;
  &lt;li&gt;The transformer simulations assumed an E3 field of 24 V/km, when the DHS &lt;a href=&quot;https://www.cisa.gov/sites/default/files/publications/19_0307_CISA_EMP-Protection-Resilience-Guidelines.pdf&quot;&gt;recommends&lt;/a&gt; protection up to 85 V/km, based on Soviet data.&lt;/li&gt;
  &lt;li&gt;The report only tested substation equipment, but did not test equipment in power generators or in distribution areas.&lt;/li&gt;
  &lt;li&gt;As mentioned above, damage to 5% of equipment could cause cascading failures that could disable other protective equipment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Now I’m confused and don’t know who to believe!&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; I agree, it’s not great. Apparently, EPRI did not coordinate with the Congressional EMP Commission to compare results and methodology. Many of the test results are inconsistent with the &lt;a href=&quot;http://www.empcommission.org/docs/A2473-EMP_Commission-7MB.pdf&quot;&gt;EMP Commission’s own test results from 2008&lt;/a&gt;, and I don’t believe anybody has resolved the discrepancies. So there is a troubling lack of communication here.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;I’m feeling overwhelmed with all this information about EMPs! Can you summarize what we’ve learned?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Sure.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The EMP Commission believes EMP attacks are more dangerous than solar storms, because they are unpredictable and because they include an E1 field, not just an E3 field.&lt;/li&gt;
  &lt;li&gt;The industry-sponsored &lt;a href=&quot;https://www.epri.com/research/products/3002014979&quot;&gt;EPRI report&lt;/a&gt;, which seems to be carefully done, claims that an EMP attack will at most cause temporary disruptions in a few states. The E1 field itself will not cause any permanent damage to DPR devices, although a small fraction of devices may be damaged by E1-induced voltage surges. The vast majority of transformers will survive the E3-induced temperature increases. The few transformers that may fail will be geographically dispersed.&lt;/li&gt;
  &lt;li&gt;However, a &lt;a href=&quot;https://othjournal.com/wp-content/uploads/2019/08/EMP-Threats-to-Americas-Electric-Grid.pdf&quot;&gt;rebuttal report from the EDTF&lt;/a&gt; raised many concerns with the EPRI report, including concerns about cascading failures in complex systems. The discrepancies between EPRI and EDTF, as well as the discrepancies between EPRI field tests and previous tests by the EMP Commission, have not been fully resolved.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;the-chances-of-an-emp-attack&quot;&gt;The chances of an EMP attack&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;I’ve heard a lot from you about how dangerous an EMP attack could be. But how likely is it that anyone will actually try to attack us with an EMP?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Unlike the EMP Commission, most national security experts view EMP attacks as a &lt;a href=&quot;https://medium.com/war-is-boring/the-overrated-threat-from-electromagnetic-pulses-46e92c3efeb9&quot;&gt;second rate threat&lt;/a&gt;. While perhaps some small terrorist groups or rogue nations might launch a localized EMP attack that might take out a substation or two, &lt;a href=&quot;https://www.thespacereview.com/article/1553/1&quot;&gt;it’s unlikely that any country capable of launching a large-scale EMP attack would actually do so&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Why not?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Because to launch a large-scale EMP attack, a country would need a large nuclear weapon. And if a country was planning on using a large nuclear weapon, it would make more sense — in the morbid logic of war — to conventionally drop it on a city than to launch an EMP attack which would at most cause some brief power disruptions in a few states. As physicist Yousaf Butt &lt;a href=&quot;https://www.thespacereview.com/article/1553/1&quot;&gt;put it&lt;/a&gt;, “A weapon of mass destruction is preferable to a weapon of mass disruption”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Interesting. So while EMP attacks could be serious, they are less dangerous than advertised, and it’s unlikely that anyone would ever want to launch a major attack.&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; What’s especially interesting is that because EMP attacks are unlikely, and because they will probably only cause transient disruption even if they do happen, some experts believe that the &lt;a href=&quot;https://www.thespacereview.com/article/1553/1&quot;&gt;bigger threat is solar storms&lt;/a&gt;! The EMP Commission, from this perspective, had their priorities backward.&lt;/p&gt;

&lt;h3 id=&quot;what-are-we-doing-about-all-of-this&quot;&gt;What are we doing about all of this?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;What is the United States doing to protect itself against solar storms and EMP attacks?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; According to the EMP Commission, not nearly enough. The EMP Commission members are &lt;a href=&quot;https://republicans-oversight.house.gov/wp-content/uploads/2015/05/Pry-Statement-5-13-EMP.pdf&quot;&gt;very critical&lt;/a&gt; of the regulatory environment, which they view as dysfunctional. FERC, the government agency that regulates utilities, does not have the power to make the utilities protect the grid. All that FERC can do is ask the industry umbrella group (NERC) to propose an EMP protection standard, but the NERC standard is determined by industry representatives and is therefore too weak.&lt;/p&gt;

&lt;p&gt;Furthermore, there is confusion about which government agency is responsible for EMP protection. From &lt;a href=&quot;https://www.govinfo.gov/content/pkg/CHRG-114hhrg96952/html/CHRG-114hhrg96952.htm&quot;&gt;George Baker’s testimony before Congress&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When I ask NERC officials about EMP protection, they informed me we don’t do EMP, that’s DOD’s responsibility. The Department of Defense tells me, EMP protection for civilian infrastructure is DHS’s responsibility. And then when I talk to DHS, I get answers that the protection should be done by the Department of Energy, since they are the infrastructure’s sector-specific agency. So we have EMP and GMD protection as finger-pointing exercises at present.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;That sounds bad.&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; Agreed. The good news is that in March 2019 the EMP hawks &lt;a href=&quot;https://thehill.com/opinion/national-security/436224-finally-a-presidential-emp-order-that-may-save-american-lives&quot;&gt;got their wish&lt;/a&gt;, when President Trump signed an &lt;a href=&quot;https://www.whitehouse.gov/presidential-actions/executive-order-coordinating-national-resilience-electromagnetic-pulses/&quot;&gt;executive order&lt;/a&gt; putting the White House in charge of national EMP preparedness, rather than relying on scattered federal agencies. EMP Commission chairman Peter Pry &lt;a href=&quot;https://thehill.com/opinion/national-security/436224-finally-a-presidential-emp-order-that-may-save-american-lives&quot;&gt;hailed the executive order&lt;/a&gt;, describing it as an “excellent first step”.&lt;/p&gt;

&lt;p&gt;Even EMP skeptics expressed support for the executive order. Former staff member of the Senate Armed Services Committee Gregory T. Kiley &lt;a href=&quot;https://thehill.com/opinion/cybersecurity/437687-putting-president-trumps-executive-order-on-electromagnetic-pulses-emp&quot;&gt;wrote&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;EMPs are by no means one of the top-tier national security challenges, nor the most pressing concern for the safety of our electrical grid. A careful and reasoned plan put forward, like what we saw last week from the White House, makes sense.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;What’s happened so far with the executive order?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; According to &lt;a href=&quot;https://www.politico.com/news/2019/11/18/is-it-lights-out-for-trumps-emp-push-071359&quot;&gt;Politico&lt;/a&gt;, the process has been disrupted due to the departure of several key advocates from the administration. Peter Pry &lt;a href=&quot;https://thehill.com/opinion/national-security/436224-finally-a-presidential-emp-order-that-may-save-american-lives&quot;&gt;warns&lt;/a&gt; of “inevitable opposition from recalcitrant lobbyists and bureaucrats”. That said, the National Defense Authorization Act passed by Congress this year includes some laws regarding EMP protection.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;How much will it cost to better prepare us for EMPs and Solar Storms?&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; It &lt;a href=&quot;https://www.govinfo.gov/content/pkg/CHRG-114hhrg96952/html/CHRG-114hhrg96952.htm&quot;&gt;depends what the plan is&lt;/a&gt;. The bare minimum is to spend \$200M to protect the extra-high-voltage transformers. The EMP Commission proposed a \$2B plan to protect all the transformers and generators. George Baker has a much more expensive plan at \$30B. That might sound like a lot, but if you amortize it over several years, it would come out to a \$2-3 charge in your monthly utility bill.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; &lt;em&gt;Getting back to how you started this blog post, you mentioned that some key high voltage transformers take 1-2 years to produce, and that we should therefore stockpile about 30 of them, at a cost of about $300 million.&lt;/em&gt;
&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; I was surprised to hear that Peter Pry, of the hawkish EMP Commission, does not support this plan. According to him, it would be much cheaper to just put surge arrestors on our active transformers. Furthermore, many transformers are custom-built for individual substations, so it will not be easy to produce stockpiled interoperable transformers that can work at any substation.&lt;/p&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Q.&lt;/strong&gt; &lt;em&gt;What’s your overall take on this?&lt;/em&gt;&lt;br /&gt; &lt;strong&gt;A:&lt;/strong&gt; The ‘mainstream’ belief is that solar storms and EMP attacks are concerning but overhyped. While almost everyone agrees that we should make our grid more resilient, many say the doomsday scenarios are extremely unlikely. Most national security experts think that countries capable of launching a major EMP attack will not want to launch one, although smaller attacks from rogue actors may be possible. Consistent with the reasonably careful EPRI report, the mainstream view is that the impact of even a major EMP attack will likely be short-term and regional, rather than long-term and national. As for solar storms, the utility companies will have plenty of advance warning to respond.&lt;/p&gt;

&lt;p&gt;Other people, especially those who have worked on the EMP Commission, are much more worried about these risks. They point to the fact that hundreds of grid components have not been thoroughly tested, and that problems in one part of the grid can cascade to other parts in unpredictable ways. The media has at times portrayed these people negatively. In an article that I thought was irresponsible, &lt;a href=&quot;https://slate.com/technology/2019/05/emp-weapons-conservatives-trump-gingrich-huckabee.html&quot;&gt;Slate&lt;/a&gt; called the EMP concern “right-wing fretting” and a conservative “fixation”. &lt;a href=&quot;https://www.wired.com/story/the-grid-might-survive-an-electromagnetic-pulse-just-fine/&quot;&gt;Wired&lt;/a&gt; says that the EMP arena is filled with a lot of “hype” and “fearmongering”.&lt;/p&gt;

&lt;p&gt;My own view is that while the ‘mainstream’ view is probably correct, and while there certainly has been some fearmongering, I am philosophically aligned with the alarmists. The mainstream belief at NASA in 1986 was that the Challenger was safe. The mainstream belief at Chernobyl in 1986 was that the reactor core could never rupture. The mainstream belief on Wall Street in 2007 was that mortgage-backed securities were safe.&lt;/p&gt;

&lt;p&gt;Now that we have seen our preparedness level for Covid-19, who are you going to believe: The people saying “Don’t worry, we have this unpredictable and complex system under control” or the people waving their hands and shouting “correlated risk!” I’m with the people shouting “correlated risk!”, even though they’ll probably end up being wrong.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Coronavirus and fragmented data pipelines</title>
   <link href="http://localhost:4000/2020/04/14/fragmented-data-pipelines/"/>
   <updated>2020-04-14T00:00:00-07:00</updated>
   <id>http://localhost:4000/2020/04/14/fragmented-data-pipelines</id>
   <content type="html">&lt;p&gt;Anybody looking at coronavirus data right now must feel very confused. The UK has a daily case count &lt;a href=&quot;https://twitter.com/jburnmurdoch/status/1249444701280878592/photo/1&quot;&gt;60 times higher&lt;/a&gt; than Australia. Italy has a case fatality rate &lt;a href=&quot;https://www.cebm.net/covid-19/global-covid-19-case-fatality-rates/&quot;&gt;3 times higher&lt;/a&gt; than nearby Greece and &lt;a href=&quot;https://www.cebm.net/covid-19/global-covid-19-case-fatality-rates/&quot;&gt;12 times higher&lt;/a&gt; than Pakistan. These &lt;a href=&quot;https://marginalrevolution.com/marginalrevolution/2020/03/where-does-all-the-heterogeneity-come-from.html&quot;&gt;heterogeneities&lt;/a&gt; seem massive and have the potential to teach us critical insights about the disease. But as any epidemiologist will &lt;a href=&quot;https://fivethirtyeight.com/features/a-comic-strip-tour-of-the-wild-world-of-pandemic-modeling/&quot;&gt;readily&lt;/a&gt; &lt;a href=&quot;https://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0003846&quot;&gt;acknowledge&lt;/a&gt;, these statistics are terribly confounded by inconsistent reporting protocols and variable testing ability, both of which could be driving inter-country differences of 10x or more.&lt;/p&gt;

&lt;p&gt;In this blog post, I want to talk about some of the enormous issues with data quality and why we have them. And I want to describe how we can go beyond merely acknowledging the biases, and instead focus on the policy changes that can fix them.&lt;/p&gt;

&lt;h3 id=&quot;testing-count&quot;&gt;Testing count&lt;/h3&gt;
&lt;p&gt;The first issue is that different countries and different states vary greatly in how much testing they do. Iceland has tested &lt;a href=&quot;https://www.covid.is/data&quot;&gt;10% of its population&lt;/a&gt;, whereas India has only tested &lt;a href=&quot;https://ourworldindata.org/covid-testing&quot;&gt;0.1% of its population&lt;/a&gt;. Obviously these differences in testing will drive differences in reported case counts.&lt;/p&gt;

&lt;p&gt;Given differences in economic development, it’s understandable that different countries will have different testing counts. What’s &lt;em&gt;not&lt;/em&gt; acceptable is that so many countries &lt;a href=&quot;https://ourworldindata.org/covid-testing&quot;&gt;don’t report their testing counts&lt;/a&gt;! They just report the number of positive tests! Even among the countries that report both numbers, many greatly underreport the testing count because they rely on commercial labs that do not provide these records.&lt;/p&gt;

&lt;p&gt;This isn’t just an issue for developing countries. Looking just at the United States, economically advanced states like California, Washington, and New York, have &lt;a href=&quot;https://covidtracking.com/about-data&quot;&gt;not regularly reported&lt;/a&gt; their total number of people tested. New York has, at various times, &lt;a href=&quot;https://covidtracking.com/about-data/faq&quot;&gt;started and stopped&lt;/a&gt; reporting negative results.&lt;/p&gt;

&lt;p&gt;If we wish to make sensible comparisons of infection rates across regions, it is utterly important to know how many people were tested in each region, especially if there may be 10x or 100x test rate differences across regions. I do not want to be too critical here, but it is astonishing to me that we cannot produce this data.&lt;/p&gt;

&lt;h3 id=&quot;mix-shift-in-reasons-for-testing&quot;&gt;Mix shift in reasons for testing&lt;/h3&gt;
&lt;p&gt;It is not enough to report just the number of tests and the number of positive results. To estimate the true infection rates, we must also know why each test was done. Imagine one country only tests symptomatic people, whereas another country tests symptomatic people and high risk non-symptomatic people (e.g. health care workers). And imagine that both countries do the same number of tests. Even though both countries do the same number of tests, you can’t calculate the true infection count by simply dividing the positive tests by the test rate. For a reasonable estimate, you need to divide the positives in each stratum by the testing rate in each stratum, and then sum them up.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.cdc.gov/coronavirus/2019-ncov/downloads/pui-form.pdf&quot;&gt;CDC case report form&lt;/a&gt; has a field for why each test was done (&lt;em&gt;Figure 1&lt;/em&gt;). Unfortunately, many states do not use this form and instead use their own form. (To be fair, some states like Washington use a &lt;a href=&quot;https://www.doh.wa.gov/Portals/1/Documents/5100/420-110-ReportForm-Coronavirus.pdf&quot;&gt;more comprehensive form&lt;/a&gt;, but there is still a lack of standardization.) Moreover, none of the states to my knowledge report the total number of people in each stratum (tested and untested), which would be necessary to do a stratified analysis.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_covid_data_quality/cdc_form_strata.png&quot; class=&quot;inner&quot; style=&quot;position:relative border:#222 2px solid; max-width:95%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: A section from the &lt;a href=&quot;https://www.cdc.gov/coronavirus/2019-ncov/downloads/pui-form.pdf&quot;&gt;CDC case report form&lt;/a&gt; which could be used for stratification. 
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;why-dont-we-do-this-already&quot;&gt;Why don’t we do this already?&lt;/h3&gt;
&lt;p&gt;None of this is groundbreaking stuff. Epidemiologists know about &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/10366179&quot;&gt;stratification&lt;/a&gt; and are &lt;a href=&quot;https://journals.plos.org/plosntds/article?id=10.1371/journal.pntd.0003846&quot;&gt;keenly aware&lt;/a&gt; of the limitations of crude infection counts and crude case fatality rates.&lt;/p&gt;

&lt;p&gt;This isn’t a knowledge problem. Instead, this is caused by some combination of three factors.&lt;/p&gt;

&lt;p&gt;First, there is a natural tendency among data professionals to focus more on modeling than on upstream data quality issues. My own field of data science is certainly guilty of this. I am not saying data quality has received no attention. I am just saying that if epidemiology is anything like data science, then data quality issues get less attention than they deserve.&lt;/p&gt;

&lt;p&gt;Second, our public health infrastructure is &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/books/NBK221231/&quot;&gt;fragmented&lt;/a&gt; in a very particular way. Internationally, the WHO has no jurisdiction over individual countries and can only ask them to “&lt;a href=&quot;https://apps.who.int/iris/bitstream/handle/10665/331509/WHO-COVID-19-lab_testing-2020.1-eng.pdf&quot;&gt;consider reporting&lt;/a&gt;” their data. In the US, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Tenth_Amendment_to_the_United_States_Constitution&quot;&gt;Tenth Amendment&lt;/a&gt; requires most public health work to be &lt;a href=&quot;https://www.cdc.gov/phlp/docs/APHL_Conference_LEI_Report_508.pdf&quot;&gt;run by the states&lt;/a&gt; rather than the federal government, and the CDC can therefore not compel states to report data or to use its forms. This is clearly a collective action that people have &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/books/NBK221231/&quot;&gt;warned about for decades&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Third, it’s really hard. It is currently quite onerous to fill out a report for &lt;a href=&quot;https://www.cdc.gov/coronavirus/2019-ncov/downloads/pui-form.pdf&quot;&gt;negative results&lt;/a&gt;, although one could imagine a world where negative cases were easy to report.&lt;/p&gt;

&lt;h3 id=&quot;the-three-levels-of-data-org-maturity-and-a-dream-for-the-future&quot;&gt;The three levels of data org maturity, and a dream for the future&lt;/h3&gt;
&lt;p&gt;In my field of data science, you can determine the maturity of data organizations by their outlook on upstream data quality issues. There are roughly three levels.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_covid_data_quality/data_maturity_table.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:95%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Table 1&lt;/strong&gt;: Levels of data maturity for a data organization. 
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Our public health infrastructure is at Level II. My strong belief, and I mean this in as constructive a way as possible, is that Level II is unacceptable. We need to be at Level III before the next pandemic hits.&lt;/p&gt;

&lt;p&gt;I want us to live in a world where every country reports positive case counts and total test counts to the WHO, stratified by test reason, and using standardized easy-to-use technology. I want to live in a world where every state reports the same information to the CDC.&lt;/p&gt;

&lt;p&gt;How do we get there? Here are some thoughts.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A cultural shift among researchers towards the hard work on upstream data quality issues. And yes, &lt;a href=&quot;https://en.wikipedia.org/wiki/Replication_crisis&quot;&gt;cultural shifts are possible&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Financial incentives for better case reporting. For example, while the CDC cannot legally compel states to do adequate reporting, it does provide financial assistance via the &lt;a href=&quot;https://www.cdc.gov/ncezid/dpei/epidemiology-laboratory-capacity.html&quot;&gt;ELC Cooperative Agreement&lt;/a&gt;. The CDC could use this program as a financial carrot for better reporting, similar to how they have used other cooperative agreements to &lt;a href=&quot;https://www.cdc.gov/cancer/npcr/pdf/npcr_standards.pdf&quot;&gt;incentivize participation standards in cancer registries&lt;/a&gt;. One can imagine similar financial arrangements at the international level.&lt;/li&gt;
  &lt;li&gt;Technology. Even today many health providers fax case reports to state agencies, where someone then converts the data by hand into a CDC report. &lt;a href=&quot;https://www.healthit.gov/sites/default/files/hie-interoperability/Roadmap-Executive%20Summary-100115-4pm.pdf&quot;&gt;The CDC is aware of the need for interoperable technology&lt;/a&gt;, but clearly this work needs to be prioritized and accelerated.&lt;/li&gt;
  &lt;li&gt;Funding for public health agencies to make this all possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the meantime, please consider &lt;a href=&quot;https://ourworldindata.org/covid-testing&quot;&gt;supporting Our World In Data&lt;/a&gt; or following the Covid Tracking Project’s &lt;a href=&quot;https://covidtracking.com/help&quot;&gt;recommendation to contact your state public health authority&lt;/a&gt;. Both of these organizations have made a herculean effort to compile incomplete data.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>The shower problem</title>
   <link href="http://localhost:4000/2020/02/08/the-shower-problem/"/>
   <updated>2020-02-08T00:00:00-08:00</updated>
   <id>http://localhost:4000/2020/02/08/the-shower-problem</id>
   <content type="html">&lt;p&gt;&lt;em&gt;Attention mathematicians and computer scientists:&lt;/em&gt; I’ve got a problem for you, and I don’t know the solution.&lt;/p&gt;

&lt;p&gt;Here’s the setup: You’re at your friend’s place and you need to take a shower. The shower knob is unlabeled. One direction is hot and the other direction is cold, and you don’t know which is which.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_shower_problem/shower_knob.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:40%;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;You turn it to the left. It’s cold. You wait.&lt;/p&gt;

&lt;p&gt;At what point do you switch over to the right?&lt;/p&gt;

&lt;h3 id=&quot;the-baseline-shower-problem&quot;&gt;The baseline shower problem&lt;/h3&gt;
&lt;p&gt;Let’s make this more explicit.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Your goal is to find a policy that minimizes the expected amount of time it takes to get hot water flowing out of the shower head. To simplify things, assume that the water coming out of the head is either hot or cold, and that the lukewarm transition time is effectively zero.&lt;/li&gt;
  &lt;li&gt;You know that the shower has a Time-To-Hot constant called &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt;. This value is defined as the time it takes for hot water to arrive, assuming you have turned the knob to the hot direction and keep it there.&lt;/li&gt;
  &lt;li&gt;The &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; constant is a fixed property of the shower and is sampled once from a known distribution. You have certain knowledge of the distribution, but you don’t know &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;The shower is memoryless, such that every time you turn the knob to the hot direction, it will take &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; seconds until the hot water arrives, regardless of your prior actions. Every time you turn it to the cold direction, only cold water will come out.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_shower_problem/distributions.gif&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Unbeknownst to the user, the hot water direction is to the left, with a time constant τ of 100 seconds. The user knows the probability distribution over τ, and follows a strategy of eliminating segments of that distribution. They initially guess the correct direction, but give up too soon at 75 seconds. They then spend another 75 seconds on the rightwards direction. Finally, they return to the left direction, passing through the 75 seconds they had already eliminated, and then finally getting the hot water after 100 seconds on the left direction. In all, it takes the user 250 seconds to find the hot water.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I don’t know how to solve this problem. But as a starting point I realize it’s possible to keep track of the probability that the hot direction is to the left or to the right. In the animation above, the probability that the hot direction is to the right is just the unexplored white area under the right curve, divided by the total unexplored white area of both curves.&lt;/p&gt;

&lt;p&gt;But how do you turn that into a policy for exploring the space? Does anybody know?&lt;/p&gt;

&lt;h3 id=&quot;submissions&quot;&gt;Submissions&lt;/h3&gt;
&lt;p&gt;If you would like to submit a proposal, please report your average duration for the sample of 20,000 &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt;’s provided &lt;a href=&quot;https://gist.github.com/csaid/a57c4ebaa1c7b0671cdc9692638ea4c4&quot;&gt;here&lt;/a&gt;. Currently, &lt;a href=&quot;https://twitter.com/Cmrn_DP&quot;&gt;Cameron Davidson-Pilon&lt;/a&gt; is in the lead with an average duration of &lt;a href=&quot;https://gist.github.com/CamDavidsonPilon/be1333d348865fbf1ab13c409e849ee2&quot;&gt;111.365 seconds&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;bonus-problem-plumbing-realities-and-the-elusive-middle-solution&quot;&gt;Bonus problem: Plumbing realities and the elusive “Middle Solution”&lt;/h3&gt;
&lt;p&gt;The baseline shower problem assumes a simplified version of reality, where the shower is memoryless and there is only a single pipe. If you want a harder problem, I have written a comment below that describes some of the plumbing realities, including lag and the existence of separate hot and cold pipes. The comment explores the tantalizing possibility that we’ve all been fiddling with our showers wrong this whole time. Instead of swinging the knob between one extreme and the other, what if the optimal solution is to start by putting the knob in the middle? To read more, see the comment below.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Optimizing sample sizes in A/B testing, Part III&#58; Aggregate time-discounted expected lift</title>
   <link href="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/"/>
   <updated>2020-01-10T00:00:00-08:00</updated>
   <id>http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III</id>
   <content type="html">&lt;div class=&quot;caption&quot;&gt;
This is Part III of a three part blog post on how to optimize your sample size in A/B testing. Make sure to read &lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/&quot;&gt;Part I &lt;/a&gt; and &lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/&quot;&gt;Part II&lt;/a&gt; if you haven't already.
&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;In &lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/&quot;&gt;Part II&lt;/a&gt;, we learned how before the experiment starts we can estimate $\hat{L}$, the expected post-experiment lift, a probability weighted average of outcomes.&lt;/p&gt;

&lt;p&gt;In Part III, we’ll discuss how to estimate what is perhaps the most important per-unit cost of experimentation: the forfeited benefits that are lost by delayed shipment. This leads to something I think is incredibly cool: A formula for the &lt;em&gt;aggregate time-discounted expected post-experiment lift&lt;/em&gt; as a function of sample size. We call this quantity $\hat{L}_a$. The formula for $\hat{L}_a$ allows you to pick optimal sample sizes specific to your business circumstances. We’ll cover two examples in Python, one where you are testing a continuous variable, and one where you are testing a binary variable (as in conversion rate experiments).&lt;/p&gt;

&lt;p&gt;As usual, the focus will be on choosing a sample size at the beginning of the experiment and committing to it, not on dynamically updating the sample size as the experiment proceeds.&lt;/p&gt;

&lt;h2 id=&quot;a-quick-modification-from-part-ii&quot;&gt;A quick modification from Part II&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/&quot;&gt;Part II&lt;/a&gt;, we saw that if you ship whichever version (A or B) does best in the experiment, your business will on average experience a post-experiment per-user lift of&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{n})}}&lt;/script&gt;

&lt;p&gt;where $\sigma_\Delta^2$ is the variance on your normally distributed zero-mean prior for $\mu_B - \mu_A$, $\sigma_X^2$ is the within-group variance, and $n$ is the per-bucket sample size.&lt;/p&gt;

&lt;p&gt;Because Part III is primarily concerned with the duration of the experiment, we’re going to modify the formula to be time-dependent. As a simplifying assumption we’re going to make &lt;em&gt;sessions&lt;/em&gt;, rather then &lt;em&gt;users&lt;/em&gt;, the unit of analysis. We’ll also assume that you have a constant number of sessions per day. This changes the definition of $\hat{L}$ to a &lt;em&gt;post-experiment per-session lift&lt;/em&gt;, and the formula becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{m\tau})}}&lt;/script&gt;

&lt;p&gt;where $m$ is the sessions per day for each bucket, and $\tau$ is duration of the experiment in days.&lt;/p&gt;

&lt;h2 id=&quot;time-discounting&quot;&gt;Time Discounting&lt;/h2&gt;

&lt;p&gt;The formula above shows that larger sample sizes result in higher $ \hat{L} $, since larger samples make it more likely you will ship the better version. But as with all things in life, there are costs to increasing your sample size. In particular, the larger your sample size, the longer you have to wait to ship the winning bucket. This is bad because lift today is much more valuable than the same lift a year from now.&lt;/p&gt;

&lt;p&gt;How much more valuable is lift today versus lift a year from now? A common way to quantify this is with exponential discounting, such that weights (or “discount factors”) on future lift follow the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w = e^{-rt}&lt;/script&gt;

&lt;p&gt;where $ r $ is a discount rate. For startup teams, the annual discount rate might be quite large, like 0.5 or even 1.0, which would correspond to a daily discount rate $r$ of 0.5/365 or 1.0/365, respectively. Figure 1 shows an example of a discount rate of 1.0/365&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/discount_function.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;aggregate-time-discounted-expected-lift-visual-intuition&quot;&gt;Aggregate time-discounted expected lift: Visual Intuition&lt;/h2&gt;

&lt;p&gt;Take a look at Figure 2, below. It shows an experiment that is planned to run for $\tau = 60$ days. The top panel shows $\hat{L}$, which we have now defined as the expected per-session lift. While the experiment is running, $\hat{L} = 0$, since our prior is that $\Delta$ is sampled from a normal distribution with mean zero. But once the experiment finishes and we launch the winning bucket, we should begin to reap our expected per-session lift.&lt;/p&gt;

&lt;p&gt;The middle panel shows our discount function.&lt;/p&gt;

&lt;p&gt;The bottom panel shows our time-discounted lift, defined as the product of the lift in the top panel and the time discount in the middle panel. (We can also multiply it by $M$, the number of post-experiment sessions per day, which for simplicity we set to 1 here.) The aggregate time-discounted expected lift, $\hat{L}_a$, is the area under the curve.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/discounted_lift_static.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now let’s see what happens with different experiment durations. Figure 3 shows that the longer you plan to run your experiment, the higher $\hat{L}$ will be (top panel). But due to time discounting, (middle panel), the area under the time-discounted lift curve (bottom panel) is low for overly large sample sizes. There is an optimal duration of the experiment (in this case, $\tau = 24$ days), that maximizes $\hat{L}_a$, the area under the curve.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/discounted_lift_dynamic.gif&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;aggregate-time-discounted-expected-lift-formula&quot;&gt;Aggregate time-discounted expected lift: Formula&lt;/h2&gt;
&lt;p&gt;The aggregate time-discounted expected lift $\hat{L}_a$, i.e. the area under the curve in the bottom panel of Figure 3, is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{L}_a = \frac{\sigma_\Delta^2 M e^{-r\tau}}{r \sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{m\tau})}}&lt;/script&gt;

&lt;p&gt;where $ \tau $ is the duration of the experiment and $M$ is the number of post-experiment sessions per day. See the Appendix for a derivation.&lt;/p&gt;

&lt;p&gt;There’s two things to note about this formula.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Increasing the number of bucketed sessions per day, $m$, always increases $\hat{L}_a$.&lt;/li&gt;
  &lt;li&gt;Increasing the duration of the experiment, $\tau$, may or may not help. Its impact is controlled by competing forces in the numerator and denominator. In the numerator, higher $\tau$ decreases $\hat{L}_a$ by delaying shipment. In the denominator, higher $\tau$ increases $\hat{L}_a$ by making it more likely you will ship the superior version.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;optimizing-sample-size&quot;&gt;Optimizing sample size&lt;/h2&gt;

&lt;p&gt;At long last, we can answer the question, “How long should we run this experiment?”. A nice way to do it is to plot $\hat{L}_a$ as a function of $\tau$. Below we see what this looks like for one set of parameters. Here the optimal duration is 38 days.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/time_aggregated_lift_by_tau.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
Note also that a set of simulated experiment and post-experiment periods (in blue) confirm the predictions of the closed form solution (in gray). See the &lt;a href=&quot;https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb&quot;&gt;notebook&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;examples-in-python&quot;&gt;Examples in Python&lt;/h2&gt;
&lt;h3 id=&quot;example-1-continuous-variable-metric&quot;&gt;Example 1: Continuous variable metric&lt;/h3&gt;
&lt;p&gt;Let’s say you want to run an experiment comparing two different versions of a website, and your main metric is revenue per session. You know in advance that the within-group variance of this metric is $\sigma_X^2 = 100$. You don’t know which version is better but you have a prior that the true difference in means is normally distributed with variance $\sigma_\Delta^2 = 1$. You have 200 sessions per day and plan to bucket 100 sessions into Version A and 100 sessions into Version B, running the experiment for $\tau=20$ days. Your discount rate is fairly aggressive at 1.0 annually, or $r = 1/365$ per day. Using the function in the &lt;a href=&quot;https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb&quot;&gt;notebook&lt;/a&gt;, you can find $\hat{L}_a$ with this command:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;get_agg_lift_via_closed_form&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var_D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var_X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tau&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;365&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# returns 26298
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find_optimal_tau&lt;/code&gt; function to determine the optimal duration, which in this case is $\tau=18$.&lt;/p&gt;

&lt;h3 id=&quot;example-2-conversion-rates&quot;&gt;Example 2: Conversion rates&lt;/h3&gt;
&lt;p&gt;Let’s say your main metric is conversion rate. You think that on average conversion rates will be about 10%, and that the difference in conversion rates between buckets will be normally distributed with variance 1%. Using the normal approximation of the binomial distribution, you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p*(1-p)&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;var_X&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;get_agg_lift_via_closed_form&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var_D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;var_X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tau&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;365&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# returns 207
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find_optimal_tau&lt;/code&gt; function to determine the optimal duration, which in this case is $\tau=49$.&lt;/p&gt;

&lt;h2 id=&quot;faq&quot;&gt;FAQ&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Has there been any similar work on this?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; As I was writing this, I came across a &lt;a href=&quot;https://arxiv.org/pdf/1811.00457.pdf&quot;&gt;fantastic in-press paper&lt;/a&gt; by &lt;a href=&quot;https://drexel.edu/now/experts/Overview/Feit-Elea/&quot;&gt;Elea Feit&lt;/a&gt; and &lt;a href=&quot;https://www.ron-berman.com/&quot;&gt;Ron Berman&lt;/a&gt;. The paper is exceptionally clear and I would recommend reading it. Like this blog post, Feit and Berman argue that it doesn’t make any sense to pick sample sizes based on statistical significance and power thresholds. Instead they recommend profit-maximizing sample sizes. They independently come to the same formula for $ \hat{L} $ as I do (see right addend in their Equation 9, making sure to substitute my $\frac{\sigma_\Delta^2}{2}$ for their $\sigma^2)$. Where they differ is that they assume there is a fixed pool of $N$ users that can only experience the product once. In their setup, you can allocate $n_1$ users to Bucket A and $n_2$ users to Bucket B. Once you have identified the winning bucket, you ship that version to the remaining $N-n_1-n_2$ users. Your expected profit is determined by the total expected lift from those users. My experience in industry differs from this setup. In my experience there is no constraint that you can only show the product once to a fixed set of users. Instead, there is often an indefinitely increasing pool of new users, and once you ship the winning bucket you can ship it to everyone, including users who already participated in the experiment. To me, the main constraint in industry is therefore time discounting, rather than a finite pool of users.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; In addition to the lift from shipping a winning bucket, doesn’t experimentation also help inform us about the types of products that might work in the future? And if so, doesn’t this mean we should run experiments longer than recommended by your formula for $\hat{L}_a$?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, experimentation can teach lessons that are generalizable beyond the particular product being tested. This is an advantage of high powered experimentation not included in my framework.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What about &lt;a href=&quot;/2016/02/28/four-pitfalls-of-hill-climbing/&quot;&gt;novelty effects&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yup, that’s a real concern not covered by my framework. You probably want to know a somewhat long term impact of your product, which means you should probably run the experiment for longer than recommended by my framework.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; If some users can show up in multiple sessions, doesn’t bucketing by session violate independence assumptions?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yeah, so this is tricky. For many companies, there is a distribution of user activity, where some users come for many sessions per week and other users come for only one session at most. Modeling this would make the framework significantly more complicated, so I tried to simplify things by making sessions the unit of analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Is there anything else on your blog vaguely related to this topic?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; I’m glad you asked!&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2016/02/28/four-pitfalls-of-hill-climbing/&quot;&gt;Four pitfalls of hill climbing&lt;/a&gt; discusses some product-focused issues in A/B testing&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2018/02/04/hyperbolic-discounting/&quot;&gt;Hyperbolic discounting — The irrational behavior that might be rational after all&lt;/a&gt; is about time discounting, although not in the context of experimentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;
&lt;p&gt;The aggregate time-discounted expected lift $\hat{L}_a$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{L}_a = \int_{\tau}^{\infty} \hat{L} M e^{-rt} \,dt&lt;/script&gt;

&lt;p&gt;where $\hat{L}$ is the expected per-session lift, $M$ is the number of post-experiment sessions per day, $r$ is the discount rate, and $ \tau $ is the duration of the experiment. Solving the integral gives:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{L}_a = \frac{\hat{L} M e^{-r\tau}}{r}&lt;/script&gt;

&lt;p&gt;Plugging in our previously solved value of $\hat{L}$ gives&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{L}_a = \frac{\sigma_\Delta^2 M e^{-r\tau}}{r \sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{m\tau})}}&lt;/script&gt;
</content>
 </entry>
 
 <entry>
   <title>Optimizing sample sizes in A/B testing, Part II&#58; Expected lift</title>
   <link href="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/"/>
   <updated>2020-01-10T00:00:00-08:00</updated>
   <id>http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II</id>
   <content type="html">&lt;div class=&quot;caption&quot;&gt;
This is Part II of a three-part blog post on how to optimize your sample size in A/B testing. Make sure to read &lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I&quot;&gt;Part I&lt;/a&gt; if you haven't already.
&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;In this blog post (Part II), I describe what I think is an incredibly cool business-focused formula that quantifies how much you can benefit from increasing your sample size. It is, in short, an &lt;em&gt;average of the value of all possible outcomes of the experiment, weighted by their probabilities&lt;/em&gt;. This post starts off kind of dry, but if you can make it through the first section, it gets a lot easier.&lt;/p&gt;

&lt;h2 id=&quot;outcome-probabilities&quot;&gt;Outcome probabilities&lt;/h2&gt;
&lt;p&gt;Imagine you are comparing two versions of a website. You currently are on version A, but you would like to compare it to version B. Imagine you are measuring some random variable $X$, which might represent something like clicks per user or page views per user. The goal of the experiment is to determine which version of the website has a higher mean value of $X$.&lt;/p&gt;

&lt;p&gt;This blog post aims to quantify the benefit of experimentation as an average of the value of all possible outcomes, weighted by their probabilities. To do that, we first need to describe the probabilities of all the different outcomes. An outcome consists of two parts: A &lt;em&gt;true&lt;/em&gt; difference in means, $\Delta$, defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta = \mu_B - \mu_A&lt;/script&gt;

&lt;p&gt;and an experimentally &lt;em&gt;observed&lt;/em&gt; difference in means $\delta$, defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta = \overline{X}_B - \overline{X}_A&lt;/script&gt;

&lt;p&gt;Let’s start with $\Delta$. While you don’t yet know which version of the website is better (that’s what the experiment is for!), you have a sense for how important the product change is. You can therefore create a normally distributed prior on $\Delta$ with mean zero and variance $ \sigma_\Delta^2 $.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/univariate_normal.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Next, let’s consider $\delta$, your experimentally observed difference in means. It will be a noisy estimate of $\Delta$. Let’s assume you have previously measured the variance of $X$ to be $ \sigma_X^2 $. It is reasonable to assume that within each group in the experiment, and for any particular $\Delta$, the variance of $X$ will still be $ \sigma_X^2$. You should therefore believe that for any particular $\Delta$, the observed difference in means $\delta$ will be sampled from a normal distribution $\mathcal{N}(\Delta, \sigma_c^2)$, where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_c^2 = \frac{2\sigma_X^2}{n}&lt;/script&gt;

&lt;p&gt;and where $n$ is the sample size in each of the two buckets. If that doesn’t make sense, check out &lt;a href=&quot;https://www.khanacademy.org/math/statistics-probability/significance-tests-confidence-intervals-two-samples/comparing-two-means/v/difference-of-sample-means-distribution&quot;&gt;this video&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Collectively, this all forms a bivariate normal distribution of outcomes, shown below.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/bivariate_normal.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;. Probabilities of possible outcomes, based on your prior beliefs. The horizontal axis is the true difference in means, and the vertical axis is the observed difference in means.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;To gain some more intuition about this, take a look at Figure 3. As sample size increases, $ \sigma^2_c $ decreases.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/three_bivariate_normals.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;outcome-lifts&quot;&gt;Outcome lifts &lt;/h2&gt;
&lt;p&gt;Now that we know the probabilities of all the different outcomes, we next need to estimate how much per-user lift, $l$, we will gain from each possible outcome, assuming we follow a policy of shipping whichever bucket (A or B) looked better in the experiment.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In cases where $\delta &amp;gt; 0$ and $\Delta &amp;gt; 0$, you would ship B and your post-experiment per-user lift will be positively valued at $l = \Delta$.&lt;/li&gt;
  &lt;li&gt;In cases where $\delta &amp;gt; 0$ and $\Delta &amp;lt; 0$, you would ship B, but unfortunately your post-experiment per-user lift will be negatively valued at $l = \Delta$, since $\Delta$ is negative.&lt;/li&gt;
  &lt;li&gt;In cases where $\delta &amp;lt; 0$, you would keep A in production, and your post-experiment lift would be zero.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A heatmap of the per-user lifts ($l$) for each outcome is shown in the plot below. Good outcomes, where shipping B was the right decision, are shown in blue. Bad outcomes, where shipping B was the wrong decision, are shown in red. There are two main ways to get a neutral outcomes, shown in white. Either you keep A (bottom segment), in which case there is zero lift, or you ship B where B is only negligibly different than A (vertical white stripe).&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/lift_matrix.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;. Heatmap of possible outcomes, where the color scale represents the lift, $l$. The horizontal axis is the true difference in means, and the vertical axis is the observed difference in means
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;probability-weighted-outcome-lifts&quot;&gt;Probability-weighted Outcome Lifts&lt;/h2&gt;
&lt;p&gt;At this point, we know the probability of each outcome, and we know the post-experiment per-user lift of each outcome. To determine how much lift we can expect, on average, by shipping the winning bucket of an experiment, we need to compute a probability-weighted average of the outcome lifts. Let’s start by looking at this visually and then later we’ll get into the math.&lt;/p&gt;

&lt;p&gt;As shown in Figure 5, if we multiply the bivariate normal distribution (left) by the lift map (center), we can obtain the probability-weighted lift of each outcome (right).&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/product_stages.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The good outcomes contribute more than the bad outcomes, simply because a good outcome is more likely than a bad outcome. To put it differently, experimentation will on average give you useful information.&lt;/p&gt;

&lt;p&gt;To gain some more intuition on this, it is helpful to see this plot for different sample sizes. As sample size increases, the probability-weighted contribution of bad outcomes gets smaller and smaller. &lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/three_products.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;computing-the-expected-post-experiment-per-user-lift&quot;&gt;Computing the expected post-experiment per-user lift&lt;/h3&gt;
&lt;p&gt;We’re almost there! To determine the expected post-experiment lift from shipping the winning bucket, we need to compute a probability-weighted average of all the post-experiment lifts. In other words, we need to sum up all the probability-weighted post-experiment lifts on the right panel of Figure 5. The formula for doing this is shown below. A derivation can be found in the Appendix.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{n})}}&lt;/script&gt;

&lt;p&gt;There’s three things to notice about this formula.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;As $n$ increases, $\hat{L}$ increases. This makes sense. The larger the sample size, the more likely it is that you’ll ship the winning bucket.&lt;/li&gt;
  &lt;li&gt;As the within-group variance $\sigma_X^2$ increases, $\hat{L}$ decreases. That’s because a high within-group variance makes experiments less informative – they’re more likely to give you the wrong answer.&lt;/li&gt;
  &lt;li&gt;As the variance prior on $\Delta$ increases, $\hat{L}$ increases. This also make sense. The more impactful (positive or negative) you think the product change might be, the more value you will get from experimentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can try this out using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_lift_via_closed_form&lt;/code&gt; formula in the &lt;a href=&quot;https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb&quot;&gt;Notebook&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;demonstration-via-simulation&quot;&gt;Demonstration via simulation&lt;/h2&gt;
&lt;p&gt;In the previous section, we derived a formula for $\hat{L}$. Should you trust a formula you found on a random internet blog? Yes! Let’s put the formula to the test, by comparing its predictions to actual simulations.&lt;/p&gt;

&lt;p&gt;First, let’s consider the case where the outcome is a continuous variable, such as the number of clicks. Let’s set $ \sigma_D^2 = 2 $ and $ \sigma_X^2 = 100 $. We then measure $\hat{L}$ for a range of sample sizes, using both the closed-form solution and simulations. To see how we determine $\hat{L}$ for simulations, refer to the box below.&lt;/p&gt;

&lt;div class=&quot;box&quot;&gt;
&lt;strong&gt;Procedure for finding $\hat{L}$ with simulations&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;
Loop through thousands of simulated experiments. On each each experiment doing the following:&lt;br /&gt;&lt;br /&gt;
&lt;ol&gt;
&lt;li&gt;Sample a true group difference $\Delta$ from $\mathcal{N}(0, \sigma_D^2)$&lt;/li&gt;
&lt;li&gt;Sample an $X$ for each of the $n$ users in each bucket A and B, using Normal distributions $\mathcal{N}(\frac{\Delta}{2}, \sigma_X^2)$ and $\mathcal{N}(-\frac{\Delta}{2}, \sigma_X^2)$, respectively.&lt;/li&gt;
&lt;li&gt;Compute $ \delta = \overline{X}_B - \overline{X}_A $.&lt;/li&gt;
&lt;li&gt;If $\delta &amp;lt;= 0$, stick with A and accrue zero lift.&lt;/li&gt;
&lt;li&gt;If $\delta &amp;gt; 0$, ship B and accrue the per-user lift of $\Delta$, which will probably, but not necessarily, be positive.&lt;/li&gt;
&lt;/ol&gt;
We run these experiments thousands of times, each time computing the per-user lift. Finally, we average all the per-user lifts together to get $\hat{L}$. See the get_lift_via_simulations_continuous function in the &lt;a href=&quot;https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb&quot;&gt;notebook&lt;/a&gt; for an implementation.
&lt;/div&gt;
&lt;p&gt;As seen in Figure 7, below, the results of the simulation closely match the closed-form solution.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/lift_by_n_continuous.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 7&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Second, let’s consider the case where the variable is binary, as in conversion rates. For reasonably large values of $ n $, we can safely assume that the error variance is normally distributed with variance $ \sigma_X^2 = p(1-p) $, where $ p $ is the baseline conversion rate. For this example, let’s set the baseline conversion rate $p = 0.1$, and let’s set $ \sigma_\Delta^2 = 0.01^2 $. The results of the simulation closely match the closed-form solution.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/lift_by_n_binary.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 8&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;thinking-about-costs-and-a-preview-of-part-iii&quot;&gt;Thinking about costs, and a preview of Part III&lt;/h2&gt;

&lt;p&gt;In this blog post, we saw how increasing the sample size improves the expected post-experiment per-user lift, $\hat{L}$. But to determine the &lt;em&gt;optimal&lt;/em&gt; sample size, we need to think about costs.&lt;/p&gt;

&lt;p&gt;The cost in dollars of an experiment can be described as $f + vn$, where $f$ is a fixed cost and $ v $ is the variable cost per participant. If you already know these costs, and if you already know the revenue increase $ u $ from each unit increase in lift, you can calculate the net revenue $R$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R = u\hat{L}  - f - vn&lt;/script&gt;

&lt;p&gt;and then find the sample size $ n $ that maximizes $ R $.&lt;/p&gt;

&lt;p&gt;Unfortunately, these costs aren’t always readily available. The good news is that there is a really nice way to calculate the most important cost: the forfeited benefit that comes from prolonging your experiment. To read about that, and about how to optimize your sample size, please continue to &lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/&quot;&gt;Part III&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;
&lt;p&gt;To determine $\hat{L}$, we start with the probability-weighted lifts on the right panel of Figure 5. This is a bivariate normal distribution over $ \Delta $ and $ \delta $, multiplied by $ \Delta $.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\Delta, \delta) = \frac{\Delta}{2 \pi \sigma_\Delta \sigma_\delta \sqrt{1-\rho^2}} e^{-\frac{
\frac{\Delta^2}{\sigma_\Delta^2} - \frac{2 \rho \Delta \delta}{\sigma_\Delta \sigma_\delta} + \frac{\delta^2}{\sigma_\delta^2}
}{2(1-\rho^2)}
}&lt;/script&gt;

&lt;p&gt;where the correlation coefficient $ \rho $, is &lt;a href=&quot;http://athenasc.com/Bivariate-Normal.pdf&quot;&gt;defined&lt;/a&gt; as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\rho = \sqrt{1 - \frac{\sigma_c^2}{\sigma_\Delta^2 + \sigma_c^2}}&lt;/script&gt;

&lt;p&gt;and $\sigma_\delta^2$ is the variance on $\delta$. By the &lt;a href=&quot;/2019/05/18/variance_after_scaling_and_summing/&quot;&gt;variance addition rules&lt;/a&gt;, $\sigma_\delta^2$ is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_\delta^2 = \sigma_\Delta^2 + \sigma_c^2&lt;/script&gt;

&lt;p&gt;We next need to sum up the probability-weighted values in $f(\Delta, \delta)$. To obtain a closed form solution, we can use integration.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{L} = \int_{0}^{\infty} \int_{-\infty}^{\infty} {\frac{\Delta}{2 \pi \sigma_\Delta \sigma_\delta \sqrt{1-\rho^2}} e^{-\frac{

\frac{\Delta^2}{\sigma_\Delta^2} - \frac{2 \rho \Delta \delta}{\sigma_\Delta \sigma_\delta} + \frac{\delta^2}{\sigma_\delta^2}

}{2(1-\rho^2)}

}
\,d\Delta\,d\delta
}&lt;/script&gt;

&lt;p&gt;The integration limits on $ \delta $ start at zero because the lift will always be zero if $ \delta &amp;lt; 0 $ (i.e if the status quo bucket A wins the experiment).&lt;/p&gt;

&lt;p&gt;Thanks to my 15-day free trial of &lt;a href=&quot;https://www.wolfram.com/mathematica/&quot;&gt;Mathematica&lt;/a&gt;, I determined that this integral comes out to the surprisingly simple &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{L} = \rho \frac{\sigma_\Delta}{\sqrt{2\pi}}&lt;/script&gt;

&lt;p&gt;The command I used was:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Integrate[(t / (2*\[Pi]*s1*s2*Sqrt[1 - p^2]))*Exp[-((t^2/s1^2 - \
(2*p*t*d)/(s1*s2) + d^2/s2^2)/(2*(1 - p^2)))], {d, 0, \[Infinity]}, \
{t, -\[Infinity], \[Infinity]}, Assumptions -&amp;gt; p &amp;gt; 0 &amp;amp;&amp;amp; p &amp;lt; 1 &amp;amp;&amp;amp; s1 &amp;gt; \
0 &amp;amp;&amp;amp; s2 &amp;gt; 0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we then substitute in previously defined formulas for $ \rho $ and $ \sigma_c^2 $, we can produce a formula that accepts more readily-available inputs.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{n})}}&lt;/script&gt;

&lt;p&gt;Continue to &lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/&quot;&gt;Part III&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Optimizing sample sizes in A/B testing, Part I&#58; General summary</title>
   <link href="http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/"/>
   <updated>2020-01-10T00:00:00-08:00</updated>
   <id>http://localhost:4000/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I</id>
   <content type="html">&lt;div class=&quot;caption&quot;&gt;
A special thanks to &lt;a href=&quot;https://www.linkedin.com/in/john-mcdonnell-65833233/&quot;&gt;John McDonnell&lt;/a&gt;, who came up with the idea for this post. Thanks also to &lt;a href=&quot;https://www.linkedin.com/in/marika-inhoff-92087313a/&quot;&gt;Marika Inhoff&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/nelson-ray-b180641b/&quot;&gt;Nelson Ray&lt;/a&gt; for comments on an earlier draft.
&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;If you’re a data scientist, you’ve surely encountered the question, “How big should this A/B test be?”&lt;/p&gt;

&lt;p&gt;The standard answer is to do a power analysis, typically aiming for 80% power at $\alpha$=5%. But if you think about it, this advice is pretty weird. Why is 80% power the best choice for your business? And doesn’t a 5% significance cutoff seem pretty arbitrary?&lt;/p&gt;

&lt;p&gt;In most business decisions, you want to choose a policy that maximizes your benefits minus your costs. In experimentation, the benefit comes from learning information to drive future decisions, and the cost comes from the experiment itself. The optimal sample size will therefore depend on the unique circumstances of your business, not on arbitrary statistical significance thresholds.&lt;/p&gt;

&lt;p&gt;In this three-part blog post, I’ll present a new way of determining optimal sample sizes that completely abandons the notion of statistical significance.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/&quot;&gt;Part I: General Overview&lt;/a&gt;. Starts with a mostly non-technical overview and ends with a section called “Three lessons for practitioners”.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/&quot;&gt;Part II: Expected lift&lt;/a&gt;. A more technical section that quantifies the benefits of experimentation as a function of sample size.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/&quot;&gt;Part III: Aggregate time-discounted lift&lt;/a&gt;. A more technical section that quantifies the costs of experimentation as a function of sample size. It then combines costs and benefits into a closed-form expression that can be optimized. Ends with an FAQ.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Throughout Parts I-III, the focus will be on choosing a sample size at the beginning of the experiment and committing to it, not on dynamically updating the sample size as the experiment proceeds.&lt;/p&gt;

&lt;p&gt;With that out of the way, let’s get started!&lt;/p&gt;

&lt;h3 id=&quot;benefits-of-large-samples&quot;&gt;Benefits of large samples&lt;/h3&gt;
&lt;p&gt;The bigger your sample size, the more likely it is that you’ll ship the right bucket. Since there is a gain to shipping the right bucket and a loss to shipping the wrong bucket, the average benefit of the experiment is a probability-weighted average of these outcomes. We call this the &lt;em&gt;expected post-experiment lift&lt;/em&gt;, $\hat{L}$, which increases with sample size. We’ll cover this in more detail in Part II.&lt;/p&gt;

&lt;h3 id=&quot;costs-of-large-samples&quot;&gt;Costs of large samples&lt;/h3&gt;
&lt;p&gt;For most businesses, increasing your sample size requires you to run your experiment longer. This brings us to the main per-unit cost of experimentation: the forfeited benefits that could come from shipping the winning bucket earlier. In a fast moving startup, there’s often good reason to accrue your wins as soon as possible. The advantage of shipping earlier can be quantified with a &lt;em&gt;discount rate&lt;/em&gt;, which describes how much you value the near future over the distant future. If you have a high discount rate, it’s critical to ship as soon as possible. If you have a low discount rate, you can afford to wait longer. This is described in more detail in Part III.&lt;/p&gt;

&lt;h3 id=&quot;combining-costs-and-benefits-into-an-optimization-function&quot;&gt;Combining costs and benefits into an optimization function&lt;/h3&gt;
&lt;p&gt;You should run your experiment long enough that you’ll likely ship the winning bucket, but not so long that you waste time not having shipped your product. The optimal duration depends on the unique circumstances of your business. The overall benefit of running an experiment, as a function of duration and other parameters, is defined as the &lt;em&gt;aggregate time-discounted expected post-experiment lift&lt;/em&gt;, or $\hat{L}_a$.&lt;/p&gt;

&lt;p&gt;Figure 1 shows $\hat{L}_a$ as a function of experiment duration in days ($\tau$) for one particular set of business parameters. The gray curve shows the result of a closed form solution presented in Part III. The blue curve shows the results of simulated experiments. As you can see, the optimal duration for this experiment should be about 38 days. Simulations match the closed-form predictions.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/time_aggregated_lift_by_tau_descriptive_labels.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Aggregate time-discounted expected post-experiment lift ($\hat{L}_a$) as a function of experiment duration in days ($\tau$), for a fairly typical set of business parameters.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;three-lessons-for-practitioners&quot;&gt;Three lessons for practitioners&lt;/h3&gt;
&lt;p&gt;I played around with the formula for $\hat{L}_a$ and came across three lessons that should be of interest to practitioners.&lt;/p&gt;

&lt;h4 id=&quot;1-you-should-run-underpowered-experiments-if-you-have-a-very-high-discount-rate&quot;&gt;1. You should run “underpowered” experiments if you have a very high discount rate&lt;/h4&gt;
&lt;p&gt;Take a look at Figure 2, which shows some recommendations for a fairly typical two-bucket conversion rate experiment with 1000 sessions per bucket per day. On the left panel we plot the optimal duration as a function of the annual discount rate. If you have a high discount rate, you care a lot more about the near future than the distant future. It is therefore critical that you ship any potentially winning version as soon as possible. In this scenario, the optimal duration is low (left panel). Power, the probability you will find a statistically significant result, is also low (right panel). For many of these cases, the optimal duration would traditionally be considered “underpowered”.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/optimal_tau_and_power_by_r.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;2-you-should-run-underpowered-experiments-if-you-have-a-small-user-base&quot;&gt;2. You should run “underpowered” experiments if you have a small user base&lt;/h4&gt;
&lt;p&gt;Now let’s plot these curves as a function of $m$, our daily sessions per bucket. If you only have a small number of daily sessions to work with, you’ll need to run the experiment for longer (left panel). So far, that’s not surprising. But here’s where it gets interesting: Even though optimal duration increases as $m$ decreases, it doesn’t increase fast enough to maintain constant power (right panel). In fact, for low $m$ scenarios where you don’t have a lot of users, the optimal duration results in power that can drop below 50%, well into what would traditionally be considered “underpowered” territory. In these situations, waiting to get a large number of sessions causes too much time-discounting loss.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/optimal_tau_and_power_by_m.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;3-that-said-its-far-better-to-run-your-experiment-too-long-than-too-short&quot;&gt;3. That said, it’s far better to run your experiment too long than too short&lt;/h4&gt;
&lt;p&gt;Let’s take another look at $\hat{L}_a$ as a function of duration. As shown in Figure 4 below, the left shoulder is steeper than the right shoulder. This means that it’s really bad if your experiment is shorter than optimal, but it’s kind of ok if your experiment is longer than optimal.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/time_aggregated_lift_by_tau_shoulders.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Aggregate time-discounted expected post-experiment lift ($\hat{L}_a$) as a function of experiment duration in days ($\tau$), for a fairly typical set of business parameters.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Is this true in general? Yes. Below we plot $\hat{L}_a$ as a function of duration for various combinations of $m$ and the discount rate, $r$. For all of these parameter combinations, it’s better to run a bit longer than optimal than a bit shorter than optimal. The only exception is if you have an insanely high discount rate (not shown).&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2020_optimizing_sample_sizes/L_a_by_tau_for_m_and_r.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;upcoming-posts-and-python-notebook&quot;&gt;Upcoming posts and Python notebook&lt;/h3&gt;
&lt;p&gt;You probably have a lot of questions about where this framework comes from and how it is justified. &lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-II/&quot;&gt;Part II&lt;/a&gt; and &lt;a href=&quot;/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-III/&quot;&gt;Part III&lt;/a&gt; dive more deeply into the math and visual intuition behind it. They also contain some example uses of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_lift_via_closed_form&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_agg_lift_via_closed_form&lt;/code&gt; functions available in the accompanying &lt;a href=&quot;https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb&quot;&gt;Python Notebook&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Variance after scaling and summing&#58; One of the most useful facts from statistics</title>
   <link href="http://localhost:4000/2019/05/18/variance_after_scaling_and_summing/"/>
   <updated>2019-05-18T00:00:00-07:00</updated>
   <id>http://localhost:4000/2019/05/18/variance_after_scaling_and_summing</id>
   <content type="html">&lt;p&gt;What do $ R^2 $, laboratory error analysis, ensemble learning, meta-analysis, and financial portfolio risk all have in common? The answer is that they all depend on a fundamental principle of statistics that is not as widely known as it should be. Once this principle is understood, a lot of stuff starts to make more sense.&lt;/p&gt;

&lt;p&gt;Here’s a sneak peek at what the principle is.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij}&lt;/script&gt;

&lt;p&gt;Don’t worry if the formula doesn’t yet make sense! We’ll work our way up to it slowly, taking pit stops along the way at simpler formulas that are useful on their own. As we work through these principles, we’ll encounter lots of neat applications and explainers.&lt;/p&gt;

&lt;p&gt;This post consists of three parts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Part 1&lt;/strong&gt;: Sums of uncorrelated random variables: Applications to social science and laboratory error analysis&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Part 2&lt;/strong&gt;: Weighted sums of uncorrelated random variables: Applications to machine learning and scientific meta-analysis&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Part 3&lt;/strong&gt;: Correlated variables and Modern Portfolio Theory&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;part-1-sums-of-uncorrelated-random-variables-applications-to-social-science-and-laboratory-error-analysis&quot;&gt;Part 1: Sums of uncorrelated random variables: Applications to social science and laboratory error analysis&lt;/h2&gt;

&lt;p&gt;Let’s start with some simplifying conditions and assume that we are dealing with &lt;em&gt;uncorrelated&lt;/em&gt; random variables. If you take two of them and add them together, the variance of their sum will equal the sum of their variances. This is amazing!&lt;/p&gt;

&lt;p&gt;To demonstrate this, I’ve written some Python code that generates three arrays, each of length 1 million. The first two arrays contain samples from two normal distributions with variances 9 and 16, respectively. The third array is the sum of the first two arrays. As shown in the simulation, its variance is 25, which is equal to the sum of the variances of the first two arrays (9 + 16).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 1M samples from normal distribution with variance=9
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 9
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 1M samples from normal distribution with variance=16
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 16
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 25
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)&quot;&gt;fact&lt;/a&gt; was first discovered in 1853 and is known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)&quot;&gt;Bienaymé’s Formula&lt;/a&gt;. While the code example above shows the sum of two random variables, the formula can be extended to multiple random variables as follows:&lt;/p&gt;

&lt;div class=&quot;box&quot;&gt;
If $ X_p $ is a sum of uncorrelated random variables $ X_1 .. X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum{\sigma^2_i} $$

where each $ X_i $ has variance $ \sigma_i^2 $.
&lt;/div&gt;

&lt;p&gt;What does the $ p $ stand for in $ X_p $? It stands for &lt;em&gt;portfolio&lt;/em&gt;, which is just one of the many applications we’ll see later in this post.&lt;/p&gt;

&lt;h3 id=&quot;why-this-is-useful&quot;&gt;Why this is useful&lt;/h3&gt;
&lt;p&gt;Bienaymé’s result is surprising and unintuitive. But since it’s such a simple formula, it is worth committing to memory, especially because it sheds light on so many other principles. Let’s look at two of them.&lt;/p&gt;

&lt;h4 id=&quot;understanding--r2--and-variance-explained&quot;&gt;Understanding $ R^2 $ and “variance explained”&lt;/h4&gt;
&lt;p&gt;Psychologists often talk about “within-group variance”, “between-group variance”, and “variance explained”. What do these terms mean?&lt;/p&gt;

&lt;p&gt;Imagine a hypothetical study that measured the extraversion of 10 boys and 10 girls, where extraversion is measured on a 10-point scale (&lt;em&gt;Figure 1&lt;/em&gt;. Orange bars). The boys have a mean extraversion of 4.4 and the girls have a mean extraversion 5.0. In addition, the overall variance of the data is 2.5.  We can decompose this variance into two parts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Between-group variance&lt;/strong&gt;: Create a 20-element array where every boy is assigned to the mean boy extraversion of 4.4, and every girl is assigned to the mean girl extraversion of 5.0. The variance of this array is 0.9. (&lt;em&gt;Figure 1&lt;/em&gt;. Blue bars).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Within-group variance&lt;/strong&gt;: Create a 20-element array of the amount each child’s extraversion deviates from the mean value for their sex. Some of these values will be negative and some will be positive. The variance of this array is 1.6. (&lt;em&gt;Figure 1&lt;/em&gt;. Pink bars).&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2019_variance/boy_girl_extraversion.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Decomposition of extraversion scores (orange) into between-group variance (blue) and within-group variance (pink).
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If you add these arrays together, the resulting array will represent the observed data (&lt;em&gt;Figure 1&lt;/em&gt;. Orange bars). The variance of the observed array is 2.5, which is exactly what is predicted by Bienaymé’s Formula. It is the sum of the variances of the two component arrays (0.9 + 1.6). Psychologists might say that sex “explains” 0.9/2.5 = 36% of the extraversion variance. Equivalently, a model of extraversion that uses sex as the only predictor would have an &lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;&gt;$ R^2 $&lt;/a&gt; of 0.36.&lt;/p&gt;

&lt;h4 id=&quot;error-propagation-in-laboratories&quot;&gt;Error propagation in laboratories&lt;/h4&gt;
&lt;p&gt;If you ever took a physics lab or chemistry lab back in college, you may remember having to perform &lt;a href=&quot;http://ipl.physics.harvard.edu/wp-uploads/2013/03/PS3_Error_Propagation_sp13.pdf&quot;&gt;error analysis&lt;/a&gt;, in which you calculated how errors would propagate through one noisy measurement after another.&lt;/p&gt;

&lt;p&gt;Physics textbooks often say that standard deviations add in “quadrature”, which just means that if you are trying to estimate some quantity that is the sum of two other measurements, and if each measurement has some error with standard deviation &lt;script type=&quot;math/tex&quot;&gt;\sigma_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\sigma_2&lt;/script&gt; respectively, the final standard deviation would be  &lt;script type=&quot;math/tex&quot;&gt;\sigma_{p} = \sqrt{\sigma^2_1 + \sigma^2_2}&lt;/script&gt;. I think it’s probably easier to just use variances, as in the Bienaymé Formula, with &lt;script type=&quot;math/tex&quot;&gt;\sigma^2_{p} = \sigma^2_1 + \sigma^2_2&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For example, imagine you are trying to estimate the height of two boxes stacked on top of each other (&lt;em&gt;Figure 2&lt;/em&gt;). One box has a height of 1 meter with variance $ \sigma^2_1 $ = 0.01, and the other has a height of 2 meters with variance $ \sigma^2_2 $ = 0.01. Let’s further assume, perhaps optimistically, that these errors are independent. That is, if the measurement of the first box is too high, it’s not any more likely that the measurement of the second box will also be too high. If we can make these assumptions, then the total height of the two boxes will be 3 meters with variance $ \sigma^2_p $ = 0.02.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2019_variance/stacked_boxes.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:50%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Two boxes stacked on top of each other. The height of each box is measured with some variance (uncertainty). The total height is the sum of the individual heights, and the total variance (uncertainty) is the sum of the individual variances.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;There is a key difference between the extraversion example and the stacked boxes example. In the extraversion example, we added two &lt;em&gt;arrays&lt;/em&gt; that each had an observed sample variance. In the stacked boxes example, we added two &lt;em&gt;scalar measurements&lt;/em&gt;, where the variance of these measurements refers to our measurement uncertainty. Since both cases have a meaningful concept of ‘variance’, the Bienaymé Formula applies to both.&lt;/p&gt;

&lt;h2 id=&quot;part-2-weighted-sums-of-uncorrelated-random-variables-applications-to-machine-learning-and-scientific-meta-analysis&quot;&gt;Part 2: Weighted sums of uncorrelated random variables: Applications to machine learning and scientific meta-analysis&lt;/h2&gt;

&lt;p&gt;Let’s now move on to the case of &lt;em&gt;weighted&lt;/em&gt; sums of uncorrelated random variables. But before we get there, we first need to understand what happens to variance when a random variable is scaled.&lt;/p&gt;

&lt;div class=&quot;box&quot;&gt;
If $ X_p $ is defined as $ X $ scaled by a factor of $ w $, then the variance $ X_p $ will be

$$ \sigma_{p}^{2} = w^2 \sigma^2 $$

where $ \sigma^2 $ is the variance of $ X $.
&lt;/div&gt;

&lt;p&gt;This means that if a random variable is scaled, the scale factor on the variance will change &lt;em&gt;quadratically&lt;/em&gt;. Let’s see this in code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;baseline_var&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;baseline_var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Array of 1M samples from normal distribution with variance=10
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 10
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Scale this by w=0.7
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;baseline_var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 4.9 (predicted variance)
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 4.9 (empirical variance) 
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;To gain some intuition for this rule, it’s helpful to think about outliers. We know that outliers have a huge effect on variance. That’s because the formula used to compute variance, $ \sum{\frac{(x_i - \bar{x})^2}{n-1}} $, squares all the deviations, and so we get really big variances when we square large deviations. With that as background, let’s think about what happens if we scale our data by 2. The outliers will spread out twice as far, which means they will have even more than twice as much impact on the variance. Similarly, if we multiply our data by 0.5, we will squash the most “damaging” part of the outliers, and so we will reduce our variance by more than a factor of two.&lt;/p&gt;

&lt;p&gt;While the above principle is pretty simple, things start to get interesting when you combine it with the Bienaymé Formula in Part I:&lt;/p&gt;

&lt;div class=&quot;box&quot;&gt;
If $ X_p $ is a weighted sum of uncorrelated random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be 

$$ \sigma_{p}^{2} = \sum{w^2_i \sigma^2_i} $$

where each $ w_i $ is a weight on $ X_i $, and each $ X_i $ has its own variance $ \sigma_i^2 $.
&lt;/div&gt;

&lt;p&gt;The above formula shows what happens when you scale and then sum random variables. The final variance is the weighted sum of the original variances, where the weights are squares of the original weights. Let’s see how this can be applied to machine learning.&lt;/p&gt;

&lt;h3 id=&quot;an-ensemble-model-with-equal-weights&quot;&gt;An ensemble model with equal weights&lt;/h3&gt;

&lt;p&gt;Imagine that you have built two separate models to predict car prices. While the models are unbiased, they have variance in their errors. That is, sometimes a model prediction will be too high, and sometimes a model prediction will be too low. Model 1 has a mean squared error (MSE) of \$1,000 and Model 2 has an MSE of \$2,000.&lt;/p&gt;

&lt;p&gt;A valuable insight from machine learning is that you can often create a better model by simply averaging the predictions of other models. Let’s demonstrate this with simulations below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy.random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;actual&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;errors1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 1000
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 2000
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Note that this section could be replaced with 
# errors_ensemble = 0.5 * errors1 + 0.5 * errors2
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actual&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;errors1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;preds2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actual&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;errors2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;preds_ensemble&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;errors_ensemble&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds_ensemble&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actual&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors_ensemble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 750. Lower than variance of component models!
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As shown in the code above, even though a good model (Model 1) was averaged with an inferior model (Model 2), the resulting Ensemble model’s MSE of \$750 is better than either of the models individually.&lt;/p&gt;

&lt;p&gt;The benefits of ensembling follow directly from the weighted sum formula we saw above, &lt;script type=&quot;math/tex&quot;&gt;\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}&lt;/script&gt;. To understand why, it’s helpful to think of models not as generating predictions, but rather as generating errors. Since averaging the predictions of a model corresponds to averaging the errors of the model, we can treat each model’s array of errors as samples of a random variable whose variance can be plugged in to the formula. Assuming the models are unbiased (i.e. the errors average to about zero), the formula tells us the expected MSE of the ensemble predictions. In the example above, the MSE would be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_{p}^{2} = 0.5^2 \times 1000 + 0.5^2 \times 2000 = 750&lt;/script&gt;

&lt;p&gt;which is exactly what we observed in the simulations.&lt;/p&gt;

&lt;p&gt;(For a totally different intuition of why ensembling works, see &lt;a href=&quot;https://www.opendoor.com/w/blog/why-ensembling-works-the-intuition-behind-opendoors-home-pricing&quot;&gt;this blog post&lt;/a&gt; that I co-wrote for my company, Opendoor.)&lt;/p&gt;

&lt;h3 id=&quot;an-ensemble-model-with-inverse-variance-weighting&quot;&gt;An ensemble model with Inverse Variance Weighting&lt;/h3&gt;

&lt;p&gt;In the example above, we obtained good results by using an equally-weighted average of the two models. But can we do better?&lt;/p&gt;

&lt;p&gt;Yes we can! Since Model 1 was better than Model 2, we should probably put more weight on Model 1. But of course we shouldn’t put all our weight on it, because then we would throw away the demonstrably useful information from Model 2. The optimal weight must be somewhere in between 50% and 100%.&lt;/p&gt;

&lt;p&gt;An effective way to find the optimal weight is to &lt;a href=&quot;http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/&quot;&gt;build another model on top of these models&lt;/a&gt;. However, if you can make certain assumptions (unbiased and uncorrelated errors), there’s an even simpler approach that is great for back-of-the envelope calculations and great for understanding the principles behind ensembling.&lt;/p&gt;

&lt;p&gt;To find the optimal weights (assuming unbiased and uncorrelated errors), we need to minimize the variance of the ensemble errors
&lt;script type=&quot;math/tex&quot;&gt;\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}&lt;/script&gt;
with the constraint that 
&lt;script type=&quot;math/tex&quot;&gt;\sum{w_i} = 1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;It &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse-variance_weighting&quot;&gt;turns out&lt;/a&gt; that the variance-minimizing weight for a model should be proportional to the inverse of its variance.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_k = \frac{\frac{1}{\sigma^2_k}}{\sum{\frac{1}{\sigma^2_i}}}&lt;/script&gt;

&lt;p&gt;When we apply this method, we obtain optimal weights of &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt; = 0.67 and &lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt; = 0.33. These weights give us an ensemble error variance of&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_{p}^{2} = 0.67^2 \times 1000 + 0.33^2 \times 2000 = 666&lt;/script&gt;

&lt;p&gt;which is significantly better than the $750 variance we were getting with equal weighting.&lt;/p&gt;

&lt;p&gt;This method is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse-variance_weightinghttps://en.wikipedia.org/wiki/Inverse-variance_weighting&quot;&gt;Inverse Variance Weighting&lt;/a&gt;, and allows you to assign the right amount of weight to each model, depending on its error.&lt;/p&gt;

&lt;p&gt;Inverse Variance Weighting is not just useful as a way to understand Machine Learning ensembles. It is also one of the core principles in scientific &lt;a href=&quot;https://en.wikipedia.org/wiki/Meta-analysis&quot;&gt;meta-analysis&lt;/a&gt;, which is popular in medicine and the social sciences. When multiple scientific studies attempt to estimate some quantity, and each study has a different sample size (and hence variance of their estimate), a meta-analysis should weight the high sample size studies more. Inverse Variance Weighting is used to determine those weights.&lt;/p&gt;

&lt;h2 id=&quot;part-3-correlated-variables-and-modern-portfolio-theory&quot;&gt;Part 3: Correlated variables and Modern Portfolio Theory&lt;/h2&gt;

&lt;p&gt;Let’s imagine we now have three unbiased models with the following MSEs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model 1: MSE = 1000&lt;/li&gt;
  &lt;li&gt;Model 2: MSE = 1000&lt;/li&gt;
  &lt;li&gt;Model 3: MSE = 2000&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By Inverse Variance Weighting, we should assign more weight to the first two models, with &lt;script type=&quot;math/tex&quot;&gt;w_1=0.4, w_2=0.4, w_3=0.2&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;But what happens if Model 1 and Model 2 have correlated errors? For example, whenever Model 2’s predictions are too high, Model 3’s predictions tend to also be too high. In that case, maybe we don’t want to give so much weight to Models 1 and 2, since they provide somewhat redundant information. Instead we might want to &lt;em&gt;diversify&lt;/em&gt; our ensemble by increasing the weight on Model 3, since it provides new independent information.&lt;/p&gt;

&lt;p&gt;To determine how much weight to put on each model, we first need to determine how much total variance there will be if the errors are correlated. To do this, we need to borrow a &lt;a href=&quot;https://en.wikipedia.org/wiki/Modern_portfolio_theory&quot;&gt;formula&lt;/a&gt; from the financial literature, which extends the formulas we’ve worked with before. This is the formula we’ve been waiting for.&lt;/p&gt;

&lt;div class=&quot;box&quot;&gt;
If $ X_p $ is a weighted sum of (correlated or uncorrelated) random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij} $$

where each $ w_i $ and $ w_j $ are weights assigned to $ X_i $ and $ X_j $, where each $ X_i $ and $ X_j $ have standard deviations $ \sigma_i $ and $ \sigma_j $, and where the correlation between $ X_i $ and $ X_j $ is $ \rho_{ij} $.
&lt;/div&gt;

&lt;p&gt;There’s a lot to unpack here, so let’s take this step by step.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\sigma_i \sigma_j \rho_{ij}&lt;/script&gt; is a scalar quantity representing the covariance between &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;X_j&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;If none of the variables are correlated with each other, then all the cases where $ i \neq j $ will go to zero, and the formula reduces to &lt;script type=&quot;math/tex&quot;&gt;\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}&lt;/script&gt;, which we have seen before.&lt;/li&gt;
  &lt;li&gt;The more that two variables &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;X_j&lt;/script&gt; are correlated, the more the total variance &lt;script type=&quot;math/tex&quot;&gt;\sigma_{p}^{2}&lt;/script&gt; increases.&lt;/li&gt;
  &lt;li&gt;If two variables &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;X_j&lt;/script&gt; are anti-correlated, then the total variance decreases, since &lt;script type=&quot;math/tex&quot;&gt;\sigma_i \sigma_j \rho_{ij}&lt;/script&gt; is negative.&lt;/li&gt;
  &lt;li&gt;This formula can be rewritten in more compact notation as &lt;script type=&quot;math/tex&quot;&gt;\sigma_{p}^{2} = \vec{w}^T\Sigma \vec{w}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\vec{w}&lt;/script&gt; is the weight vector, and &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; is the covariance matrix (not a summation sign!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you skimmed the bullet points above, go back and re-read them! They are super important.&lt;/p&gt;

&lt;p&gt;To find the set of weights that minimize variance in the errors, you must minimize the above formula, with the constraint that &lt;script type=&quot;math/tex&quot;&gt;\sum{w_i} = 1&lt;/script&gt;. One way to do this is to use a &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize&quot;&gt;numerical optimization method&lt;/a&gt;. In practice, however, it is more common to just find weights by &lt;a href=&quot;http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/&quot;&gt;building another model on top of the base models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Regardless of how the weights are found, it will usually be the case that if Models 1 and 2 are correlated, the optimal weights will reduce redundancy and put lower weight on these models than simple Inverse Variance Weighting would suggest.&lt;/p&gt;

&lt;h3 id=&quot;applications-to-financial-portfolios&quot;&gt;Applications to financial portfolios&lt;/h3&gt;

&lt;p&gt;The formula above was discovered by economist &lt;a href=&quot;https://en.wikipedia.org/wiki/Harry_Markowitz&quot;&gt;Harry Markowitz&lt;/a&gt; in his &lt;a href=&quot;https://en.wikipedia.org/wiki/Modern_portfolio_theory&quot;&gt;Modern Portfolio Theory&lt;/a&gt;, which describes how an investor can optimally trade off between expected returns and expected risk, often measured as variance. In particular, the theory shows how to maximize expected return given a fixed variance, or minimize variance given a fixed expected return. We’ll focus on the latter.&lt;/p&gt;

&lt;p&gt;Imagine you have three stocks to put in your portfolio. You plan to sell them at time $ T $, at which point you expect that Stock 1 will have gone up by 5%, with some uncertainty. You can describe your uncertainty as variance, and in the case of Stock 1, let’s say &lt;script type=&quot;math/tex&quot;&gt;\sigma_1^2&lt;/script&gt; = 1. This stock, as well Stocks 2 and 3, are summarized in the table below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Stock ID&lt;/th&gt;
      &lt;th&gt;Expected Return&lt;/th&gt;
      &lt;th&gt;Expected Risk (&lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This financial example should remind you of ensembling in machine learning. In the case of ensembling, we wanted to minimize variance of the weighted sum of error arrays. In the case of financial portfolios, we want to minimize the variance of the weighted sum of scalar financial returns.&lt;/p&gt;

&lt;p&gt;As before, if there are no correlations between the expected returns (i.e. if Stock 1 exceeding 5% return does not imply that Stock 2 or Stock 3 will exceed 5% return), then the total variance in the portfolio will be
&lt;script type=&quot;math/tex&quot;&gt;\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}&lt;/script&gt;
and we can use Inverse Variance Weighting to obtain weights $ w_1=0.4, w_2=0.4, w_3=0.2 $.&lt;/p&gt;

&lt;p&gt;However, sometimes stocks have correlated expected returns. For example, if two of the stocks are in oil companies, then one stock exceeding 5% implies the other is also likely to exceed 5%. When this happens, the total variance becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij}&lt;/script&gt;

&lt;p&gt;as we saw before in the ensemble example. Since this includes an additional positive term for &lt;script type=&quot;math/tex&quot;&gt;w_1 w_2 \sigma_1 \sigma_2 \rho_{1,2}&lt;/script&gt;, the expected variance is higher than in the uncorrelated case, assuming the correlations are positive. To reduce this variance, we should put less weight on Stocks 1 and 2 than we would otherwise.&lt;/p&gt;

&lt;p&gt;While the example above focused on minimizing the variance of a financial portfolio, you might also be interested in having a portfolio with high return. Modern Portfolio Theory describes how a portfolio can reach any abitrary point on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Efficient_frontier&quot;&gt;efficient frontier&lt;/a&gt; of variance and return, but that’s outside the scope of this blog post. And as you might expect, financial markets can be more complicated than Modern Portfolio Theory suggests, but that’s also outside scope.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;That was a long post, but I hope that the principles described have been informative. It may be helpful to summarize them in &lt;em&gt;backwards&lt;/em&gt; order, starting with the most general principle.&lt;/p&gt;

&lt;div class=&quot;box&quot;&gt;

If $ X_p $ is a weighted sum of (correlated or uncorrelated) random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij} $$

where each $ w_i $ and $ w_j $ are weights assigned to $ X_i $ and $ X_j $, where each $ X_i $ and $ X_j $ have standard deviations $ \sigma_i $ and $ \sigma_j $, and where the correlation between $ X_i $ and $ X_j $ is $ \rho_{ij} $. The term $ \sigma_i \sigma_j \rho_{ij} $ is a scalar quantity representing the covariance between $ X_i $ and $ X_j $.

&lt;br /&gt;&lt;br /&gt;If none of the variables are correlated, then all the cases where $ i \neq j $ go to zero, and the formula reduces to 

$$ \sigma_{p}^{2} = \sum{w^2_i \sigma^2_i} $$

And finally, if we are computing a simple sum of random variables where all the weights are 1, then the formula reduces to

$$ \sigma_{p}^{2} = \sum{\sigma^2_i} $$
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Using your ears and head to escape the Cone Of Confusion</title>
   <link href="http://localhost:4000/2018/08/06/cone-of-confusion/"/>
   <updated>2018-08-06T00:00:00-07:00</updated>
   <id>http://localhost:4000/2018/08/06/cone-of-confusion</id>
   <content type="html">&lt;p&gt;One of coolest things I ever learned about sensory physiology is how the auditory system is able to locate sounds. To determine whether sound is coming from the right or left, the brain uses inter-ear differences in amplitude and timing. As shown in the figure below, if the sound is louder in the right ear compared to the left ear, it’s probably coming from the right side. The smaller that difference is, the closer the sound is to the midline (i.e the vertical plane going from your front to your back). Similarly, if the sound arrives at your right ear before the left ear, it’s probably coming from the right. The smaller the timing difference, the closer it is to the midline. There’s a &lt;a href=&quot;https://en.wikipedia.org/wiki/Coincidence_detection_in_neurobiology#Sound_localization&quot;&gt;fascinating&lt;/a&gt; &lt;a href=&quot;https://nba.uth.tmc.edu/homepage/cnjclub/2004spring/McAlpineGrothe2003.pdf&quot;&gt;literature&lt;/a&gt; on the neural mechanisms behind this.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_cone_of_confusion/right_and_front_right.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:50%;&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;Inter-ear loudness and timing differences are pretty useful, but unfortunately they still leave a lot of ambiguity. For example, a sound from your front right will have the exact same loudness differences and timing differences as a sound from your back right.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_cone_of_confusion/front_right_and_back_right.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:50%;&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;Not only does this system leave ambiguities between front and back, it also leaves ambiguities between top and down. In fact, there is an entire &lt;em&gt;cone of confusion&lt;/em&gt; that cannot be disambiguated by this system. Sound from all points along the surface of the cone will have the same inter-ear loudness differences and timing differences.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_cone_of_confusion/cone_of_confusion.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:50%;&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;While this system leaves a cone of confusion, humans are still able to determine the location of sounds from different points on the cone, at least to some extent. How are we able to do this?&lt;/p&gt;

&lt;p&gt;Amazingly, we are able to do this because of the shape of our ears and heads. When sound passes through our ears and head, certain frequencies are attenuated more than others. Critically, the attenuation pattern is highly dependent on sound direction.&lt;/p&gt;

&lt;p&gt;This location-dependent attenuation pattern is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Head-related_transfer_function&quot;&gt;Head-related transfer function&lt;/a&gt; (HRTF) and in theory this could be used to disambiguate locations along the cone of confusion. An example of someone’s HRTF is shown below, with frequency on the horizontal axis and polar angle on the vertical axis. Hotter colors represent less attenuation (i.e. more power). If your head and ears gave you this HRTF, you might decide a sound is coming from the front if it has more high frequency power than you’d expect.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_cone_of_confusion/cone_and_hrtf.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;
    HRTF image from Simon Carlile's &lt;a href=&quot;https://sonification.de/handbook/download/TheSonificationHandbook-chapter3.pdf&quot;&gt;Psychoacoustics chapter&lt;/a&gt; in The Sonification Handbook.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
This system sounds good in theory, but do we actually use these cues in practice? In 1988, Frederic Wightman and Doris Kistler performed an ingenious set of experiments (&lt;a href=&quot;http://public.vrac.iastate.edu/~charding/audio/Headphone%20simulation%20of%20free-field%20listening.%20I-%20Stimulus%20synthesis%20-%20J%20Acoust%20Soc%20Am%201989%20-%20Wightman.pdf&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;http://public.vrac.iastate.edu/~charding/audio/Headphone%20simulation%20of%20free-field%20listening.%20II-%20Psychophysical%20validation%20-%20J%20Acoust%20Soc%20Am%201989%20-%20Wightman.pdf&quot;&gt;2&lt;/a&gt;) to show that that people really do use HRTFs to infer location. First, they measured the HRTF of each participant by putting a small microphone in their ears and playing sounds from different locations. Next they created a digital filter for each location and each participant. That is to say, these filters implemented each participant’s HRTF. Finally, they placed headphones on the listeners and played sounds to them, each time passing the sound through one of the digital filters. Amazingly, participants were able to correctly guess the “location” of the sound, depending on which filter was used, even though the sound was coming from headphones. They were also much better at sound localization when using their own HRTF, rather than someone else’s HRTF.&lt;/p&gt;

&lt;p&gt;Further evidence for this hypothesis comes from &lt;a href=&quot;https://doi.org/10.1038/1633&quot;&gt;Hofman et al., 1998&lt;/a&gt;, who showed that by using putty to reshape people’s ears, they were able to change the HRTFs and thus disrupt sound localization. Interestingly, people were able to quickly relearn how to localize sound with their new HRTFs.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_cone_of_confusion/putty.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:50%;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;
    Image from &lt;a href=&quot;https://doi.org/10.1038/1633&quot;&gt;Hofman et al., 1998&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;A final fun fact: to improve the sound localization of humanoid robots, researchers in Japan attached &lt;a href=&quot;https://doi.org/10.1007/s10489-014-0544-y&quot;&gt;artificial ears to the robot heads&lt;/a&gt; and implemented some sophisticated algorithms to infer sound location. Here are some pictures of the robots.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_cone_of_confusion/robots.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:50%;&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;Their paper is kind of ridiculous and has some questionable justifications for not just using microphones in multiple locations, but I thought it was fun to see these principles being applied.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Hyperbolic discounting — The irrational behavior that might be rational after all</title>
   <link href="http://localhost:4000/2018/02/04/hyperbolic-discounting/"/>
   <updated>2018-02-04T00:00:00-08:00</updated>
   <id>http://localhost:4000/2018/02/04/hyperbolic-discounting</id>
   <content type="html">&lt;p&gt;&lt;/p&gt;
&lt;p&gt;When I was in grad school I occasionally overheard people talk about how humans do something called “hyperbolic discounting”. Apparently, hyperbolic discounting was considered irrational under standard economic theory.&lt;/p&gt;

&lt;p&gt;I recently decided to learn what hyperbolic discounting was all about, so I set out to write this blog post. I have to admit that hyperbolic discounting has been pretty hard for me to understand, but I think I now finally have a good enough handle on it to write about it. Along the way, I learned something interesting: Hyperbolic discounting might be rational after all.&lt;/p&gt;

&lt;h2 id=&quot;rational-and-irrational-discounting&quot;&gt;Rational and irrational discounting&lt;/h2&gt;
&lt;p&gt;If I offered you &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;50 now or &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;100 in 6 months, which would you pick? It’s not crazy to choose the &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;50 now. One reason is that it’s a safer bet. If you had chosen the delayed &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;100, there’s a risk that I might forget about the deal when the time came to pay &lt;em&gt;[personal note: I wouldn’t]&lt;/em&gt;, and you would never get your money. Another reason is that if you invest the $50 now, you might be able to make up some of the remainder in interest.&lt;/p&gt;

&lt;p&gt;Valuing immediate money more than future money is a rational behavior known as discounting. Everybody has their own discount factor. Some people might value money in 6 months at, say, 75% of what they’d value it today. Others might value it at 90%.&lt;/p&gt;

&lt;p&gt;In the early 1980s, psychologist George Ainslie discovered something peculiar. He &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperbolic_discounting#Observations&quot;&gt;found&lt;/a&gt; that while a lot of people would prefer &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;50 immediately rather than &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;100 in 6 months, they would &lt;em&gt;not&lt;/em&gt; prefer &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;50 in 3 months rather than &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;100 in 9 months. These two different scenarios are shown in the diagram below, where the green checks indicate the options that people tended to choose.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_hyperbolic_discounting/timeline_diagram.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;If you think about it, there’s something inconsistent about this behavior. The two scenarios are actually identical, just shifted by 3 months, and yet the same people behave differently depending on when the scenario would be presented. If we waited 3 months and then asked them again if they would prefer &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;50 immediately or &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;100 in 6 months, their original response to Scenario 1 implies they would prefer the fast money (i.e. they would have a high discount rate), but their original response to Scenario 2 implies they would prefer the delayed money (i.e. they would have a low discount rate). In other words, their present self today would make a decision in Scenario 2 that three months from now they will have regretted making. This behavior is &lt;em&gt;time-inconsistent&lt;/em&gt; and is therefore considered irrational according to standard economic theory.&lt;/p&gt;

&lt;p&gt;The usual way to achieve rational time-consistent discounting is with an exponential discount curve, where the value of receiving something at future time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is a fixed fraction &lt;script type=&quot;math/tex&quot;&gt;s(t)&lt;/script&gt; of its present value, and where &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is the constant discount rate.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s(t) = e^{-\lambda t}&lt;/script&gt;

&lt;p&gt;With an exponential curve, a dollar delayed by six months is always worth the same fixed fraction of a dollar at the baseline date, no matter what the baseline date it is. If you are indifferent between &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;100 now and &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;50 in 6 months, you should also be indifferent between &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;100 in 3 months and &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;50 in 9 months. The discount rate is constant.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_hyperbolic_discounting/fig_exp_fracs.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;In contrast to an exponential curve, humans tend to show a &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperbolic_discounting&quot;&gt;hyperbolic discount curve&lt;/a&gt;, which is considered irrational according to standard economic theory. Confusingly, the hyperbolic discount curve is not defined by one of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperbolic_function&quot;&gt;hyperbolic functions&lt;/a&gt; you may remember from high school. Instead, it is defined as follows, where $ \tau $ is the relative time from now:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s(\tau) = \frac{1}{1+k \tau}&lt;/script&gt;

&lt;p&gt;Whereas an exponential curve has a constant discount rate, a hyperbolic discount curve has a higher discount rate in the near future and lower discount rate in the distant future. That’s why the participants in Ainslie’s experiment cared more about the delay from 0 to 6 months than about the same delay from 3 to 9 months.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_hyperbolic_discounting/fig_exp_and_hyp.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The apparent rationality of an exponential function is often presented either with a hazard rate interpretation or an interest rate interpretation. It turns out, however, that both of these interpretations make implausible assumptions about the world. The rest of this blog post describes how if we make some more plausible assumptions, hyperbolic discount function becomes rational.&lt;/p&gt;

&lt;h2 id=&quot;hazard-rate-interpretation&quot;&gt;Hazard Rate interpretation&lt;/h2&gt;

&lt;p&gt;According to the hazard-based interpretation of discounting, you should prefer immediate money to future money because there is a risk that the future money will never be delivered. This interpretation is more common in the animal behavior literature. (&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2648524/&quot;&gt;Pigeons&lt;/a&gt;, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2648524/&quot;&gt;rats&lt;/a&gt;, and &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3107575/&quot;&gt;monkeys&lt;/a&gt; all show hyperbolic discounting.)&lt;/p&gt;

&lt;h5 id=&quot;apparent-rationality-of-exponential-discounting&quot;&gt;Apparent rationality of exponential discounting&lt;/h5&gt;

&lt;p&gt;Imagine that there could be an event at some time that could cause you to no longer receive your reward. Perhaps the person who owes you the money could die or lose their assets. Assuming a constant hazard rate (i.e. assuming the event is equally likely to happen at any time), the probability that the event has not happened by time $ \tau $ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s(\tau) = e^{-\lambda \tau}&lt;/script&gt;

&lt;p&gt;The &lt;script type=&quot;math/tex&quot;&gt;s(\tau)&lt;/script&gt; function is called a “survival” function because it describes the probability that the deal is still “alive” by time &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt;. The &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; parameter is the constant hazard rate.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_hyperbolic_discounting/fig_exp_survival.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;A key insight here is that the survival function can also be interpreted as a discount function. If someone says you can receive your reward at time $ \tau $ if you’re willing to wait for it, and if there’s only a 60% chance you’ll actually receive it, you should value that offer at 60% of the current value of the reward. Since the survival function is exponential, and since the survival function &lt;em&gt;is&lt;/em&gt; the discount function, your discount function is also exponential, at least according to standard economic theory.&lt;/p&gt;

&lt;h5 id=&quot;rationality-of-hyperbolic-discounting&quot;&gt;Rationality of hyperbolic discounting&lt;/h5&gt;

&lt;p&gt;The problem with assuming an exponential survival function is that it assumes you know the hazard rate $ \lambda $. In most situations, you don’t know exactly what the hazard rate is. Instead you have uncertainty around the parameter. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1689473/pdf/T9KA20YDP8PB1QP4_265_2015.pdf&quot;&gt;Souza (2015)&lt;/a&gt; proposes an exponential prior distribution over the hazard rate.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\lambda) = \frac{1}{k} e^{-\frac{\lambda}{k}}&lt;/script&gt;

&lt;p&gt;The exponential prior is shown in the graph below. Although this curve looks like a discount function, it is not. It is a distribution over a parameter.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_hyperbolic_discounting/fig_hazard_prior.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;According to Souza, if you average over all the possible exponential survival functions generated by this prior, you get a hyperbolic survival function, or equivalently, a hyperbolic discount function. Let’s see this in action.&lt;/p&gt;

&lt;p&gt;In the plot below, I’ve drawn 30 exponential survival functions in blue, each with a &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; sampled from the &lt;script type=&quot;math/tex&quot;&gt;p(\lambda)&lt;/script&gt; prior distribution defined above. The pink curve is the mean of all of them. Notice that whereas the individual survival curves are all exponential, their mean is hyperbolic, with a distant future characterized by a flat slope and relatively high survival values.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2018_hyperbolic_discounting/fig_mean_of_exponentials.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;As a consequence, the further you look into the future, the lower your discount rate. This is exactly what hyperbolic discounting is and exactly what humans do. For the choice between &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;50 now and &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;100 in 6 months, we apply a heavy discount rate. For the choice between &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;50 in 3 months and &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;100 in 9 months, we apply a lighter discount rate.&lt;/p&gt;

&lt;p&gt;For a more qualitative intuition, you can think of it this way: When you make a deal with someone, you don’t know what the hazard rate is going to be. But if the deal is still alive after 80 months, then your hazard rate is probably favorable and thus the deal is likely to still be alive after 100 months. You can therefore have a light discount rate between 80 months and 100 months.&lt;/p&gt;

&lt;h2 id=&quot;interest-rate-interpretation&quot;&gt;Interest Rate interpretation&lt;/h2&gt;

&lt;p&gt;While hazard functions are one way to explain discounting, another common explanation involves interest rates. A dollar now is worth more than a dollar in a year, because if you take the dollar now and invest it, it will worth more in a year.&lt;/p&gt;

&lt;h5 id=&quot;apparent-rationality-of-exponential-discounting-1&quot;&gt;Apparent rationality of exponential discounting&lt;/h5&gt;
&lt;p&gt;If the interest rate is 5%, you should be indifferent between &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;100 today and &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;105 in a year, since you could just invest the &lt;span class=&quot;tex2jax_ignore&quot;&gt;$&lt;/span&gt;100 today and get the same amount in a year. If the interest rate is constant, the value of the dollar will rise exponentially, according to $ v(\tau) = e^{0.05\tau} $, where $ v $ is the value. To rationally maintain indifference between a dollar and its equivalent value in the future after investment, your discount function should decay exponentially, according to $ s(\tau) = e^{-0.05\tau} $.&lt;/p&gt;

&lt;h5 id=&quot;rationality-of-hyperbolic-discounting-1&quot;&gt;Rationality of hyperbolic discounting&lt;/h5&gt;
&lt;p&gt;The problem with this story is that it only works if you assume that the interest rate is constant. In the real world, the interest rate fluctuates.&lt;/p&gt;

&lt;p&gt;Before taking on the fluctuating interest rate scenario, let’s first take on a different assumption that is still somewhat simplified. Let’s assume that the interest rate is constant but we don’t know what it is, just as we didn’t know what the hazard rate was in the previous interpretation. With this assumption, the justification for hyperbolic discounting becomes similar to the explanation in the blue and pink plots above. When you do a probability-weighted average over these decaying exponential curves, you get a hyperbolic function.&lt;/p&gt;

&lt;p&gt;The previous paragraph assumed that the interest rate was constant but unknown. In the real world, the interest rate is known but fluctuates over time. &lt;a href=&quot;https://campuspress.yale.edu/johngeanakoplos/files/2017/07/Hyperbolic-Discounting-is-Rational-Valuing-the-Far-Future-with-Uncertain-Discount-Rates-1uc47ft.pdf&quot;&gt;Farmer and Geanakoplos (2009)&lt;/a&gt; showed that if you assume that interest rate fluctuations follow a &lt;a href=&quot;http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/tutorials/sfehtmlnode21.html&quot;&gt;geometric random walk&lt;/a&gt;, hyperbolic discounting becomes optimal, at least asymptotically as &lt;script type=&quot;math/tex&quot;&gt;\tau \rightarrow \infty&lt;/script&gt;. In the near future, you know the interest rate with reasonable certainty and should therefore discount with an exponential curve. But as you look further into the future, your uncertainty about the interest rate increases and you should therefore discount with a hyperbolic curve.&lt;/p&gt;

&lt;p&gt;Is the geometric random walk a process that was cherry picked by the authors to produce this outcome? Not really. &lt;a href=&quot;https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/7542/NewellPizerDiscountingPew.pdf?sequence=1&quot;&gt;Newell and Pizer (2003)&lt;/a&gt; studied US bond rates in the 19th and 20th century and found that the geometric random walk provided a better fit than any of the other interest rate models tested.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;When interpreting discounting as a survival function, a hyperbolic discounting function is rational if you introduce uncertainty into the hazard parameter via an exponential prior (&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1689473/pdf/T9KA20YDP8PB1QP4_265_2015.pdf&quot;&gt;Souza, 2015&lt;/a&gt;). When interpreting the discount rate as an interest rate, a hyperbolic discounting function is asymptotically rational if you introduce uncertainty in the interest rate via a geometric random walk (&lt;a href=&quot;https://campuspress.yale.edu/johngeanakoplos/files/2017/07/Hyperbolic-Discounting-is-Rational-Valuing-the-Far-Future-with-Uncertain-Discount-Rates-1uc47ft.pdf&quot;&gt;Farmer and Geanakoplos, 2009&lt;/a&gt;).&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Religions as firms</title>
   <link href="http://localhost:4000/2017/12/02/religions-as-firms/"/>
   <updated>2017-12-02T00:00:00-08:00</updated>
   <id>http://localhost:4000/2017/12/02/religions-as-firms</id>
   <content type="html">&lt;p&gt;I recently came across a magazine that helps pastors manage the financial and operational challenges of church management. The magazine is called &lt;em&gt;Church Executive&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Readers concerned about seasonal effects on tithing can learn how to “&lt;a href=&quot;https://churchexecutive.com/archives/church-executive-remote-roundtable-2&quot;&gt;sustain generosity&lt;/a&gt;” during the weaker summer months. Technology like &lt;a href=&quot;https://churchexecutive.com/archives/church-executive-remote-roundtable-6&quot;&gt;push notifications and text messages&lt;/a&gt; is encouraged as a way to remind people to tithe. There is also some emphasis on messaging, as pastors are told to “make sure your &lt;a href=&quot;https://churchexecutive.com/archives/church-executive-remote-roundtable-6&quot;&gt;generosity-focused sermons&lt;/a&gt; are hitting home with your audience”.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_religions_as_firms/church_executive.jpg&quot; height=&quot;250&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Churches need money to stay active, and it’s natural that pastors would want to maintain a healthy cash flow. But the brazen language of &lt;em&gt;Church Executive&lt;/em&gt; reminded me of the language of profit-maximizing firms. This got me thinking: What are the other ways in which religions act like a business?&lt;/p&gt;

&lt;p&gt;This post is my attempt to understand religions as if they were businesses. This isn’t a perfect metaphor. Most religious leaders are motivated by genuine beliefs, and few are motivated primarily by profit. But it can still be instructive to view religions through the lens of business and economics, if only as an exercise. After working through this myself, I feel like I have a better understanding of why religions act the way they do.&lt;/p&gt;

&lt;h3 id=&quot;competition&quot;&gt;Competition&lt;/h3&gt;
&lt;p&gt;As with any business, one of the most pressing concerns of a religion is competition. According to sociologist Carl Bankston, the set of religions &lt;a href=&quot;https://en.wikipedia.org/wiki/Theory_of_religious_economy&quot;&gt;can be described&lt;/a&gt; as a marketplace of competing firms that vie for customers. Religious consumers can leave one church to go to another. To hedge their bets on the afterlife, some consumers may even belong to several churches simultaneously, in a strategy that has been described as “&lt;a href=&quot;http://users.clas.ufl.edu/kenwald/pos6292/iannaccone.pdf&quot;&gt;portfolio diversification&lt;/a&gt;”.&lt;/p&gt;

&lt;p&gt;One way that a religion can ward off competitors is to prohibit its members from following them. The Bible is insistent on this point, with &lt;a href=&quot;https://bible.knowing-jesus.com/topics/Avoiding-Idolatry&quot;&gt;26 separate verses&lt;/a&gt; banning idolatry. Other religions have been able to eliminate competition entirely by forming &lt;a href=&quot;https://en.wikipedia.org/wiki/Islam_in_Saudi_Arabia#Role_in_the_state_and_society&quot;&gt;state-sponsored monopolies&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;pricing&quot;&gt;Pricing&lt;/h3&gt;
&lt;p&gt;Just like a business, religions need to determine how to price their product. According to economists &lt;a href=&quot;https://www.researchgate.net/profile/Feler_Bose/publication/271328680_Funding_the_Faiths_Toward_a_Theory_of_Religious_Finance/links/54c5b8790cf219bbe4f59dac.pdf&quot;&gt;Laurence Iannaccone and Feler Bose&lt;/a&gt;, the optimal pricing strategy for a religion depends on whether it is proselytizing or non-proselytizing.&lt;/p&gt;

&lt;p&gt;Non-proselytizing religions like Judaism and Hinduism earn much of their income from &lt;a href=&quot;http://www.hinducenterofvirginia.org/Membership.html&quot;&gt;membership fees&lt;/a&gt;. While exceptions are often made for people who are too poor to pay, and while donations are still accepted, the explicit nature of the membership fees help these religions avoid having too many &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.483.5309&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;free riders&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Proselytizing religions like Christianity are &lt;a href=&quot;http://freakonomics.com/2010/10/19/churches-versus-synagogues-voluntary-donations-versus-dues/&quot;&gt;different&lt;/a&gt;. Because of their strong emphasis on growth, they are willing to forgo explicit membership fees and instead rely more on donations that are up to the member’s discretion. Large donations from wealthy individuals can &lt;a href=&quot;https://www.researchgate.net/profile/Feler_Bose/publication/271328680_Funding_the_Faiths_Toward_a_Theory_of_Religious_Finance/links/54c5b8790cf219bbe4f59dac.pdf&quot;&gt;cross-subsidize&lt;/a&gt; the membership of those who make smaller donations. Even free riders who make no donations at all &lt;a href=&quot;https://www.economics.uci.edu/files/docs/workingpapers/2006-07/06-07-22.pdf&quot;&gt;may be worthwhile&lt;/a&gt;, since they may attract more members in the future.&lt;/p&gt;

&lt;h3 id=&quot;surge-pricing&quot;&gt;Surge Pricing&lt;/h3&gt;
&lt;p&gt;Like Uber, some religions raise the price during periods of peak demand. While attendance at Jewish synagogue for a regular Shabbat service is normally free, attendance during one of the High Holidays typically &lt;a href=&quot;http://www.nytimes.com/2013/09/14/us/selling-holy-day-tickets-is-dilemma-for-synagogues.html&quot;&gt;requires a payment&lt;/a&gt; for seating, in part to &lt;a href=&quot;https://reformjudaism.org/jewish-holidays/rosh-hashanah/five-things-know-about-attending-high-holiday-services&quot;&gt;ensure space for everyone&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_religions_as_firms/traffic_synagogue.png&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Surge pricing makes sense for non-proselytizing religions such as Judaism, but it does not make sense for proselytizing religions such as Christianity, which views the higher demand during peak season as an &lt;a href=&quot;https://www.researchgate.net/profile/Feler_Bose/publication/271328680_Funding_the_Faiths_Toward_a_Theory_of_Religious_Finance/links/54c5b8790cf219bbe4f59dac.pdf&quot;&gt;opportunity&lt;/a&gt; to convert newcomers and to reactivate lapsed members. Thus, Christian churches tend to expand seating and schedule extra services during Christmas and Easter, rather than charging fees.&lt;/p&gt;

&lt;h3 id=&quot;product-quality&quot;&gt;Product Quality&lt;/h3&gt;
&lt;p&gt;Just as business consumers will &lt;a href=&quot;https://books.google.com/books/about/Acts_of_Faith.html?id=913O9xcpBB4C&amp;amp;printsec=frontcover&amp;amp;source=kp_read_button#v=onepage&amp;amp;q=%22higher%20prices%22&amp;amp;f=false&quot;&gt;pay&lt;/a&gt; higher prices for better products, consumers of polytheistic religions will pay higher “prices” for gods with more wide-ranging powers. Even today, some American megachurches have found success with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Prosperity_theology&quot;&gt;prosperity gospel&lt;/a&gt;, which emphasizes that God can make you wealthy.&lt;/p&gt;

&lt;p&gt;Of course, not all religious consumers will prefer the cheap promises of the prosperity gospel. For many religions, product quality is defined primarily by community, a sense of meaning, and in some cases the promise of an afterlife.&lt;/p&gt;

&lt;h3 id=&quot;software-updates&quot;&gt;Software Updates&lt;/h3&gt;
&lt;p&gt;A good business should be constantly updating its product to fix bugs and to respond to changes in consumer preference or government regulation. Some religions do the same thing, via the process of &lt;a href=&quot;https://en.wikipedia.org/wiki/Continuous_revelation&quot;&gt;continuous revelation&lt;/a&gt; from their deity. Perhaps no church exemplifies this better than the Church of Jesus Christ of Latter-day Saints.&lt;/p&gt;

&lt;p&gt;For most of the history of the Mormon Church, individuals of African descent were prohibited from serving as priests. By the 1960s, as civil rights protests against the church received media attention, the policy became increasingly untenable. On June 1, 1978, Mormon leaders reported that God had instructed them to update the policy and allow black priests. This event was known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/1978_Revelation_on_Priesthood&quot;&gt;1978 Revelation on the Priesthood&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the late 19th Century, when the Mormon Church was under intense pressure from the US Government regarding polygamy, the Church president claimed to receive a revelation from Jesus Christ asking him to prohibit it. This revelation, known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/1890_Manifesto&quot;&gt;1890 Revelation&lt;/a&gt;, overturned the previous &lt;a href=&quot;https://en.wikipedia.org/wiki/Origin_of_Latter_Day_Saint_polygamy#1843_revelation&quot;&gt;1843 Revelation&lt;/a&gt; which allowed polygamy.&lt;/p&gt;

&lt;p&gt;While frequent updates usually make sense in business, they don’t always make sense in religion. Most religions have a fairly static doctrine, as the prospect of future updates undermines the authority of current doctrine.&lt;/p&gt;

&lt;h3 id=&quot;growth-and-marketing&quot;&gt;Growth and marketing&lt;/h3&gt;
&lt;p&gt;Instead of focusing only on immediate profitability, many businesses invest in user growth. As mentioned earlier, many religions are willing to &lt;a href=&quot;https://www.researchgate.net/profile/Feler_Bose/publication/271328680_Funding_the_Faiths_Toward_a_Theory_of_Religious_Finance/links/54c5b8790cf219bbe4f59dac.pdf&quot;&gt;cross-subsidize&lt;/a&gt; participation from new members, especially young members, with older members bearing most of the costs.&lt;/p&gt;

&lt;p&gt;Christianity’s concept of a heaven and hell encouraged its members to convert their friends and family. In some ways, this is reminiscent of &lt;a href=&quot;https://en.wikipedia.org/wiki/Viral_marketing&quot;&gt;viral marketing&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;international-expansion&quot;&gt;International expansion&lt;/h3&gt;
&lt;p&gt;Facebook and Netflix both experienced rapid adoption, starting with a U.S. audience. But as U.S. growth began to slow down, both companies needed to look towards international expansion.&lt;/p&gt;

&lt;p&gt;A similar thing happened with the Mormon church. By the 20th century, U.S. growth was driven only by increasing family sizes, so the church turned towards international expansion.&lt;/p&gt;

&lt;p&gt;The graph below shows similar US and international growth curves for Netflix and the Church of Jesus Christ of Latter-day Saints.&lt;sup&gt;[&lt;a href=&quot;https://en.wikipedia.org/wiki/The_Church_of_Jesus_Christ_of_Latter-day_Saints_membership_history&quot;&gt;1&lt;/a&gt;,&lt;a href=&quot;https://rsc.byu.edu/archived/latter-day-saint-social-life-social-research-lds-church-and-its-members/4-vital-statistics&quot;&gt;2&lt;/a&gt;,&lt;a href=&quot;http://www.churchistrue.com/blog/lds-membership-statistics-report-2017/&quot;&gt;3&lt;/a&gt;,&lt;a href=&quot;http://www.businessinsider.com/netflix-subscribers-international-vs-us-earnings-chart-2017-7&quot;&gt;4&lt;/a&gt;]&lt;/sup&gt;&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_religions_as_firms/LDS_netflix.png&quot; style=&quot;position:relative border: #222 2px solid; max-width:100%;&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;branding&quot;&gt;Branding&lt;/h3&gt;
&lt;p&gt;Like any company, most religions try to maintain a good brand. But unlike businesses, most religions do not have brand protection, and thus their brands can be co-opted by other religions. Marketing from Mormons and from Jehovah’s Witnesses tends to &lt;a href=&quot;https://www.jw.org/en/jehovahs-witnesses/faq/jehovah-witness-beliefs/&quot;&gt;emphasize&lt;/a&gt; the good brand of Jesus Christ, even though most mainstream Christians regard these churches as heretical.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_religions_as_firms/follow_the_christ.jpg&quot; height=&quot;250&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;One of the most interesting risks to brands is &lt;a href=&quot;https://en.wikipedia.org/wiki/Generic_trademark&quot;&gt;&lt;em&gt;genericide&lt;/em&gt;&lt;/a&gt;, in which a popular trademark becomes synonymous with the general class of product, thereby diluting its distinctive meaning. Famous examples of generic trademarks include Kleenex and Band-Aid. Amazingly, genericide can also happen to religious deities. The ancient Near East god &lt;a href=&quot;https://en.wikipedia.org/wiki/El_(deity)&quot;&gt;&lt;em&gt;El&lt;/em&gt;&lt;/a&gt; began as a distinct god with followers, but &lt;a href=&quot;https://twitter.com/suchaone/status/869909666217607168&quot;&gt;gradually became a generic name for “God”&lt;/a&gt; and eventually merged with the Hebrew god Yahweh.&lt;/p&gt;

&lt;h3 id=&quot;mergers-and-spin-offs&quot;&gt;Mergers and spin-offs&lt;/h3&gt;
&lt;p&gt;In business, companies can spin off other companies or merge with other companies. But with rare exceptions, religions only seem to have spin-offs. Why do religions hardly ever merge with other religions? My guess is that since there is no protection for religious intellectual property, religions can acquire the intellectual property of another religion without requiring a merger. Religions can simply copy each other’s ideas.&lt;/p&gt;

&lt;p&gt;Another reason that religious mergers are rare is that religions are strongly tied to personal identity and tap into tribal thinking. When WhatsApp was acquired, its leadership was happy to adopt new identities as Facebook employees. But it is far less likely that members of, say, the Syriac Catholic Church would ever tolerate merging into the rival Syriac Maronite Church, even if it might provide them with economies of scale and more political power.&lt;/p&gt;

&lt;p&gt;On Twitter, I asked why there are so few religious mergers and got lots of interesting &lt;a href=&quot;https://twitter.com/Chris_Said/status/936635424012828672&quot;&gt;responses&lt;/a&gt;. People pointed out that reconciliation of doctrine could &lt;a href=&quot;https://twitter.com/KyleCSN/status/936637304688427009&quot;&gt;undermine the authority&lt;/a&gt; of the leaders, and that there is &lt;a href=&quot;https://twitter.com/AcostaRomay/status/936635959453585408&quot;&gt;little benefit&lt;/a&gt; from economies of scale. Others noted that religious mergers aren’t that rare. Hinduism and Judaism may have &lt;a href=&quot;https://twitter.com/imrankhan/status/936648636158611456&quot;&gt;began&lt;/a&gt; as mergers of smaller religions, many &lt;a href=&quot;https://twitter.com/thechelleshock/status/937033585638326272&quot;&gt;Christian traditions&lt;/a&gt; involve mergers with religions they replaced, and that even today Hinduism &lt;a href=&quot;https://twitter.com/venusatuluri/status/936664575591583745&quot;&gt;continues to be&lt;/a&gt; a merging of various sects.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It’s worth repeating that economic explanations aren’t always great at describing the &lt;em&gt;conscious&lt;/em&gt; motivations of religious individuals, who generally have sincere beliefs. Nevertheless, economic reasoning does a decent job of predicting the behavior of &lt;em&gt;systems&lt;/em&gt;, and it’s been pretty interesting to learn how religion is no exception.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Part 2&#58; A bipartisan list of people who argue in good faith</title>
   <link href="http://localhost:4000/2017/07/09/part-2-a-bipartisan-list-of-people-who-argue-in-good-faith/"/>
   <updated>2017-07-09T00:00:00-07:00</updated>
   <id>http://localhost:4000/2017/07/09/part-2-a-bipartisan-list-of-people-who-argue-in-good-faith</id>
   <content type="html">&lt;p&gt;In &lt;a href=&quot;/2017/07/09/part-1-a-bipartisan-list-of-people-who-are-bad-for-america/&quot;&gt;Part 1&lt;/a&gt;, I posted a bipartisan list of people who are bad for America. Those people present news stories that cherry pick the worst actions from the other side so that they can get higher TV ratings and more social media points.&lt;/p&gt;

&lt;p&gt;Here in Part 2, I post a list of people who don’t do that, at least for the most part. This isn’t a list of centrists. If anything, it is a more politically diverse list than the list in Part 1. This is a list of people who usually make good-faith attempts to persuade others about their point of view.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Megan McArdle&lt;/strong&gt; (&lt;a href=&quot;https://twitter.com/asymmetricinfo&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;https://www.bloomberg.com/view/contributors/AQjVOcPejrY/megan-mcardle&quot;&gt;Bloomberg&lt;/a&gt;) – Moderately libertarian ideas presented to a diverse audience&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Noah Smith&lt;/strong&gt; (&lt;a href=&quot;https://twitter.com/Noahpinion&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;https://www.bloomberg.com/view/contributors/AR3OYuAmvcU/noah-smith&quot;&gt;Bloomberg&lt;/a&gt;) – Center-left economics&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ross Douthat&lt;/strong&gt; (&lt;a href=&quot;https://twitter.com/DouthatNYT&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;https://www.nytimes.com/column/ross-douthat&quot;&gt;NYT&lt;/a&gt;) – Social conservatism presented to a left-of-center audience&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Noam Chomsky&lt;/strong&gt; (&lt;a href=&quot;https://chomsky.info/articles/&quot;&gt;Website&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conor Friedersdorf&lt;/strong&gt; (&lt;a href=&quot;https://www.theatlantic.com/author/conor-friedersdorf&quot;&gt;The Atlantic&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ben Sasse&lt;/strong&gt; — Has the third-most conservative voting record in the Senate but never caricatures the other side and is very concerned about filter bubbles.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Julia Galef&lt;/strong&gt; (&lt;a href=&quot;https://twitter.com/juliagalef&quot;&gt;Twitter&lt;/a&gt;) – Has some &lt;a href=&quot;https://twitter.com/juliagalef/status/850126407963262976&quot;&gt;great advice&lt;/a&gt; for understanding the other side&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Nicky Case&lt;/strong&gt; (&lt;a href=&quot;https://twitter.com/ncasenmare&quot;&gt;Twitter&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fareed Zakaria&lt;/strong&gt; (&lt;a href=&quot;https://www.washingtonpost.com/people/fareed-zakaria/&quot;&gt;Washington Post&lt;/a&gt;) – Center-left foreign policy&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Eli Lake&lt;/strong&gt; (&lt;a href=&quot;https://twitter.com/EliLake&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;https://www.bloomberg.com/view/contributors/ASD1bG3hdiI/eli-lake&quot;&gt;Bloomberg&lt;/a&gt;) – Hawkish foreign policy&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kevin Drum&lt;/strong&gt; (&lt;a href=&quot;http://www.motherjones.com/kevin-drum/&quot;&gt;Mother Jones&lt;/a&gt;) – Center-left blogger who writes in good faith&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;John Carl Baker&lt;/strong&gt; (&lt;a href=&quot;https://twitter.com/johncarlbaker&quot;&gt;Twitter&lt;/a&gt;) – One of the few modern socialists I have found who avoids in-group snark.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Michael Dougherty&lt;/strong&gt; (&lt;a href=&quot;https://twitter.com/michaelbd&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;http://theweek.com/authors/michael-brendan-dougherty&quot;&gt;The Week&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reihan Salam&lt;/strong&gt; (&lt;a href=&quot;https://twitter.com/reihan&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;http://www.nationalreview.com/author/reihan-salam&quot;&gt;NRO&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Avik Roy&lt;/strong&gt; (&lt;a href=&quot;https://twitter.com/Avik&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;http://www.nationalreview.com/author/avik-roy&quot;&gt;NRO&lt;/a&gt;) – Conservative health care&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ezra Klein&lt;/strong&gt; (&lt;a href=&quot;https://www.vox.com/authors/ezra-klein&quot;&gt;Vox&lt;/a&gt;, &lt;a href=&quot;http://prospect.org/authors/ezra-klein&quot;&gt;early days at the American Prospect&lt;/a&gt;) – While at the American Prospect, Ezra did an amazing job trying to persuade people about the benefits of Obamacare. Vox, the explainer site that he started, sometimes slips into red meat clickbait. But to its credit, Vox has managed to reach a wide audience with mostly explainer content.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reading the people on this list with an open mind will broaden your worldview.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Part 1&#58; A bipartisan list of people who are bad for America</title>
   <link href="http://localhost:4000/2017/07/09/part-1-a-bipartisan-list-of-people-who-are-bad-for-america/"/>
   <updated>2017-07-09T00:00:00-07:00</updated>
   <id>http://localhost:4000/2017/07/09/part-1-a-bipartisan-list-of-people-who-are-bad-for-america</id>
   <content type="html">&lt;p&gt;Imagine that alien researchers visited America to learn about our political culture. If they wrote a report to send back to their planet, I imagine it would look something like this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Americans have split themselves up into two opposing political tribes. Most people who associate with these groups are well-intentioned, but occasionally some members of a tribe do something bad or say something dumb. Whenever this happens, members of the opposite side feel good about themselves.&lt;/p&gt;

  &lt;p&gt;Certain writers and media personalities have learned to exploit this fact for personal gain. They have found that they can maximize their TV ratings and social media points by writing news stories that either cherry pick the worst actions of the other side or which interpret the other side’s actions in the least charitable way possible. As a result, news readers have developed increasingly distorted beliefs about their political opponents. The civic culture of the society is broken.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Below is a bipartisan list of people who are stoking partisan outrage for personal gain. Some of them do it for retweets, some of them do it for TV ratings, and some of them – still culpable – do it because they have entered a filter bubble themselves, fueling their own distorted and harmful sense of mission.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sean Davis&lt;/strong&gt; (The Federalist) His Twitter account is deliberately uncharitable.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sopan Deb&lt;/strong&gt; (New York Times) During the presidential campaign, his Twitter feed was nonstop, “look what this stupid Trump supporter said”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stephen Miller&lt;/strong&gt; (The Wilderness, ex-NRO)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Chris Cillizza&lt;/strong&gt; (The Washington Post)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sean Hannity&lt;/strong&gt; (Fox News)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tucker Carlson&lt;/strong&gt; (Fox News)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Samantha Bee&lt;/strong&gt; I know, she’s a comedian. I like jokes. But given how many people get their news from selectively edited comedy shows, it’s fair to say that comedians bear some responsibility.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;John Oliver&lt;/strong&gt; (HBO) It pains me to include him on this list, since he is funny and since his show also includes some constructive &lt;a href=&quot;https://www.youtube.com/watch?v=Nn_Zln_4pA8&quot;&gt;policy explainers&lt;/a&gt;. But much of the content is selectively edited clips that paint a very distorted picture of the other side.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Rachel Maddow&lt;/strong&gt; (MSNBC)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Shaun King&lt;/strong&gt; (Facebook personality)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Greg Gutfeld&lt;/strong&gt; (Fox News comedian)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It doesn’t matter if some of the people on this list do accurate reporting. What matters is that their reporting is &lt;em&gt;selective&lt;/em&gt;. It doesn’t matter if some of the people on this list support some good policy ideas. What matters is that listening to them will destroy your brain’s ability to understand where the other side is coming from. And it doesn’t matter if one side is more filter-bubbled than the other. Both sides are badly filter-bubbled. Avoiding the people in this list is a good place to start.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;/2017/07/09/part-2-a-bipartisan-list-of-people-who-argue-in-good-faith/&quot;&gt;Part 2&lt;/a&gt;, I’ll post a bipartisan list of people who argue in good faith.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Three questions for social scientists&#58; Internet virtue edition</title>
   <link href="http://localhost:4000/2017/06/19/three-questions-for-social-scientists/"/>
   <updated>2017-06-19T00:00:00-07:00</updated>
   <id>http://localhost:4000/2017/06/19/three-questions-for-social-scientists</id>
   <content type="html">&lt;p&gt;This isn’t news to anybody, but the internet is changing our culture. Recently, I’ve been thinking about how it has changed our moral culture, and I realized that most of our beliefs on this topic are weirdly in tension with one another. Below are three questions that I feel are very much unresolved. I don’t have any good answers to them, and so I think they might be good topics for social science research.&lt;/p&gt;

&lt;h4 id=&quot;1-moral-substitution-and-moral-licensing-versus-moral-contagion&quot;&gt;1. Moral Substitution and Moral Licensing versus Moral Contagion&lt;/h4&gt;
&lt;p&gt;When people do the Ice Bucket Challenge or put a Pride symbol on their profile avatar, they are sometimes accused of &lt;em&gt;virtue signalling&lt;/em&gt;, a derogatory term akin to &lt;em&gt;moral grandstanding&lt;/em&gt;. Virtue signallers are said to care more about showcasing their virtue than about creating real change.&lt;/p&gt;

&lt;p&gt;Virtue signalling is bad, allegedly, for two reasons:
First, instead of performing truly impactful moral acts, virtue signallers spend more time performing easy and symbolic acts. This could be called &lt;em&gt;moral substitution&lt;/em&gt;.
Second, after doing something good, people often feel like they’ve earned enough virtue points that they can get away with doing something bad. This well-studied phenomenon is called &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Self-licensing&quot;&gt;moral licensing&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_three_questions/fig_kony.jpg&quot; height=&quot;250&quot; class=&quot;inner&quot; style=&quot;position:relative; border: #ccc 1px solid;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;While there are some clear ways that virtue signalling can be bad, there is another way in which it is good. Doing good things makes other people more likely to do good things. This process, known as &lt;em&gt;moral contagion&lt;/em&gt;, was famously demonstrated in the Milgram experiments. Participants in those experiments who saw other participants behave morally were &lt;a href=&quot;https://www.saylor.org/site/wp-content/uploads/2011/08/PSYCH202A-3.1.1-Milgram-experiment.pdf&quot;&gt;dramatically more likely&lt;/a&gt; to behave morally as well.&lt;/p&gt;

&lt;p&gt;If the social science research is right, then we can conclude that putting a Pride symbol on your avatar make you behave &lt;em&gt;worse&lt;/em&gt; (via moral licensing and moral substitution), but it makes other people behave &lt;em&gt;better&lt;/em&gt; (via moral contagion). This leaves a couple of open questions:&lt;/p&gt;

&lt;p&gt;First, how do the pros and cons balance out? Perhaps your Pride avatar is net positive if you have a large audience on Facebook, but net negative if you have a small audience. And second, how does moral contagion work with symbolic acts? Does the Pride avatar just make other people add Pride symbols to their avatars? Or does it make them behave more ethically in real and impactful ways?&lt;/p&gt;

&lt;p&gt;We are beginning to get &lt;a href=&quot;http://smart-meter-analytics.de/downloads/papers/Tiefenbeck%20-%20For%20better%20or%20for%20worse.pdf&quot;&gt;some&lt;/a&gt; quantitative answers to these questions. Clever research from &lt;a href=&quot;https://twitter.com/LindaSkitka&quot;&gt;Linda Skitka&lt;/a&gt; and others has &lt;a href=&quot;https://www.researchgate.net/profile/Linda_Skitka/publication/265606809_Morality_in_everyday_life/links/57457f2608ae9f741b410416.pdf&quot;&gt;shown&lt;/a&gt; that committing a moral act makes you about 40% &lt;em&gt;less&lt;/em&gt; likely to commit another moral act later in the day (moral licensing), whereas hearing about someone else’s moral act makes you about 25% &lt;em&gt;more&lt;/em&gt; likely to commit a moral act later in the day (moral contagion), although the latter finding fell short of statistical significance. More research is needed though, particularly when it comes to social media and symbolic virtue signalling.&lt;/p&gt;

&lt;h4 id=&quot;2-slacktivism-versus-violent-revolution&quot;&gt;2. Slacktivism versus Violent Revolution&lt;/h4&gt;

&lt;p&gt;This question is more for political scientists.&lt;/p&gt;

&lt;p&gt;Many people are concerned that the internet encourages &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Slacktivism&quot;&gt;slacktivism&lt;/a&gt;&lt;/em&gt;, a phenomenon closely related to &lt;em&gt;moral substitution&lt;/em&gt;. It’s easier to slap a Pride symbol on your Facebook than to engage in real activism. In this way, the internet is really a tool of the already powerful.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_three_questions/fig_homer.png&quot; height=&quot;250&quot; class=&quot;inner&quot; style=&quot;position:relative border: #222 2px solid;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;On the other hand, some people are concerned that the internet cultivates &lt;em&gt;violent radicalism&lt;/em&gt;. Online filter bubbles create anger and online networks create alliances, ultimately leading to violent rhetoric and homegrown terrorism.  Many observers already sense the undercurrents of violent revolution.&lt;/p&gt;

&lt;p&gt;How can we be worried that the internet is causing &lt;em&gt;both&lt;/em&gt; slacktivism and violent radicalism? One possibility is that we only need to worry about slacktivism, and that the violent rhetoric isn’t actually violent – it’s just rhetoric. But the &lt;a href=&quot;http://firstmonday.org/article/view/3336/2767&quot;&gt;other&lt;/a&gt; possibility is that the internet has made slacktivists out of people who otherwise wouldn’t be doing anything at all, and it has made violent radicals out of people who would otherwise be mere activists. I’m not sure what the answer is, but &lt;a href=&quot;https://www.nytimes.com/2017/05/10/opinion/crisis-or-stasis.html?_r=0&quot;&gt;it would be useful&lt;/a&gt; to understand this more.&lt;/p&gt;

&lt;h4 id=&quot;3-political-correctness-overton-windows-versus-wolf-crying&quot;&gt;3. Political Correctness: Overton Windows versus Wolf Crying&lt;/h4&gt;
&lt;p&gt;Perhaps because of filter bubbles on both sides of the political spectrum, the term “political correctness” is back with a vengeance. Leaving aside the question of whether political correctness is good or bad, it would be interesting to understand whether it is effective. On the one hand, political correctness may help define an &lt;a href=&quot;https://en.wikipedia.org/wiki/Overton_window&quot;&gt;Overton Window&lt;/a&gt;, setting useful bounds around opinions that can be aired in polite company. But on the other hand, if the enforcers squeeze the boundaries too much, imposing stricter and stricter controls on the range of acceptable discourse, they risk undermining their own credibility by “crying wolf”. For what it’s worth, many internet trolls credit their success to a perception that social justice activists overplayed their cards. I’m not sure how much to believe them, but it seems possible.&lt;/p&gt;

&lt;p&gt;Just in terms of effectiveness, is there a point at which political correctness starts to backfire? And more broadly, what is the optimal level of political correctness for a society? Neither of these questions seems easy to answer, but I would love to learn more.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Keyboard shortcuts I couldn't live without</title>
   <link href="http://localhost:4000/2017/05/22/keyboard-shortcuts-i-couldnt-live-without/"/>
   <updated>2017-05-22T00:00:00-07:00</updated>
   <id>http://localhost:4000/2017/05/22/keyboard-shortcuts-i-couldnt-live-without</id>
   <content type="html">&lt;p&gt;Keyboard shortcuts are interesting. Even though I know they are almost always worth learning, I often find myself shying away from the uncomfortable task of actually learning them. But after years of clumsily reaching for the mouse while my colleagues looked at me with a kindly sense of pity, I have slowly accumulated enough keyboard tricks that I’d like to share them. This set is probably far from optimal, and different people have their own solutions, but it has worked well for me. Before jumping in, here’s a reference table of key symbols and their common names:&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
&lt;table style=&quot;font-size:.7rem; margin: auto; width: 250px&quot;&gt;
    &lt;tr&gt;
        &lt;th&gt;Key Symbol&lt;/th&gt;
        &lt;th&gt;Key Name&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align=&quot;center&quot;&gt;⌘&lt;/td&gt;
        &lt;td&gt;Command&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align=&quot;center&quot;&gt;⇧&lt;/td&gt;
        &lt;td&gt;Shift&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align=&quot;center&quot;&gt;⌃&lt;/td&gt;
        &lt;td&gt;Control&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align=&quot;center&quot;&gt;⌥&lt;/td&gt;
        &lt;td&gt;Alt/Option&lt;/td&gt;
    &lt;/tr&gt;
     &lt;tr&gt;
        &lt;td align=&quot;center&quot;&gt;↵&lt;/td&gt;
        &lt;td&gt;Enter&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Table 1.&lt;/strong&gt; Key symbol map.&lt;/div&gt;

&lt;h3 id=&quot;sublime-text&quot;&gt;Sublime Text&lt;/h3&gt;
&lt;p&gt;Sublime has &lt;a href=&quot;http://docs.sublimetext.info/en/latest/reference/keyboard_shortcuts_osx.html&quot;&gt;lots&lt;/a&gt; of great shortcuts. My favorites is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘D&lt;/code&gt;, which allows you to sequentially select exact matches of highlighted text. Once the matches are selected, you can simultaneously edit them with a multi-cursor. If you want to select all matches simultaneously, rather than sequentially, you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌃⌘G&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_keyboard_shortcuts/gif_sublime_shining_fast.gif&quot; width=&quot;350&quot; class=&quot;inner&quot; style=&quot;position:relative&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;
    &lt;strong&gt;Figure 1.&lt;/strong&gt; Demonstration of &lt;code&gt;⌘D&lt;/code&gt;, &lt;code&gt;⌘←&lt;/code&gt;, &lt;code&gt;⌘→&lt;/code&gt;, &lt;code&gt;⌘A&lt;/code&gt; and &lt;code&gt;⌘KU&lt;/code&gt; in Sublime Text.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;chrome&quot;&gt;Chrome&lt;/h3&gt;
&lt;p&gt;With the exception of scrolling and link clicking, everything you do in Chrome should be done with keyboard only. If you’re new to this, I’d recommend starting with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘L&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘T&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘W&lt;/code&gt; and then expanding from there. Special bonus:	 if you ever accidentally close a tab, you can reopen it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘⇧T&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/2017_keyboard_shortcuts/no_touching.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; The &quot;No Touching&quot; Chrome Zone. Your mouse should never come anywhere near here.&lt;/div&gt;

&lt;h3 id=&quot;mac-os-x&quot;&gt;Mac OS X&lt;/h3&gt;
&lt;p&gt;On Mac OS X, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘ Tab&lt;/code&gt; switches between applications, and &lt;code&gt;⌘`&lt;/code&gt; switches between windows of the same application. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘+&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘-&lt;/code&gt; shortcuts change the display size of text and other items. You can jump to the beginning of a line with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘←&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌃A&lt;/code&gt;, and to the end of a line with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘→&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌃E&lt;/code&gt;. In Terminal, you can delete to the beginning of a line with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌃U&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For easy window snapping, use the &lt;a href=&quot;https://www.spectacleapp.com/&quot;&gt;Spectacle&lt;/a&gt; app. Because Spectacle’s default mappings conflict with Chrome’s tab switching shortcuts, I’d recommend setting the four main screen position shortcuts to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘⌃←&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘⌃→&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘⌃↑&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘⌃↓&lt;/code&gt;, and eliminating all the other shortcuts, except the full screen shortcut, which should be set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌘⌃F&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_keyboard_shortcuts/gif_spectacle.gif&quot; width=&quot;350&quot; class=&quot;inner&quot; style=&quot;position:relative; border: #666666 2px solid;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;
    &lt;strong&gt;Figure 3.&lt;/strong&gt; Arranging windows with custom shortcuts in Spectacle.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;gmail&quot;&gt;Gmail&lt;/h3&gt;
&lt;p&gt;If you’re using a mouse on Gmail, you’re doing it wrong. With the exception of a few word processing operations, literally everything you do in Gmail should be done with keyboard only. Compose, Reply, Reply All, Forward, Send, Search, Navigate, Open, Inbox, Sent, Drafts, Archive. All of these should be done with the keyboard. To enable these shortcuts, you must go into your Settings and navigate to the General tab. Once shortcuts have been enabled, you can see a list of all them by typing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&lt;/code&gt;.&lt;/p&gt;
&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/2017_keyboard_shortcuts/fig_gmail_annotated.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; A small sample of things you can do on Gmail without ever touching your mouse.&lt;/div&gt;

&lt;h3 id=&quot;twitter&quot;&gt;Twitter&lt;/h3&gt;
&lt;p&gt;With shortcuts similar to Gmail’s, you can jump to different pages using only the keyboard: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gh&lt;/code&gt; brings you to the Home Timeline and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gu&lt;/code&gt; lets you jump to another user’s profile. The most useful shortcut is probably &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.&lt;/code&gt;, which loads any new tweets that are waiting for you at the top of the Timeline. You can see a list of all shortcuts by typing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;jetbrains&quot;&gt;JetBrains&lt;/h3&gt;
&lt;p&gt;JetBrains products like DataGrip, PyCharm, and IntelliJ offer plenty of keyboard shortcuts. My favorites are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌃G&lt;/code&gt;, for sequential highlighting, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌥⌥↓&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;⌥⌥↑&lt;/code&gt; for &lt;a href=&quot;https://www.jetbrains.com/help/idea/2017.1/multicursor.html&quot;&gt;multi-line cursors&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;jupyter&quot;&gt;Jupyter&lt;/h3&gt;
&lt;p&gt;Jupyter has tons of &lt;a href=&quot;http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Notebook%20Basics.html?highlight=keyboard#Keyboard-Navigation&quot;&gt;essential&lt;/a&gt; keyboard shortcuts that can be found by typing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&lt;/code&gt; while in command mode. In addition, it’s possible to get Sublime-style text editing by following the instructions described &lt;a href=&quot;http://blog.rtwilson.com/how-to-get-sublime-text-style-editing-in-the-ipythonjupyter-notebook/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_keyboard_shortcuts/gif_jupyter.gif&quot; width=&quot;350&quot; class=&quot;inner&quot; style=&quot;position:relative; border: #ccc 1px solid;&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;
    &lt;strong&gt;Figure 5.&lt;/strong&gt; Common Jupyter workflow done entirely with the keyboard, with help from some Sublime-style editing: &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt; to copy and paste a cell, &lt;code&gt;⌘D&lt;/code&gt; for multiple selection, &lt;code&gt;⌘→&lt;/code&gt; to jump to the end of line, &lt;code&gt;dd&lt;/code&gt; to delete a cell.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Learning by flip-flopping</title>
   <link href="http://localhost:4000/2017/05/08/learning-by-flip-flopping/"/>
   <updated>2017-05-08T00:00:00-07:00</updated>
   <id>http://localhost:4000/2017/05/08/learning-by-flip-flopping</id>
   <content type="html">&lt;p&gt;I recently came across &lt;a href=&quot;https://nintil.com/&quot;&gt;Artir&lt;/a&gt;’s &lt;a href=&quot;https://nintil.com/2016/03/06/the-pyramid-of-economic-insight-and-virtue/&quot;&gt;Pyramid of Economic Insight and Virtue&lt;/a&gt;. It’s not actually a pyramid, but is instead a riff on the Expanding Brain meme. &lt;a href=&quot;https://twitter.com/ArtirKel/status/843886928135159819&quot;&gt;Check it out&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_learning_by_flip_flopping/fig_artir.jpeg&quot; width=&quot;400&quot; class=&quot;inner&quot; style=&quot;position:relative&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;What’s interesting about Artir’s Pyramid is that at every step, the position flip-flops from the previous step. This isn’t just a dialogue between two sides. It is a description of the belief sequence that people traverse as they learn more about an issue. We might call this &lt;em&gt;learning by flip-flopping&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This got me thinking: In what other issues do people go through a sequence of flip-flops as they learn more about it? In this blog post, I’d like to suggest a few.&lt;/p&gt;

&lt;p&gt;Let me stress that in presenting these I don’t necessarily think that the “highest” levels in these examples are correct, nor do I think I have a strong understanding on many of these issues. It’s just something that’s fun to think about.&lt;/p&gt;

&lt;h3 id=&quot;increasing-the-minimum-wage&quot;&gt;Increasing the minimum wage&lt;/h3&gt;
&lt;p&gt;This is arguably a special case of Artir’s Pyramid and is probably the canonical example of learning by flip-flopping.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_learning_by_flip_flopping/fig_minimum_wage.png&quot; width=&quot;400&quot; class=&quot;inner&quot; style=&quot;position:relative&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;When it comes to the minimum wage debate and other debates, I often sometimes see a Stage 2 person talking to someone they believe is at Stage 1 but who is in fact at Stage 3.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_learning_by_flip_flopping/fig_mansplaining.jpg&quot; width=&quot;400&quot; class=&quot;inner&quot; style=&quot;position:relative&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Further reading on the minimum wage&lt;/em&gt;: &lt;a href=&quot;http://www.nber.org/papers/w4509&quot;&gt;Card and Krueger&lt;/a&gt;, &lt;a href=&quot;http://www.uvm.edu/~vlrs/doc/min_wage.htm&quot;&gt;criticism&lt;/a&gt; of Card and Krueger’s data, another &lt;a href=&quot;https://www.forbes.com/sites/timworstall/2015/08/01/why-the-card-and-krueger-paper-on-minimum-wages-rises-and-unemployment-is-wrong&quot;&gt;case&lt;/a&gt; against Card and Krueger, two &lt;a href=&quot;http://cepr.net/documents/publications/min-wage-2013-02.pdf&quot;&gt;better&lt;/a&gt; &lt;a href=&quot;http://irle.berkeley.edu/files/2010/Minimum-Wage-Effects-Across-State-Borders.pdf&quot;&gt;studies&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;how-to-deal-with-a-recession&quot;&gt;How to deal with a recession&lt;/h3&gt;
&lt;p&gt;Recession flip-flopping is less related to Artir’s Pyramid, but is still quite common. I may be bungling some of the later stages here, as my macro knowledge is mostly cobbled together from &lt;a href=&quot;https://www.youtube.com/watch?v=GTQnarzmTOc&quot;&gt;parody rap videos&lt;/a&gt;, so I welcome any suggestions for additional further reading.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_learning_by_flip_flopping/fig_recessions.jpg&quot; width=&quot;400&quot; class=&quot;inner&quot; style=&quot;position:relative&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Further reading on recessions&lt;/em&gt;: &lt;a href=&quot;https://www.theguardian.com/money/us-money-blog/2013/mar/26/federal-budget-household-finances-fed&quot;&gt;The government is not a household&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Keynesian_economics&quot;&gt;Keynesian economics&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=GTQnarzmTOc&quot;&gt;boom and bust cycles&lt;/a&gt;, and a &lt;a href=&quot;https://www.amazon.com/Undercover-Economist-Strikes-Back-Ruin/dp/1594631409&quot;&gt;wonderful book&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/TimHarford&quot;&gt;Tim Harford&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;twitters-140-character-limit&quot;&gt;Twitter’s 140 character limit&lt;/h3&gt;
&lt;p&gt;I’d like to keep this blog post as value-judgment free as possible, but I’ll make a special exception for this one. The 140 character limit is no longer a good idea, and Stage 3 is the correct stage.&lt;/p&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_learning_by_flip_flopping/fig_twitter.jpg&quot; width=&quot;400&quot; class=&quot;inner&quot; style=&quot;position:relative&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;the-meaning-of-life&quot;&gt;The meaning of life&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://meaningness.com/metablog/stem-fluidity-bridge&quot;&gt;David Chapman&lt;/a&gt; writes about how STEM-trained people should think about meaning. Extending Robert Kegen’s theory of human development, he believes that most STEM-trained people can find meaning in ideological rationalism (Stage 4) but, upon finding that rationality does not provide any meaning, they become in danger of falling into the Nihilism trap (Stage 4.5). Chapman claims that there is a Stage 5, sometimes called meta-rationality or fluidity, in which meaning can once again be found. You can read more about it on his &lt;a href=&quot;https://meaningness.com/&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/2017_learning_by_flip_flopping/fig_meaning.png&quot; width=&quot;400&quot; class=&quot;inner&quot; style=&quot;position:relative&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;What other examples of learning by flip-flopping are out there?&lt;/p&gt;

&lt;p&gt;UPDATE: &lt;a href=&quot;https://twitter.com/johnvmcdonnell&quot;&gt;John McDonnell&lt;/a&gt; points me towards &lt;a href=&quot;https://en.wikipedia.org/wiki/Dialectic#Hegelian_dialectic&quot;&gt;Hegelian Dialectic&lt;/a&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Empirical Bayes for multiple sample sizes</title>
   <link href="http://localhost:4000/2017/05/03/empirical-bayes-for-multiple-sample-sizes/"/>
   <updated>2017-05-03T00:00:00-07:00</updated>
   <id>http://localhost:4000/2017/05/03/empirical-bayes-for-multiple-sample-sizes</id>
   <content type="html">&lt;p&gt;Here’s a data problem I encounter &lt;em&gt;all the time&lt;/em&gt;. Let’s say I’m running a website where users can submit movie ratings on a continuous 1-10 scale. For the sake of argument, let’s say that the users who rate each movie are an unbiased random sample from the population of users. I’d like to compute the average rating for each movie so that I can create a ranked list of the best movies.&lt;/p&gt;

&lt;p&gt;Take a look at my data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017_shrinkage/fig_movies.png&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Each circle represents a movie rating by a user. Diamonds represent sample means for each movie.&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
I’ve got two big problems here. First, nobody is using my website. And second, I’m not sure if I can trust these averages. The movie at the top of the rankings was only rated by two users, and I’ve never even heard of it! Maybe the movie really is good. But maybe it just got lucky. Maybe just by chance, the two users who gave it ratings happened to be the users who liked it to an unusual extent. It would be great if there were a way to adjust for this.&lt;/p&gt;

&lt;p&gt;In particular, I would like a method that will give me an estimate closer to the movie’s &lt;em&gt;true mean&lt;/em&gt; (i.e. the mean rating it would get if an infinite number of users rated it). Intuitively, movies with mean ratings at the extremes should be nudged, or “shrunk”, towards the center. And intuitively, movies with low sample sizes should be shrunk more than the movies with large sample sizes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017_shrinkage/fig_movies_shrinkage.png&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; Each circle represents a movie rating by a user. Diamonds represent sample means for each movie. Arrows point towards shrunken estimates of each movie's mean rating. Shrunken estimates are obtained using the MSS James-Stein Estimator, described in more detail below.&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As common a problem as this is, there aren’t a lot of accessible resources describing how to solve it. There are tons of &lt;a href=&quot;http://varianceexplained.org/r/empirical_bayes_baseball/&quot;&gt;excellent blog posts&lt;/a&gt; about the Beta distribution, which is useful if you wish to estimate the true fraction of an occurrence among some events. This works well in the case of Rotten Tomatoes, where one might want to know the true fraction of “thumbs up” judgments. But in my case, I’m dealing with a continuous 1-10 scale, not thumbs up / thumbs down judgments. The Beta-binomial distribution will be of little use.&lt;/p&gt;

&lt;p&gt;Many resources mention the James-Stein Estimator, which provides a way to shrink the mean estimates only when the variances of those means can be assumed to be equal. That assumption usually only holds when the &lt;em&gt;sample sizes&lt;/em&gt; of each group are equal. But in most real world examples, the sample sizes (and thus the variances of the means) are not equal. When that happens, it’s a lot less clear what to do.&lt;/p&gt;

&lt;p&gt;After doing a lot of digging and &lt;a href=&quot;https://twitter.com/Chris_Said/status/851211601445240832&quot;&gt;asking&lt;/a&gt; some very helpful folks on Twitter, I found several solutions. For many of the solutions, I ran simulations to determine which worked best. This blog post is my attempt to summarize what I learned. Along the way, we’ll cover the original James-Stein Estimator, two extensions to the James-Stein Estimator, Markov Chain Monte Carlo (MCMC) methods, and several other strategies.&lt;/p&gt;

&lt;p&gt;Before diving in, I want to include a list of symbol definitions I’ll be using because – side rant – it sure would be great if all stats papers did this, given that literally every paper I read used its own idiosyncratic notations! I’ll define everything again in the text, but this is just here for reference:&lt;/p&gt;

&lt;div style=&quot;margin:40px&quot;&gt;
&lt;table style=&quot;font-size:.7rem&quot;&gt;
    &lt;tr&gt;
        &lt;th&gt;Symbol&lt;/th&gt;
        &lt;th&gt;Definition&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ k $&lt;/td&gt;
        &lt;td&gt;The number of groups.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \theta_i $&lt;/td&gt;
        &lt;td&gt;The true mean of a group.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ x_i $&lt;/td&gt;
        &lt;td&gt;The sample mean of a group. The MLE estimate of $ \theta_i $.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \epsilon^{2}_i $&lt;/td&gt;
        &lt;td&gt;The true variance of observations within a group.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \epsilon^2 $&lt;/td&gt;
        &lt;td&gt;The true variance of observations within a group if we assume all groups have the same variance.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ s^{2}_i $&lt;/td&gt;
        &lt;td&gt;The sample variance of a group. The MLE estimate of $ \epsilon^{2}_i $.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ n_i $&lt;/td&gt;
        &lt;td&gt;The number of observations in a group.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ n $&lt;/td&gt;
        &lt;td&gt;The number of observations in a group, if we assume all groups have the same size.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \sigma^{2}_i $&lt;/td&gt;
        &lt;td&gt;The true variance of a group's mean. If each group has the same variance of observations, then $ \sigma^{2}_i  = \epsilon^{2} / n_i $. If each group has different variances of observations, then $ \sigma^{2}_i = \epsilon^{2}_i / n_i $.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \sigma^{2} $&lt;/td&gt;
        &lt;td&gt;Like $ \sigma^{2}_i $, but if we assume all groups had the same variance of the mean. Equal to $ \epsilon^{2} / n $.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \hat{\sigma^{2}_i} $&lt;/td&gt;
        &lt;td&gt;Estimate of $ \sigma^{2}_i $.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \hat{\sigma^{2}} $&lt;/td&gt;
        &lt;td&gt;Estimate of $ \sigma^{2} $. 
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \mu $&lt;/td&gt;
        &lt;td&gt;The true mean of the $ \theta_i $'s (the true group means). The mean of the distribution from which the $ \theta_i $'s are drawn.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \overline{X} $&lt;/td&gt;
        &lt;td&gt;The sample mean of the sample means.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \tau^{2} $&lt;/td&gt;
        &lt;td&gt;The true variance of the $ \theta_i $'s (the true group means). The variance of the distribution from which the $ \theta_i $'s are drawn.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \hat{\tau^{2}} $&lt;/td&gt;
        &lt;td&gt;Estimate of $ \tau^{2} $.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \hat{B} $&lt;/td&gt;
        &lt;td&gt;Estimate of the best term for weighting $ x_i $ and $ \overline{X} $ when calculating $ \hat{\theta_i} $. Assumes each group has the same $ \sigma^2 $.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \hat{B_i} $&lt;/td&gt;
        &lt;td&gt;Estimate of the best term for weighting $ x_i $ and $ \overline{X} $ when calculating $ \hat{\theta_i} $. Does not assume that all group's have the same $ \sigma^{2}_i $.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \hat{\theta_i} $&lt;/td&gt;
        &lt;td&gt;Estimate of a true group means. Its value depends on the method we use.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ k_{\Gamma} $&lt;/td&gt;
        &lt;td&gt;Shape parameter for the Gamma distribution from which sample sizes are drawn.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \theta_{\Gamma} $&lt;/td&gt;
        &lt;td&gt;Scale parameter for the Gamma distribution from which sample sizes are drawn.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \mu_v $&lt;/td&gt;
        &lt;td&gt;In simulations in which group observation variances $ \epsilon^{2}_i $ are allowed to vary, this is the mean parameter of the log-normal distribution from which the $ \epsilon^{2}_i $'s are drawn.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;$ \tau^{2}_v $&lt;/td&gt;
        &lt;td&gt;In simulations in which group observation variances $ \epsilon^{2}_i $ are allowed to vary, this is the variance parameter of the log-normal distribution from which the $ \epsilon^{2}_i $'s are drawn.&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;quick-analytic-solutions&quot;&gt;Quick Analytic Solutions&lt;/h3&gt;

&lt;p&gt;Our goal is to find a better way of estimating $ \theta_i $, the true mean of a group. A &lt;a href=&quot;http://projecteuclid.org/download/pdf_1/euclid.cbms/1462106062&quot;&gt;common&lt;/a&gt; &lt;a href=&quot;https://mathmodelsblog.wordpress.com/2010/02/02/introduction-to-buhlmann-credibility/&quot;&gt;theme&lt;/a&gt; &lt;a href=&quot;https://www.soa.org/files/pdf/c-24-05.pdf&quot;&gt;in&lt;/a&gt; &lt;a href=&quot;http://www.stat.cmu.edu/~acthomas/724/Efron-Morris.pdf&quot;&gt;many&lt;/a&gt; &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/09332480.2007.10722861&quot;&gt;of&lt;/a&gt; &lt;a href=&quot;http://intlpress.com/site/pub/files/_fulltext/journals/sii/2010/0003/0004/SII-2010-0003-0004-a011.pdf&quot;&gt;the&lt;/a&gt; &lt;a href=&quot;http://conservancy.umn.edu/bitstream/handle/11299/5852/Staffpaper10.pdf;jsessionid=433FD5983CCE7EE49AE0319D9B9FFA02?sequence=1&quot;&gt;papers&lt;/a&gt; I read is that good estimates of $ \theta_i $ are usually weighted averages of the group’s sample mean $ x_i $ and the global mean of all group means $ \overline{X} $. Let’s call this weighting factor $ \hat{B_i} $.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta_i} = \left(1-\hat{B_i}\right) x_i + \hat{B_i} \overline{X}&lt;/script&gt;

&lt;p&gt;This seems very sensible. We want something that is in between the sample mean (which is probably too extreme) and the mean of means. But how do we know what value to use for $ \hat{B_i} $? Different methods exist, and each leads to different results.&lt;/p&gt;

&lt;p&gt;Let’s start by defining $ \sigma^{2}_i $, the true variance of a group’s mean. This is equivalent to $ \epsilon^{2}_i / n_i $, where $ \epsilon^{2}_i $ is the true variance of the observations within that group, and $ n_i $ is the sample size of the group. According to the original James-Stein approach, if we assume that all the group means have the same known variance $ \hat{\sigma^2} $, which would usually only happen if the groups all had the same sample size, then we can define a common $ \hat{B} $ for all groups as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{B} = \frac{\left(k-3\right)\hat{\sigma^2}}{\sum{(x_i - \overline{X})^2}}&lt;/script&gt;

&lt;p&gt;This formula seems really weird and arbitrary, but it begins to make more sense if we rearrange it a bit and sweep that pesky $ \left(k-3\right) $ under the rug and replace it with a $ (k-1) $. Sorry hardliners!&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{eqnarray} 
\hat{B} &amp;\approx&amp; \frac{\left(k-1\right)\hat{\sigma^2}}{\sum{(x_i - \overline{X})^2}}\\
  \\
  &amp;=&amp; \frac{\hat{\sigma^2}}{\sum{(x_i - \overline{X})^2}/\left(k-1\right)}  \\
  \\
  &amp;=&amp; \frac{\hat{\sigma^2}}{\hat{\tau^{2}} + \hat{\sigma^2}}

\end{eqnarray} %]]&gt;&lt;/script&gt;

&lt;p&gt;Before getting to why this makes sense, I should explain the last step above. The denominator $ \sum{(x_i - \overline{X})^2}/\left(k-1\right) $ is the observed variance of the observed sample means. This variance comes from two sources: $ \tau^2 $ is the true variance in the true means and $ \sigma^2 $ is the true variance caused by the fact that each $ x_i $ is computed from a sample. Since variances add, the total variance of the observed means is $ \tau^{2} + \sigma^{2} $.&lt;/p&gt;

&lt;p&gt;Anyway, back to the result. This result is actually pretty neat. When we estimate a $ \theta_i $, the weight that we place on the global mean $ \overline{X} $ is the fraction of total variance in the means that is caused by within-group sampling variance. In other words, when the sample mean comes with high uncertainty, we should weight the global mean more. When the sample mean comes with low uncertainty, we should weight the global mean less. At least directionally, this formula makes sense. Later in this blog post, we’ll see how it falls naturally out of Bayes Law.&lt;/p&gt;

&lt;p&gt;The James-Stein Estimator is so widely applicable that many other fields have discovered it independently. In the image processing literature, it is a special case of the &lt;a href=&quot;http://www.dfmf.uned.es/~daniel/www-imagen-dhp/biblio/adaptive-wiener-noisy.pdf&quot;&gt;Wiener Filter&lt;/a&gt;, assuming that both the signal and the additive noise are Gaussian. In the insurance world, actuaries call it the &lt;a href=&quot;https://en.wikipedia.org/wiki/B%C3%BChlmann_model&quot;&gt;Bühlmann model&lt;/a&gt;. And in &lt;a href=&quot;https://en.wikipedia.org/wiki/Charles_Roy_Henderson&quot;&gt;animal breeding&lt;/a&gt;, early researchers called it the Best Unbiased Linear Prediction or &lt;a href=&quot;http://www.public.iastate.edu/~dnett/S511/27BLUP.pdf&quot;&gt;BLUP&lt;/a&gt; (technically the Empirical BLUP). The BLUP approach is so useful, in fact, that it has received the highly coveted endorsement of the &lt;a href=&quot;http://www.nsif.com/guidel/guidelines.htm&quot;&gt;National Swine Improvement Federation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While the original James-Stein formula is useful, the big limitation is that it only works when we believe that all groups have the same $ \sigma^2 $. In cases where we have different sample sizes, each group will have it’s own $ \sigma^{2}_i $. (Recall that $ \sigma^{2}_i = \epsilon^{2}_i / n_i $.) We’re going to want to shrink some groups more than others, and the original James-Stein estimator does not allow this. In the following sections, we’ll look at a couple of extensions to the James-Stein estimator. These extensions have analogues in the Bühlmann model and BLUP literature.&lt;/p&gt;

&lt;h4 id=&quot;the-multi-sample-size-james-stein-estimator&quot;&gt;The Multi Sample Size James-Stein Estimator&lt;/h4&gt;

&lt;p&gt;The most natural extension of James-Stein is to define each group’s $ \hat{\sigma^{2}_i} $ as the squared standard error of the group’s mean. This allows us to estimate a weighting factor $ \hat{B_i} $ tailored to each group. Let’s call this the Multi Sample Size James-Stein Estimator, or MSS James-Stein Estimator.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{B_i} = \frac{\hat{\sigma^{2}_i}}{\hat{\tau^{2}} + \hat{\sigma^{2}_i}}&lt;/script&gt;

&lt;p&gt;The denominator can just be estimated as the variance across group sample means.&lt;/p&gt;

&lt;p&gt;As reasonable as this approach sounds, it somehow didn’t feel totally kosher to me. But when I looked into the literature, it seems like most researchers basically said “&lt;a href=&quot;http://projecteuclid.org/download/pdf_1/euclid.cbms/1462106062&quot;&gt;yup&lt;/a&gt;, &lt;a href=&quot;https://mathmodelsblog.wordpress.com/2010/02/02/introduction-to-buhlmann-credibility/&quot;&gt;that sounds&lt;/a&gt; &lt;a href=&quot;https://www.soa.org/files/pdf/c-24-05.pdf&quot;&gt;pretty reasonable&lt;/a&gt;”.&lt;/p&gt;

&lt;p&gt;To test this approach, I ran some &lt;a href=&quot;https://github.com/csaid/empirical_bayes/blob/master/simulations.ipynb&quot;&gt;simulations&lt;/a&gt; on 1000 artificial datasets. Each dataset involved 25 groups with sample sizes drawn from a Gamma distribution $ \Gamma(k_{\Gamma}=1.5,\theta_{\Gamma}=10) $. True group means ($ \theta_i $’s) were sampled from a Normal distribution $ \mathcal{N}(\mu, \tau^2) $. Observations within each group were sampled from $ \mathcal{N}(\theta_i, \epsilon^2) $, where $ \epsilon $ was shared between groups.&lt;/p&gt;

&lt;p&gt;For each dataset I computed the Mean Squared Error (MSE) between the vector of true group means and the vector of estimated group means. I then averaged the MSEs across datasets. This process was repeated for a variety of different values of $ \epsilon $ and for two different estimators: The MSS James-Stein Estimator and the Maximum Likelihood Estimator (MLE). To compute the MLE, I just used $ x_i $ as my $ \hat{\theta_i} $ estimate.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/2017_shrinkage/fig_shared_2.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Mean Squared Error between true group means and estimated group means. In this simulation, the $ \epsilon $ parameter for within-group variance of observations is shared by all groups.&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
As expected, the MSS James-Stein Estimator outperformed the MLE, with lower MSEs particularly for high values of $ \epsilon $. This make sense. When the raw sample means are noisy, the MLE should be especially untrustworthy and it makes sense to pull extreme estimates back towards the global mean.&lt;/p&gt;

&lt;h4 id=&quot;the-multi-sample-size-pooled-james-stein-estimator&quot;&gt;The Multi Sample Size Pooled James-Stein Estimator&lt;/h4&gt;

&lt;p&gt;One thing that’s a little weird about the MSS James-Stein Estimator is that even though we know all the groups should have the same within-group variance $ \epsilon^2 $, we still estimate each group’s standard error separately. Given what we know, it might make more sense to &lt;a href=&quot;https://en.wikipedia.org/wiki/Pooled_variance&quot;&gt;pool&lt;/a&gt; the data from all groups to estimate a common $ \epsilon^2 $. Then we can estimate each group’s $ \sigma^{2}_i $ as $ \epsilon^2 / n_i $. Let’s call this approach the MSS Pooled James-Stein Estimator.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/2017_shrinkage/fig_shared_3.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Mean Squared Error between true group means and estimated group means. In this simulation, the $ \epsilon $ parameter for within-group variance of observations is shared by all groups.&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
This works a bit better. By obtaining more accurate estimates of each group’s $ \sigma^{2}_i $, we are able to find a more appropriate shrinking factor $ B_i $ for each group.&lt;/p&gt;

&lt;p&gt;Of course, this only works better because we created the simulation data in such a way that all groups have the same $ \epsilon^2 $. But if we run a different set of simulations, in which each group’s $ \epsilon_i $ is drawn from a log-normal distribution $ ln\mathcal{N}\left(\mu_v, \tau^{2}_v\right) $, we obtain the reverse results. The MSS James-Stein Estimator, which estimates a separate $ \hat{\epsilon^{2}_i} $ for each group, does a better job than the  MSS Pooled James-Stein Estimator. This makes sense.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/2017_shrinkage/fig_unshared_3.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; Mean Squared Error between true group means and estimated group means. In this simulation, each group has its own variance parameter $ \epsilon^2 $ for the observations within the group. These parameters are sampled from a log-normal distribution $ ln\mathcal{N}\left(\mu_v, \tau^{2}_v\right) $. For simplicity, the two parameters of this distribution are always set to be identical, and are shown on the horizontal axis. &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Which method you choose should depend on whether you think your groups have similar or different variances of their observations. Here’s an interim summary of the methods covered so far.&lt;/p&gt;

&lt;div class=&quot;box&quot;&gt;
&lt;h4 style=&quot;margin-top:0rem&quot;&gt;Summary of analytic solutions&lt;/h4&gt;
All of these estimators define $ \hat{\theta_i} $ as a weighted average of the group sample mean $ x_i $ and the mean of group sample means $ \overline{X} $.
$$ \hat{\theta_i} = \left(1-\hat{B_i}\right) x_i + \hat{B_i} \overline{X} $$

Make sure to clip $ \hat{B_i} $ to the range [0, 1].&lt;br /&gt;&lt;br /&gt;

&lt;ol&gt;
    &lt;li style=&quot;margin-bottom: 10px&quot;&gt;
        &lt;div&gt;&lt;strong&gt;Maximum Likelihood Estimation (MLE)&lt;/strong&gt;&lt;/div&gt;
        &lt;div style=&quot;margin-top: 0px; line-height: 5.7em&quot;&gt; $ \hat{B_i} = 0 $&lt;/div&gt;
    &lt;/li&gt;

    &lt;li style=&quot;margin-bottom: 20px&quot;&gt;
        &lt;div&gt;&lt;strong&gt;MSS James-Stein Estimator&lt;/strong&gt;&lt;/div&gt;
        &lt;div style=&quot;margin-top: 0px; line-height: 5.7em&quot;&gt; $ \hat{B_i} = \frac{s^{2}_i/n_i}{\sum\frac{(x_i - \overline{X})^2}{k-1}} $&lt;/div&gt;
        where $ s_i $ is the standard deviation of observations with a group.&lt;br /&gt;
    &lt;/li&gt;

    &lt;li&gt;
        &lt;div&gt;&lt;strong&gt;MSS Pooled James-Stein Estimator&lt;/strong&gt;&lt;/div&gt;
        &lt;div style=&quot;margin-top: 0px; line-height: 5.7em&quot;&gt; $ \hat{B_i} = \frac{s^{2}_p/n_i}{\sum\frac{(x_i - \overline{X})^2}{k-1}} $&lt;/div&gt;
        where $ s^{2}_p $ is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Pooled_variance&quot;&gt;pooled&lt;/a&gt; estimate of variance.
    &lt;/li&gt;
&lt;/ol&gt;
Implementations in Python and R are available &lt;a href=&quot;https://github.com/csaid/empirical_bayes&quot;&gt;here&lt;/a&gt;.
&lt;/div&gt;

&lt;h4 id=&quot;a-bayesian-interpretation-of-the-analytic-solutions&quot;&gt;A Bayesian interpretation of the analytic solutions&lt;/h4&gt;

&lt;p&gt;So far, the analytic approaches make sense directionally. As described above, our estimate of $ \theta_i $ should be a weighted average of $ x_i $ and $ \overline{X} $, where the weight depends on the ratio of sample mean variance to total variance of the means.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_i = \left(1 - \frac{\hat{\sigma^{2}_i}}{\hat{\tau^{2}} + \hat{\sigma^{2}_i}}\right) x_i + \left(\frac{\hat{\sigma^{2}_i}}{\hat{\tau^{2}} + \hat{\sigma^{2}_i}}\right) \overline{X}&lt;/script&gt;

&lt;p&gt;But is this really the best weighting? Why use a ratio of variances instead of, say, a ratio of standard deviations? Why not use something else entirely?&lt;/p&gt;

&lt;p&gt;It turns out this formula falls out naturally from Bayes Law. Imagine for a moment that we already know the prior distribution $ \mathcal{N}\left(\mu, \tau^2\right) $ over the $ \theta_i $’s. And imagine we know the likelihood function for a group mean is $ \mathcal{N}\left(x_i, \epsilon^{2}_i/n_i\right) $. According to the Wikipedia page on &lt;a href=&quot;https://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions&quot;&gt;conjugate priors&lt;/a&gt;, the posterior distribution for the group mean is itself a Gaussian distribution with mean:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta_i} = \frac{\frac{x_i n_i}{\epsilon^{2}_i} + \frac{\mu}{\tau^{2}}}{\frac{1}{\tau^2} + \frac{n_i}{\epsilon^{2}_i}}&lt;/script&gt;

&lt;p&gt;(Note that the Wikipedia &lt;a href=&quot;https://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions&quot;&gt;page&lt;/a&gt; uses the symbol ‘$ x_i $’ to refer to observations, whereas this blog post will always use the term to refer to the sample mean, including in the equation above. Also note that Wikipedia refers to the variance of observations within a group as ‘$ \sigma^2 $’ whereas this blog post uses $ \epsilon^{2}_i $.)&lt;/p&gt;

&lt;p&gt;If we multiply all terms in the numerator and denominator by $ \frac{\tau^2 \epsilon^{2}_i}{n_i} $, we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta_i} = \frac{\tau^2 x_i + \sigma^{2}_i \mu}{\sigma^{2}_i + \tau^2}&lt;/script&gt;

&lt;p&gt;Or equivalently,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta_i} = \left(1-B_{i}\right) x_i + B_{i} \mu&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B_{i} = \frac{\sigma^{2}_i}{\sigma^{2}_i + \tau^2}&lt;/script&gt;

&lt;p&gt;This looks familiar! It is basically the MSS James-Stein estimator. The only difference is that in the pure Bayesian approach you must somehow know $ \mu $, $ \tau^2 $, and $ \sigma^{2}_i $ in advance. In the MSS James-Stein approach, you estimate those parameters from the data itself. This is the key insight in Empirical Bayes: Use priors to keep your estimates under control, but obtain the priors &lt;em&gt;empirically&lt;/em&gt; from the data itself.&lt;/p&gt;

&lt;h3 id=&quot;hierarchical-modeling-with-mcmc&quot;&gt;Hierarchical Modeling with MCMC&lt;/h3&gt;

&lt;p&gt;In previous sections we looked at some analytic solutions. While these solutions have the advantage of being quick to calculate, they have the disadvantage of being less accurate than they could be. For more accuracy, we can turn to Hierarchical Model estimation using Markov Chain Monte Carlo (MCMC) methods. MCMC is an iterative process for approximate Bayesian inference. While it is slower than analytical approximations, it tends to be more accurate and has the added benefit of giving you the full posterior distribution. I’m not an expert in how it works internally, but &lt;a href=&quot;http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/&quot;&gt;this post&lt;/a&gt; looks like a good place to start.&lt;/p&gt;

&lt;p&gt;To implement this, I first defined a &lt;a href=&quot;http://mc-stan.org/documentation/case-studies/radon.html&quot;&gt;Hierarchical Model&lt;/a&gt; of my data. The model is a description of how I think the data is generated: True means are sampled from a normal distribution, and observations are sampled from a normal distribution centered around the true mean of each group. Of course, I know exactly how my data was generated, because I was the one who generated it! The key thing to understand though is that the Hierarchical Model does not contain any information about the value of the parameters. It’s the MCMC’s job to figure that out. In particular, I used &lt;a href=&quot;https://pystan.readthedocs.io/en/latest/&quot;&gt;PyStan’s&lt;/a&gt; MCMC implementation to fit the parameters of the model based on my data, although I later &lt;a href=&quot;https://twitter.com/talyarkoni/status/859837371034062848&quot;&gt;learned&lt;/a&gt; that it would be even easier to use &lt;a href=&quot;https://github.com/bambinos/bambi&quot;&gt;bambi&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/2017_shrinkage/fig_shared_4.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; Mean Squared Error between true group means and estimated group means. In this simulation, the $ \epsilon $ parameter for within-group variance of observations is shared by all groups.&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;For simulated data with shared $ \epsilon^2 $, MCMC did well, outperforming both the MSS James-Stein estimator and the MSS Pooled James-Stein estimator.&lt;/p&gt;

&lt;p&gt;If you don’t care about speed and are willing to write the Stan code, then this is probably your best option. It’s also good to learn about MCMC methods, since they can be applied to more complicated models with multiple variables. But if you just want a quick estimate of group means, then one of the analytic solutions above makes more sense.&lt;/p&gt;

&lt;h3 id=&quot;other-solutions&quot;&gt;Other solutions&lt;/h3&gt;

&lt;p&gt;There are several other solutions that I did not include in my simulations.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/seanjtaylor/status/851227324796157953&quot;&gt;Regularization&lt;/a&gt;. Pick the set of $ \hat{\theta_i} $’s that minimize &lt;script type=&quot;math/tex&quot;&gt;\sum{(\hat{\theta_i} - x_i)^2} + \lambda \sum{(\hat{\theta_i} - \overline{X})^2}&lt;/script&gt;. Use cross-validation to choose the best $ \lambda $. This will probably work pretty well, although it takes a bit more work and time than the analytic solutions described above.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf&quot;&gt;Mixed Models&lt;/a&gt;. Over in Mixed Models World, there’s a whole ’nother literature on how to shrink estimates depending on sample size. They are especially suited for the movie ratings situation because in addition to shrinking the means, they also can correct for rater bias. I don’t really understand all the math behind Mixed Models, but I was able to use &lt;a href=&quot;https://cran.r-project.org/web/packages/lme4/lme4.pdf&quot;&gt;lme4&lt;/a&gt; to estimate group means in simulated data under the assumption that the group means are a random effect. This gave me slightly different results compared to the James-Stein / Empirical Bayes approach. I would love if some expert who understood this could write an accessible and authoritative blog post on the differences between Mixed Models and Empirical Bayes. The closest I could find was &lt;a href=&quot;http://projecteuclid.org/download/pdf_1/euclid.ss/1177011928&quot;&gt;this comment&lt;/a&gt; by David Harville.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.stat.cmu.edu/~acthomas/724/Efron-Morris.pdf&quot;&gt;Efron and Morris&lt;/a&gt;’ generalization of James-Stein to unequal sample sizes (Section 3 of their paper). I thought this paper was difficult to read. A more accessible presentation can be found at the end of &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/09332480.2007.10722861&quot;&gt;this column&lt;/a&gt;. The Efron and Morris approach is a numerical solution that seemed to work reasonably well when I played around with it, but I didn’t take it very far. If you want to implement it, be sure to prevent any variance estimates from falling below zero. If one of them does, just set it to zero and then compute your estimates of the means. That being said, I feel like if you’re going to go with a numerical solution, you may as well just go with MCMC.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://intlpress.com/site/pub/files/_fulltext/journals/sii/2010/0003/0004/SII-2010-0003-0004-a011.pdf&quot;&gt;Double Shrinkage&lt;/a&gt;. When we think that different groups not only have different sample sizes, but also different $ \epsilon_i $’s, we are faced with an interesting conundrum. As shown above, the MSS James-Stein Estimator outperforms the MSS Pooled James-Stein Estimator, because it computes $ \hat{\epsilon_i} $’s specific to each group. However, these estimates of group variances are probably noisy! Just like we don’t trust the raw estimates of group sample means, why should we trust the raw estimates of group sample variances? One way to address this is to use Zhao’s Double Shinkage Estimator, which not only shrinks the means, but also shrinks the variances.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.jstor.org/stable/2284137?seq=1#fndtn-page_scan_tab_contents&quot;&gt;Kleinman’s weighted moment estimator&lt;/a&gt;. Apparently this was motivated by groups of proportions (i.e. the Rotten Tomatoes case), but the estimator can be applied generally.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Which method you choose depends on your situation. If you want a simple and computationally fast estimate, and if you don’t want to assume that the group variances $ \epsilon^{2}_i $ are identical, I would recommend either the MSS James-Stein Estimator or the Double Shrinkage Estimator if you can get it to work. If you want a fast estimate and can assume all groups share the same $ \epsilon^{2} $, I’d recommend the MSS Pooled James-Stein Estimator. If you don’t care about speed or code complexity, I’d recommend MCMC, Mixed Models, or regularization with a cross-validated penalty term.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;
&lt;p&gt;Special thanks to the many people who responded to my &lt;a href=&quot;https://twitter.com/Chris_Said/status/851211601445240832&quot;&gt;original question on Twitter&lt;/a&gt;, including: &lt;a href=&quot;https://twitter.com/seanjtaylor&quot;&gt;Sean Taylor&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/johnmyleswhite&quot;&gt;John Myles White&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/drob&quot;&gt;David Robinson&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/stat110&quot;&gt;Joe Blitzstein&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/NeuroStats&quot;&gt;Manjari Narayan&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/oldjacket&quot;&gt;Otis Anderson&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/alex_peys&quot;&gt;Alex Peysakhovich&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/bechhof&quot;&gt;Nathaniel Bechhofer&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/jamneuf&quot;&gt;James Neufeld&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/awmercer&quot;&gt;Andrew Mercer&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/ptrckprry&quot;&gt;Patrick Perry&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/RichmanRonald&quot;&gt;Ronald Richman&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/Hacktuarial&quot;&gt;Timothy Sweetster&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/talyarkoni&quot;&gt;Tal Yarkoni&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/AlxCoventry&quot;&gt;Alex Coventry&lt;/a&gt;. Special thanks also to Marika Inhoff and Leo Pekelis for many discussions.&lt;/p&gt;

&lt;p&gt;All code used in this blog post is available on &lt;a href=&quot;https://github.com/csaid/empirical_bayes&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Optimizing things in the USSR</title>
   <link href="http://localhost:4000/2016/05/11/optimizing-things-in-the-ussr/"/>
   <updated>2016-05-11T00:00:00-07:00</updated>
   <id>http://localhost:4000/2016/05/11/optimizing-things-in-the-ussr</id>
   <content type="html">&lt;meta charset=&quot;utf-8&quot; /&gt;

&lt;style&gt;

.node rect {
  cursor: move;
  fill-opacity: .9;
  shape-rendering: crispEdges;
}

.node text {
  pointer-events: none;
  text-shadow: 0 1px 0 #fff;
}

.link {
  fill: none;
  stroke: #000;
  stroke-opacity: .2;
}

.link:hover {
  stroke-opacity: .5;
}


text {
  font: 10px sans-serif;
}


&lt;/style&gt;

&lt;p&gt;As a data scientist, a big part of my job involves picking metrics to optimize and thinking about how to do things as efficiently as possible. With these types of questions on my mind, I recently discovered a totally fascinating book about about economic problems in the USSR and the team of data-driven economists and computer scientists who wanted to solve them. The book is called &lt;a href=&quot;http://www.amazon.com/Red-Plenty-Francis-Spufford/dp/1555976042&quot;&gt;&lt;em&gt;Red Plenty&lt;/em&gt;&lt;/a&gt;. It’s actually written as a novel, weirdly, but it nevertheless presents an accurate economic history of the USSR. It draws heavily on an earlier book from 1973 called &lt;a href=&quot;http://www.amazon.com/Planning-Problems-USSR-Contribution-Mathematical/dp/0521202493&quot;&gt;&lt;em&gt;Planning Problems in the USSR&lt;/em&gt;&lt;/a&gt;, which I also picked up. As I read these books, I couldn’t help but notice some parallels with planning in any modern organization. In what will be familiar to any data scientist today, the second book even includes a quote from a researcher who complained that 90% of his time was spent cleaning the data, and only 10% of his time was spent doing actual modeling!&lt;/p&gt;

&lt;p&gt;Beyond all the interesting parallels to modern data science and operations research, these books helped me understand a lot of interesting things I previously knew very little about, such as linear programming, price equilibria, and Soviet history. This blog post is about I learned.&lt;/p&gt;

&lt;h4 style=&quot;text-align: center;&quot;&gt;Balance sheets and manual calculation: Kind of a trainwreck&lt;/h4&gt;

&lt;p&gt;The main task in the centrally planned Soviet economy was to allocate resources so that a desired assortment of goods and services was produced. Every year, certain target outputs for each good were established. Armed with estimates of the available input resources, central administrators used balance sheets to set plans for every factory, specifying exactly how much input commodities each factory would receive, and how much output it should produce. Up through the 1960s, this was always done by manual calculation. Since there were hundreds of thousands of commodities, and since the supply chains had many dependency steps, it was impossible to compute the full balance sheets for the economy. The administrators therefore decided to make some simplifying assumptions. As a result of these these simplifying assumptions, resource allocation became a bit of a trainwreck. Below are a few of the simplifications and their consequences.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dimensionality reduction by removing variables.&lt;/strong&gt; Because there were too many commodities to track, administrators often limited their analysis to the 10,000 most important commodities in the economy. But when the production of those commodities were planned, there was often a hidden shortage of commodities whose output was not planned centrally but which were used as inputs to one of the 10,000 planned products. Factories that depended on those commodities often sat idle for months as they waited for the shortages to end.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dimensionality reduction by aggregation.&lt;/strong&gt; Apparently, steel tubes can come in thousands of different types. They can come in different lengths, different shapes, and different compositions. To reduce the dimensionality of the problem, administrators would often track the total tonnage of a few broad classes of steel tubes in the models, rather than using a more detailed classification scheme. While their models successfully balanced the tonnage of tubes for the broad categories (the output in tons of tube-producing factories matched the input requirements in tons of tube-consuming factories), there were constant surpluses of some specific types of tubes, and shortages of other specific types of tubes. In particular, since tonnage was used as a metric, tube-producing factories were overly incentivized to make easy-to-produce thick tubes. As a result, thin tubes were always in short supply.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Propagating adjustments only a few degrees back.&lt;/strong&gt; Let’s say that during balance calculations, the administrators realized they needed to bump up the target output of one commodity. If they did that, it was also necessary to bump up the output targets of commodities that were input into the target commodity. But if they did &lt;em&gt;that&lt;/em&gt;, they also needed to bump up the output targets of commodities that fed into those commodities, and so on! This involved a crazy amount of extra hand calculations every time they needed make an adjustment. To simplify things, the administrators typically made adjustments to the first-order suppliers, without making the necessary adjustments to the suppliers of the suppliers. This of course led to critical shortages of input commodities, which again led to idle factories.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- The tooltip has absolute positioning, which means it is positioned
&quot;relative&quot; to any parent it has who has either absolute or relative positioning.
The #econ_scatter parent would by default be static, so I have to change it to
relative --&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;div id=&quot;chart&quot; class=&quot;inner&quot; style=&quot;position:relative&quot;&gt;&lt;/div&gt;
  &lt;!-- &lt;div id=&quot;econ_scatter&quot; class=&quot;inner&quot; style=&quot;position:relative&quot;&gt;&lt;/div&gt; --&gt;
  &lt;div class=&quot;caption&quot;&gt;
&lt;strong&gt;Figure 1.&lt;/strong&gt; Some example inputs and outputs in the Soviet economy in 1951, described in units of weight. This summary shows an extreme dimensionality reduction, more extreme than was ever used in planning. In this diagram, most commodities are excluded and each displayed commodity collapses across multiple different product types. Multiple steps in the supply chain are collapsed into a single step. (Source: &lt;a href=&quot;http://www.foia.cia.gov/sites/default/files/document_conversions/89801/DOC_0000380738.pdf&quot;&gt;CIA&lt;/a&gt;)
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;Even if the administrators could get the accounting correct, which they couldn’t, their attempts to allocate resources would still be far from optimal. In the steel industry, for example, some factories were better at producing some types of tubes whereas others were better at producing other types of tubes. Since there were thousands of different factories and tube types, it was non-trivial to decide how to best distribute resources and output requirements, and it was not immediately obvious which factories should be expanded and which should be closed down.&lt;/p&gt;

&lt;h4 style=&quot;text-align: center;&quot;&gt;Supply chain optimizations&lt;/h4&gt;

&lt;p&gt;In the late 1960’s, a group of economists and computer scientists known as the “optimal planners” began to push for a better way of doing things. The group argued that a technique called &lt;a href=&quot;https://www.math.ucla.edu/~tom/LP.pdf&quot;&gt;linear programming&lt;/a&gt;, invented by &lt;a href=&quot;https://en.wikipedia.org/wiki/Leonid_Kantorovich&quot;&gt;Leonid Kantorovich&lt;/a&gt;, could optimally solve the problems with the supply chain. At a minimum, since the process could be computerized, it would be possible to perform more detailed calculations than could be done by hand, with less dimensionality reduction. But more importantly, linear programming allowed you to optimize arbitrary objective functions given certain constraints. In the case of the supply chain, it showed you how to efficiently allocate resources, identifying efficient factories that should get more input commodities, and inefficient factories that should be shut down.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/kantorovich.jpg&quot; height=&quot;300&quot; class=&quot;inner&quot; style=&quot;position:relative&quot; /&gt;
  &lt;div class=&quot;caption&quot;&gt;
    &lt;strong&gt;Figure 2.&lt;/strong&gt; Leonid Kantorovich, inventor of linear programming and winner of the 1975 Nobel Prize in Economics.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;The optimal planners had some success here. For example, in the steel industry, about 60,000 consumers requested 10,000 different types of products from 500 producers. The producers were not equally efficient in their production. Some producers were efficient for some types of steel products, but less efficient for other types of steel products. Given the total amount of each product requested, and given the constraints of how much each factory can produce, the goal was decide how much each factory should produce of each type of product. If we simplify the problem by just asking how much each factory should produce without considering how the products will be distributed to the consuming factories, this becomes a straightforward application of the &lt;a href=&quot;https://www.math.ucla.edu/~tom/LP.pdf&quot;&gt;Optimal Assignment Problem&lt;/a&gt;, a well-studied example in linear programming. If we additionally want to optimize distribution, taking into account the distance-dependent costs of shipments from one factory to another, the problem becomes more complicated but is still doable. The problem becomes similar to the &lt;a href=&quot;https://www.math.ucla.edu/~tom/LP.pdf&quot;&gt;Transportation Problem&lt;/a&gt;, another well-studied example in linear programming, but in this case generalized to multiple commodities instead of just one.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/steel_tubes.jpg&quot; height=&quot;200&quot; class=&quot;inner&quot; style=&quot;position:relative&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;By introducing linear programming, the optimal planners were modestly successful at improving the efficiency of some industries, but their effect was limited. First, political considerations prevented many of the recommendations surfaced by the model from being implemented. Cement factories that were known to be too inefficient or too far away from consumers were allowed to remain open even though the optimal solution recommended that they be closed. Second, since the planners were only allowed to work in certain narrow parts of the economy, they never had an opportunity to propagate their recommendations back in the supply chain, although one could imagine extending the models to do so. Third, and perhaps most importantly, the value of each commodity was set by old-school administrators in an unprincipled way, and so the optimal planners were forced to optimize objective functions that didn’t even make sense.&lt;/p&gt;

&lt;h4 style=&quot;text-align: center;&quot;&gt;Ideas about optimizing the entire economy&lt;/h4&gt;

&lt;p&gt;While the optimal planners were able to improve the efficiency of a few industries, they had more ambitious plans. They believed they could use linear programming to optimize the entire economy and outperform capitalist societies. Doing so involved more than just scaling out the supply chain optimizations adopted by certain industries. It involved shadow prices and interest rates, and a few other things I’ll admit I don’t totally understand. But while I don’t really understand the implementation, I feel like the broader goal of the planners is easier to understand and explain:&lt;/p&gt;

&lt;p&gt;Basically, in a completely free market, at least under &lt;a href=&quot;https://en.wikipedia.org/wiki/Arrow%E2%80%93Debreu_model&quot;&gt;certain assumptions&lt;/a&gt;, prices are supposed to converge to what’s called a General Equilibrium. The equilibrium prices have a some nice properties. They balance aggregate supply and demand, so that no commodities are in shortage or surplus. They are also &lt;a href=&quot;https://en.wikipedia.org/wiki/Pareto_efficiency&quot;&gt;Pareto efficient&lt;/a&gt;, which means that nobody in the economy can be made better off without making someone else worse off.&lt;/p&gt;

&lt;p&gt;The optimal planners thought that they could do better. In particular, they pointed to two problems with capitalism: First, prices in a capitalist society were determined by individual agents using trial and error to guess the best price. Surely these agents, who had imperfect information, were not picking the exactly optimal prices. In contrast, a central planner using optimal computerized methods could pick prices that hit the equilibrium more exactly. Second, and more importantly, capitalism targeted an objective function that — while Pareto efficient — was not socially optimal. Because of huge differences in wealth, some people were able to obtain far more goods and services than other people. The optimal planners proposed using linear programming to optimize an objective function that would be more socially optimal. For example, it could aim to distribute goods more equitably. It could prioritize certain socially valuable goods (e.g. books) over socially destructive goods (e.g. alcohol). It could prioritize sectors that provide benefits over longer time horizons (e.g. heavy industry). And it could include constraints to ensure full employment.&lt;/p&gt;

&lt;h4 style=&quot;text-align: center;&quot;&gt;What happened&lt;/h4&gt;
&lt;p&gt;None of this ever really happened. The ambitious ideas of the optimal planners were never adopted, and by the 1970s it was clear that living standards in the USSR were falling further behind those of the West. Perhaps things would have been better if the optimal planners got their way, but it seems like the consensus is that their plans would have failed even if they were implemented. Below are some of the main problems that would have been encountered.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Computational complexity.&lt;/strong&gt; As described in a &lt;a href=&quot;http://crookedtimber.org/2012/05/30/in-soviet-union-optimization-problem-solves-you/&quot;&gt;wonderful blog post by Cosma Shalizi&lt;/a&gt;, the number of calculations needed to solve a linear programming problem is: &lt;script type=&quot;math/tex&quot;&gt;(m+n)^{3/2} n^2 log(1/h)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is the number of products, &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; is the number of constraints, and &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt; is how much error you are willing to tolerate. Since the number of products, &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;, was in the millions, and since the complexity was proportional to &lt;script type=&quot;math/tex&quot;&gt;n^{3.5}&lt;/script&gt;, it would have been practically impossible for the Soviets to compute a solution to their planning problem with sufficient detail (although see below). Any attempt to reduce the dimensionality would lead to the same perverse incentives and shortages that bedeviled earlier systems driven by hand calculations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data quality.&lt;/strong&gt; The optimal planners thought that optimal computer methods could find prices that more exactly approximated equilibrium than could be done in a market economy, where fallible human actors guessed at prices by trial and error. The reality, however, would have been the exact opposite. Individual actors in a market economy understand their local needs and constraints pretty well, whereas central planners have basically no idea what’s going on. For example, central planners don’t have good information on when a factory fails to receive a shipment and they don’t have an accurate sense for how much more efficient some devices are than others. Even worse, in order to obtain more resources, factory managers in the USSR routinely &lt;em&gt;lied&lt;/em&gt; to the central planners about their production capabilities. The situation became so bad that, &lt;a href=&quot;https://books.google.com/books?id=Jlmm9GZqxkoC&amp;amp;printsec=frontcover#v=onepage&amp;amp;q&amp;amp;f=false&quot;&gt;according to&lt;/a&gt; &lt;a href=&quot;http://crookedtimber.org/2012/05/30/in-soviet-union-optimization-problem-solves-you/#comment-416126&quot;&gt;one of the deep state secrets&lt;/a&gt; of the USSR, central planners preferred to use the CIA’s analyses of certain Russian commodities rather than reports from local Party bosses!   This is especially crazy if you consider that the CIA described its own data as being of &lt;a href=&quot;http://www.foia.cia.gov/sites/default/files/document_conversions/89801/DOC_0000380738.pdf&quot;&gt;“debilitatingly”&lt;/a&gt; poor quality.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Nonlinearities.&lt;/strong&gt; The optimal planners assumed linearity, such that the cost for a factory producing its 1000th widget was assumed to be the same as the cost for producing its first widget. In the real world, this is obviously false, as there are increasing returns to scale. It’s possible to model increasing returns to scale, but it becomes harder to solve computationally.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Choosing an objective function.&lt;/strong&gt; Choosing what the society should value is really a political problem, and Cosma Shalizi does a very &lt;a href=&quot;http://crookedtimber.org/2012/05/30/in-soviet-union-optimization-problem-solves-you/&quot;&gt;nice job&lt;/a&gt; describing why it would be so hard to come to agreement.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Incentives for innovation.&lt;/strong&gt; The central planners couldn’t determine resource allocation for products that didn’t exist yet, and more importantly neither they nor the factories had much incentive to invent new products. That’s why the Soviet Union remained so focused on the steel/coal/cement economy while Western nations shifted their focus to plastics and microelectronics.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Political resistance.&lt;/strong&gt; As described in a previous example, the model-based recommendations to shut down certain factories were ignored for political reasons. It is likely that many recommendations for the broader economy would have been ignored as well. For example, if a computer recommended that the price of heating oil should be doubled in the winter, how many politicians would let that happen?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 style=&quot;text-align: center;&quot;&gt;Could this work in the future?&lt;/h4&gt;

&lt;p&gt;Had the optimal planners’ ideas been adopted at the time, they would have failed. But what about the future? In a hundred years, could we have the technical capability to pull off a totally planned economy? I did some poking around the internet and found, somewhat to my surprise, that the answer is actually… &lt;em&gt;maybe&lt;/em&gt;. It turns out that two of the most serious problems with central planning could have technological solutions that may seem far-fetched but are perhaps not impossible:&lt;/p&gt;

&lt;p&gt;Let’s start with &lt;strong&gt;computational complexity&lt;/strong&gt;. As described above and in &lt;a href=&quot;http://crookedtimber.org/2012/05/30/in-soviet-union-optimization-problem-solves-you/&quot;&gt;Cosma Shalizi’s post&lt;/a&gt;, the number of steps required to solve a linear programming problem with &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; products and &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; constraints is proportional to &lt;script type=&quot;math/tex&quot;&gt;(m+n)^{3/2} n^2&lt;/script&gt;. The USSR had about 12 million types of goods. If you cross them over about 1000 possible locations, that gives you 12 billion variables, which according to Cosma would correspond to an optimization problem that would take a thousand years to solve on a modern desktop computer. However, if Moore’s Law holds up, it would be possible in 100 years to solve this problem reasonably quickly. It’s also worth pointing out that the economy’s input-output matrix is sparse, since not every product depends on every other product as input. It &lt;em&gt;may&lt;/em&gt; be possible that someone might develop a faster algorithm that leverages this sparsity, although Cosma is somewhat skeptical that this could happen. &lt;em&gt;[In an earlier version of this post, I discussed a &lt;a href=&quot;http://ricardo.ecn.wfu.edu/~cottrell/socialism_book/new_socialism.pdf&quot;&gt;sparsity-based proposal&lt;/a&gt; that supposedly brought things down to &lt;script type=&quot;math/tex&quot;&gt;m \times n&lt;/script&gt; complexity. This was apparently a &lt;a href=&quot;http://bactra.org/weblog/919.html&quot;&gt;red herring&lt;/a&gt; that doesn’t actually solve the optimization problem.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As described earlier, the second serious issue with a centrally planned economy was &lt;strong&gt;data quality&lt;/strong&gt;: Central planners’ knowledge about the input requirements and output capabilities of individual factories was simply not as good as the people actually working in the factory. While this was certainly the case in the Soviet Union, one can’t help but wonder about technological improvements in supply chain management. Imagine if every product had a GPS device to track its location, with other sensors and cameras to determine product quality. Already Amazon is moving in that direction for pretty much all consumer goods, and one could imagine a world where demand could be measured with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Internet_of_Things&quot;&gt;Internet of Things&lt;/a&gt;. Whether a government would be able to harness this data as competently as Amazon is doubtful, and it’s obviously worth asking whether we would ever want a government to be using that type of data. But from a technical point of view it’s interesting to think about how the data quality issues that destroyed the USSR could be much less serious in the future.&lt;/p&gt;

&lt;p&gt;All that being said, it’s still unclear to me how an objective function could be chosen in way that would democratically satisfy people, how innovation could be incentivized, or how political freedoms could be preserved. Socialism has poor track record historically, with lots of failed promises that “this time will be different”. If you’d like to read more about how things worked in the USSR, you should definitely check out &lt;a href=&quot;http://www.amazon.com/Red-Plenty-Francis-Spufford/dp/1555976042&quot;&gt;Red Plenty&lt;/a&gt;. It was one of the weirdest and most interesting books I have read.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;img src=&quot;/assets/red_plenty.jpeg&quot; height=&quot;300&quot; class=&quot;inner&quot; style=&quot;position:relative&quot; /&gt;
&lt;/div&gt;

&lt;script src=&quot;/scripts/sankey.js&quot;&gt;&lt;/script&gt;

&lt;script&gt;

// Names to Index
var n2i = {
'petro': 0,
'steel': 1,
'copper': 2,
'aluminum': 3,
'ammonia': 4,
'food': 5,
'textiles': 6,
'logging': 7,
'automotive': 8
}

var commodities = {
  &quot;nodes&quot;:[
    {'name': 'Petroleum residuals'},
    {'name': 'Crude Steel'},
    {'name': 'Copper'},
    {'name': 'Aluminum'},
    {'name': 'Ammonia'},
    {'name': 'Agriculture and Food'},
    {'name': 'Textiles and Apparel'},
    {'name': 'Logging and Paper Products'},
    {'name': 'Automotive Equipment'}
  ],
  'links':[
    {'source':n2i['petro'],'target':n2i['food'],'value':510},
    {'source':n2i['steel'],'target':n2i['food'],'value':80},
    {'source':n2i['ammonia'],'target':n2i['food'],'value':250},
    {'source':n2i['steel'],'target':n2i['textiles'],'value':10},
    {'source':n2i['ammonia'],'target':n2i['textiles'],'value':10},
    {'source':n2i['petro'],'target':n2i['logging'],'value':30},
    {'source':n2i['steel'],'target':n2i['logging'],'value':40},
    {'source':n2i['ammonia'],'target':n2i['logging'],'value':5},
    {'source':n2i['petro'],'target':n2i['automotive'],'value':60},
    {'source':n2i['steel'],'target':n2i['automotive'],'value':1800},
    {'source':n2i['copper'],'target':n2i['automotive'],'value':14},
    {'source':n2i['aluminum'],'target':n2i['automotive'],'value':3}
  ]
}

if( /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) ) {
  var full_width = 400
  var full_height = 400;
} else {
  var full_width = 620
  var full_height = 500;
}

var margin = {top: 20, right: 40, bottom: 20, left: 40},
    width = full_width - margin.left - margin.right,
    height = full_height - margin.top - margin.bottom;

var formatNumber = d3.format(&quot;,.0f&quot;),
    format = function(d) { return formatNumber(d) + &quot; Th MT&quot;; },
    color = d3.scaleOrdinal(d3.schemeCategory10);

var svg = d3.select(&quot;#chart&quot;).append(&quot;svg&quot;)
    .attr(&quot;width&quot;, width + margin.left + margin.right)
    .attr(&quot;height&quot;, height + margin.top + margin.bottom)
  .append(&quot;g&quot;)
    .attr(&quot;transform&quot;, &quot;translate(&quot; + margin.left + &quot;,&quot; + margin.top + &quot;)&quot;);


var sankey = d3.sankey()
    .nodeWidth(15)
    .nodePadding(10)
    .size([width, height]);

var path = sankey.link();

  sankey
      .nodes(commodities.nodes)
      .links(commodities.links)
      .layout(32);

  var link = svg.append(&quot;g&quot;).selectAll(&quot;.link&quot;)
      .data(commodities.links)
    .enter().append(&quot;path&quot;)
      .attr(&quot;class&quot;, &quot;link&quot;)
      .attr(&quot;d&quot;, path)
      .style(&quot;stroke-width&quot;, function(d) { return Math.max(1, d.dy); })
      .sort(function(a, b) { return b.dy - a.dy; });

  link.append(&quot;title&quot;)
      .text(function(d) { return d.source.name + &quot; → &quot; + d.target.name + &quot;\n&quot; + format(d.value); });

  var node = svg.append(&quot;g&quot;).selectAll(&quot;.node&quot;)
      .data(commodities.nodes)
    .enter().append(&quot;g&quot;)
      .attr(&quot;class&quot;, &quot;node&quot;)
      .attr(&quot;transform&quot;, function(d) { return &quot;translate(&quot; + d.x + &quot;,&quot; + d.y + &quot;)&quot;; });

  node.append(&quot;rect&quot;)
      .attr(&quot;height&quot;, function(d) { return d.dy; })
      .attr(&quot;width&quot;, sankey.nodeWidth())
      .style(&quot;fill&quot;, function(d) { return d.color = color(d.name.replace(/ .*/, &quot;&quot;)); })
      .style(&quot;stroke&quot;, function(d) { return d3.rgb(d.color).darker(2); })
    .append(&quot;title&quot;)
      .text(function(d) { return d.name + &quot;\n&quot; + format(d.value); });

  node.append(&quot;text&quot;)
      .attr(&quot;x&quot;, -6)
      .attr(&quot;y&quot;, function(d) { return d.dy / 2; })
      .attr(&quot;dy&quot;, &quot;.35em&quot;)
      .attr(&quot;text-anchor&quot;, &quot;end&quot;)
      .attr(&quot;transform&quot;, null)
      .text(function(d) { return d.name; })
    .filter(function(d) { return d.x &lt; width / 2; })
      .attr(&quot;x&quot;, 6 + sankey.nodeWidth())
      .attr(&quot;text-anchor&quot;, &quot;start&quot;);


  // Input label
  svg.append(&quot;text&quot;)
      .attr(&quot;transform&quot;, &quot;rotate(-90)&quot;)
      .attr(&quot;x&quot;, -height/2)
      .attr(&quot;y&quot;, -20)
      .attr(&quot;dy&quot;, &quot;.71em&quot;)
      .style(&quot;text-anchor&quot;, &quot;middle&quot;)
      .style(&quot;font-size&quot;, &quot;16px&quot;)
      .text(&quot;Inputs&quot;);

  // Output label
  svg.append(&quot;text&quot;)
      .attr(&quot;transform&quot;, &quot;rotate(-90)&quot;)
      .attr(&quot;x&quot;, -height/2)
      .attr(&quot;y&quot;, width+10)
      .attr(&quot;dy&quot;, &quot;.71em&quot;)
      .style(&quot;text-anchor&quot;, &quot;middle&quot;)
      .style(&quot;font-size&quot;, &quot;16px&quot;)
      .text(&quot;Outputs&quot;);

// });
&lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>Comparing the opinions of economic experts and the general public</title>
   <link href="http://localhost:4000/2016/04/10/comparing-the-opinions-of-economic-experts/"/>
   <updated>2016-04-10T00:00:00-07:00</updated>
   <id>http://localhost:4000/2016/04/10/comparing-the-opinions-of-economic-experts</id>
   <content type="html">&lt;meta charset=&quot;utf-8&quot; /&gt;

&lt;style&gt;

.axis path,
.axis line {
  fill: none; /* chance go none */
  stroke: #000;
  shape-rendering: crispEdges;
}

.axis text {
  font: 12px sans-serif;
}

.survey_dot {
  stroke: #fff;
  fill: #f58032;
}

.tooltip {
  position: absolute;
  text-align: left;
  width: 150px;
  padding: 8px;
  margin-top: -20px;
  font: 10px sans-serif;
  background: rgba(200, 200, 200, 0.9);
  pointer-events: none;
}

&lt;/style&gt;

&lt;p&gt;Last week on Marginal Revolution, there was a link to a wonderful paper comparing the policy opinions of economic experts to those of the general public. The &lt;a href=&quot;http://faculty.chicagobooth.edu/luigi.zingales/papers/research/economic-experts-vs-average-americans.pdf&quot;&gt;paper&lt;/a&gt;, by &lt;a href=&quot;http://www.kellogg.northwestern.edu/faculty/directory/sapienza_paola.aspx&quot;&gt;Paola Sapienza&lt;/a&gt; and &lt;a href=&quot;http://www.chicagobooth.edu/faculty/directory/z/luigi-zingales&quot;&gt;Luigi Zingales&lt;/a&gt;, found some pretty significant discrepancies between the two groups. The authors attributed this difference to the degree of trust each group put in the implicit assumptions embedded into the economists’ answers. It’s an excellent and fascinating read. However, like pretty much all academic economics papers, it displays its data in cumbersome text tables rather than figures. In this blog post, I’ve created some figures from the data that I think are easier to read than tables.&lt;/p&gt;

&lt;p&gt;For each policy question, the paper provides two numbers. One is the percentage of economic experts who agree with the policy position. The second is the percentage of general public respondents who agree with the policy position. This sounds like a good opportunity for a scatter plot:&lt;/p&gt;

&lt;!-- The tooltip has absolute positioning, which means it is positioned
&quot;relative&quot; to any parent it has who has either absolute or relative positioning.
The #econ_scatter parent would by default be static, so I have to change it to
relative --&gt;
&lt;div class=&quot;wrapper&quot;&gt;
  &lt;div id=&quot;econ_scatter&quot; class=&quot;inner&quot; style=&quot;position:relative&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;There’s a light negative correlation (&lt;em&gt;r&lt;/em&gt; = -0.47) between how much economic experts and the general public agree with these positions. As shown in the bottom right, the vast majority of economists prefer a carbon tax to car emissions standards, whereas the vast majority of the general public prefers the reverse. As shown in the top left, the vast majority of the general public believes that requiring the government to “Buy American” is an effective way to improve manufacturing employment, whereas the majority of economists do not. For the exact wording of the questions, see the &lt;a href=&quot;http://docplayer.net/9302120-Economic-experts-vs-average-americans-online-appendix.html&quot;&gt;Appendix&lt;/a&gt; to the original paper.&lt;/p&gt;

&lt;p&gt;Another way to visualize the same data is with a &lt;a href=&quot;http://charliepark.org/slopegraphs/&quot;&gt;slopegraph&lt;/a&gt;. Below, the left column shows all the policy positions ranked by agreement from economic experts. The right column shows the same policy positions ranked by agreement from the general public. This type of plot vividly shows how unanimous the experts are on a few beliefs: It’s very hard to predict the stock market, we’re on the left side of the Laffer Curve, and the US economy is fiscally unsustainable without healthcare cuts or taxes hikes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://chris-said.io/assets/fig_econ_poll.png&quot;&gt;&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_econ_poll.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Slopegraphs are useful in this context because they create less text overlap than scatter plots. With the scatter plot above, I had to use interactive mouseover events to selectively show the text for individual data points. This wouldn’t be possible in a publication, since most economics journals still require static PDFs.&lt;/p&gt;

&lt;p&gt;Even with a slopegraph, there is still some risk of overlapping text. To keep this from happening, I added some light repulsion to the data points.&lt;/p&gt;

&lt;p&gt;For those interested, the experts in this dataset come from the &lt;a href=&quot;http://www.igmchicago.org/igm-economic-experts-panel&quot;&gt;Economic Expert Panel&lt;/a&gt;, a diverse set of economists comprising Democrats, Republicans, and Independents. This panel is the same panel that generates the data used in my &lt;a href=&quot;http://whichfamouseconomistareyoumostsimilarto.com/&quot;&gt;Which Famous Economist&lt;/a&gt; website.&lt;/p&gt;

&lt;p&gt;[Python &lt;a href=&quot;https://gist.github.com/csaid/21677bb64c1579f9e9d4852529331ac2&quot;&gt;code&lt;/a&gt; for the slope graph. Scatter plot is in page source.]&lt;/p&gt;

&lt;script&gt;
&lt;!-- Example based on http://bl.ocks.org/weiglemc/6185069 --&gt;


var survey_results = [
  {
    &quot;issue&quot;: &quot;Moving education funding to school vouchers would benefit students&quot;,
    &quot;public_agreement&quot;: 56.2,
    &quot;expert_agreement&quot;: 51.4
  },
  {
    &quot;issue&quot;: &quot;Benefits of automakers bailouts will exceed their cost&quot;,
    &quot;public_agreement&quot;: 51.9,
    &quot;expert_agreement&quot;: 57.5
  },
  {
    &quot;issue&quot;: &quot;To reduce student loan risk, link college eligibility to performance&quot;,
    &quot;public_agreement&quot;: 61.0,
    &quot;expert_agreement&quot;: 69.7
  },
  {
    &quot;issue&quot;: &quot;2009 Stimulus: benefits will exceed its costs&quot;,
    &quot;public_agreement&quot;: 43.4,
    &quot;expert_agreement&quot;: 52.7
  },
  {
    &quot;issue&quot;: &quot;Large banks are big mostly for efficiency gains, not for political power&quot;,
    &quot;public_agreement&quot;: 39.4,
    &quot;expert_agreement&quot;: 17.9
  },
  {
    &quot;issue&quot;: &quot;CEOs are paid more than the value they add to firms&quot;,
    &quot;public_agreement&quot;: 66.8,
    &quot;expert_agreement&quot;: 39.3
  },
  {
    &quot;issue&quot;: &quot;2010 unemployment rate was lower thanks to automaker bailout&quot;,
    &quot;public_agreement&quot;: 54.8,
    &quot;expert_agreement&quot;: 84.8
  },
  {
    &quot;issue&quot;: &quot;2008 bank bailouts: benefits outweighed costs&quot;,
    &quot;public_agreement&quot;: 38.7,
    &quot;expert_agreement&quot;: 69.7
  },
  {
    &quot;issue&quot;: &quot;Raising taxes on the rich would increase tax revenue&quot;,
    &quot;public_agreement&quot;: 66.3,
    &quot;expert_agreement&quot;: 97.4
  },
  {
    &quot;issue&quot;: &quot;Large banks would be smaller without government support&quot;,
    &quot;public_agreement&quot;: 65.2,
    &quot;expert_agreement&quot;: 33.3
  },
  {
    &quot;issue&quot;: &quot;Fannie &amp; Freddie do not rebate subsidies through lower interest rates&quot;,
    &quot;public_agreement&quot;: 66.7,
    &quot;expert_agreement&quot;: 31.4
  },
  {
    &quot;issue&quot;: &quot;Changes in US gasoline prices are mainly due to market factors&quot;,
    &quot;public_agreement&quot;: 54.3,
    &quot;expert_agreement&quot;: 92.3
  },
  {
    &quot;issue&quot;: &quot;It is hard to predict stock prices&quot;,
    &quot;public_agreement&quot;: 55.2,
    &quot;expert_agreement&quot;: 100.0
  },
  {
    &quot;issue&quot;: &quot;2009 ARRA lowered unemployment rate&quot;,
    &quot;public_agreement&quot;: 45.6,
    &quot;expert_agreement&quot;: 91.6
  },
  {
    &quot;issue&quot;: &quot;NAFTA increased welfare&quot;,
    &quot;public_agreement&quot;: 46.1,
    &quot;expert_agreement&quot;: 94.5
  },
  {
    &quot;issue&quot;: &quot;Eliminating mortgage deduction improves individual finance efficiency&quot;,
    &quot;public_agreement&quot;: 35.6,
    &quot;expert_agreement&quot;: 89.4
  },
  {
    &quot;issue&quot;: &quot;\&quot;Buy American\&quot; has significant impact on manufacturing employment&quot;,
    &quot;public_agreement&quot;: 75.6,
    &quot;expert_agreement&quot;: 11.4
  },
  {
    &quot;issue&quot;: &quot;US economy is sustainable w/o healthcare cuts or tax hikes&quot;,
    &quot;public_agreement&quot;: 67.6,
    &quot;expert_agreement&quot;: 0.00
  },
  {
    &quot;issue&quot;: &quot;A carbon tax is more efficient than car emission standards&quot;,
    &quot;public_agreement&quot;: 22.5,
    &quot;expert_agreement&quot;: 92.5
  }
  ]

if( /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) ) {
  var full_width = 400
  var full_height = 300;
  var r = 10
} else {
  var full_width = 550
  var full_height = 400;
  var r = 12
}

var margin = {top: 30, right: 20, bottom: 50, left: 50},
    width = full_width - margin.left - margin.right,
    econ_survey_height = full_height - margin.top - margin.bottom;

/*
 * value accessor - returns the value to encode for a given data object.
 * scale - maps value to a visual display encoding, such as a pixel position.
 * map function - maps from data value to display value
 * axis - sets up axis
 */

// setup x
var xValue = function(d) { return d.expert_agreement;}, // data -&gt; value
    xScale = d3.scaleLinear().range([0, width]), // value -&gt; display
    xMap = function(d) { return xScale(xValue(d));}, // data -&gt; display
    xAxisEconSurvey = d3.axisBottom(xScale);

// setup y
var yValue = function(d) { return d.public_agreement;}, // data -&gt; value
    yScale = d3.scaleLinear().range([econ_survey_height, 0]), // value -&gt; display
    yMap = function(d) { return yScale(yValue(d));}, // data -&gt; display
    yAxisEconSurvey = d3.axisLeft(yScale);


// add the graph canvas to the #econ_scatter of the webpage
var svg = d3.select(&quot;#econ_scatter&quot;).append(&quot;svg&quot;)
    .attr(&quot;width&quot;, width + margin.left + margin.right)
    .attr(&quot;height&quot;, econ_survey_height + margin.top + margin.bottom)
  .append(&quot;g&quot;)
    .attr(&quot;transform&quot;, &quot;translate(&quot; + margin.left + &quot;,&quot; + margin.top + &quot;)&quot;);

// add the tooltip area to the webpage
var tooltip = d3.select(&quot;#econ_scatter&quot;).append(&quot;div&quot;)
    .attr(&quot;class&quot;, &quot;tooltip&quot;)
    .style(&quot;display&quot;, &quot;none&quot;)
    // .style(&quot;opacity&quot;, 0);


  // don't want dots overlapping axis, so add in buffer to data domain
  xScale.domain([-5, 105]);
  yScale.domain([15, 85]);

  // x-axis
  svg.append(&quot;g&quot;)
      .attr(&quot;class&quot;, &quot;x axis&quot;)
      .attr(&quot;transform&quot;, &quot;translate(0,&quot; + econ_survey_height + &quot;)&quot;)
      .call(xAxisEconSurvey);

  // y-axis
  svg.append(&quot;g&quot;)
      .attr(&quot;class&quot;, &quot;y axis&quot;)
      .call(yAxisEconSurvey);

  // x-label
  svg.append(&quot;text&quot;)
      .attr(&quot;x&quot;, width/2)
      .attr(&quot;y&quot;, econ_survey_height + 40)
      .style(&quot;text-anchor&quot;, &quot;middle&quot;)
      .style(&quot;font-size&quot;, &quot;14px&quot;)
      .text(&quot;Expert Agreement (%)&quot;);

  // y-label
  svg.append(&quot;text&quot;)
      .attr(&quot;transform&quot;, &quot;rotate(-90)&quot;)
      .attr(&quot;x&quot;, -econ_survey_height/2)
      .attr(&quot;y&quot;, -45)
      .attr(&quot;dy&quot;, &quot;.71em&quot;)
      .style(&quot;text-anchor&quot;, &quot;middle&quot;)
      .style(&quot;font-size&quot;, &quot;14px&quot;)
      .text(&quot;Public Agreement (%)&quot;);

  // draw dots
  svg.selectAll(&quot;.survey_dot&quot;)
      .data(survey_results)
    .enter().append(&quot;circle&quot;)
      .attr(&quot;class&quot;, &quot;survey_dot&quot;)
      .attr(&quot;r&quot;, r)
      .attr(&quot;cx&quot;, xMap)
      .attr(&quot;cy&quot;, yMap)
      .on(&quot;mouseover&quot;, mouseover)
      .on(&quot;mousemove&quot;, mousemove)
      .on(&quot;mouseout&quot;, mouseout);


  function mouseover(d) {
    tooltip.style(&quot;display&quot;, &quot;inline&quot;)
      .style(&quot;position&quot;, &quot;absolute&quot;);
  }

  function mousemove(d) {
    tooltip
        .text(d.issue)
        .style(&quot;position&quot;, &quot;absolute&quot;)
        .style(&quot;left&quot;, (xMap(d)) + &quot;px&quot;)
        .style(&quot;top&quot;, (yMap(d)) + &quot;px&quot;);
  }

  function mouseout(d) {
    tooltip.style(&quot;display&quot;, &quot;none&quot;)
    .style(&quot;position&quot;, &quot;absolute&quot;);
  }

&lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>Four pitfalls of hill climbing</title>
   <link href="http://localhost:4000/2016/02/28/four-pitfalls-of-hill-climbing/"/>
   <updated>2016-02-28T00:00:00-08:00</updated>
   <id>http://localhost:4000/2016/02/28/four-pitfalls-of-hill-climbing</id>
   <content type="html">&lt;meta charset=&quot;utf-8&quot; /&gt;

&lt;p&gt;One of the great developments in product design has been the adoption of A/B testing. Instead of just guessing what is best for your customers, you can offer a product variant to a subset of customers and measure how well it works. While undeniably useful, A/B testing is sometimes said to encourage too much “hill climbing”, an incremental and short-sighted style of product development that emphasizes easy and immediate wins.&lt;/p&gt;

&lt;p&gt;Discussion around hill climbing can sometimes get a bit vague, so I thought I would make some animations that describe four distinct pitfalls that can emerge from an overreliance on hill climbing.&lt;/p&gt;

&lt;h4 id=&quot;1-local-maxima&quot;&gt;1. Local maxima&lt;/h4&gt;

&lt;p&gt;If you climb hills incrementally, you may end up in a local maximum and miss out on an opportunity to land on a global maximum with much bigger reward. Concerns about local maxima are often wrapped up in critiques of &lt;em&gt;incrementalism&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;div class=&quot;inner&quot; id=&quot;plot1&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Local maxima and global maxima can be illustrated with hill diagrams like the one above. The horizontal axis represents product space collapsed into a single dimension. In reality, of course, there are many dimensions that a product could explore.&lt;/p&gt;

&lt;h4 id=&quot;2-emergent-maxima&quot;&gt;2. Emergent maxima&lt;/h4&gt;

&lt;p&gt;If you run short A/B tests, or A/B tests that do not fully capture network effects, you might not realize that a change that initially seems bad may be good in the long run. This idea, which is distinct from concerns about incrementalism, can be described with a dynamic reward function animation. As before, the horizontal axis is product space. Each video frame represents a time step, and the vertical axis represents immediate, measurable reward.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;div class=&quot;inner&quot; id=&quot;plot2&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;When a product changes, the intial effect is negative. But eventually, customers begin to enjoy the new version, as shown by changes in the reward function. By waiting at a position that initially seemed negative, you are able to discover an emergent mountain, and receive greater reward than you would have from short-term optimization.&lt;/p&gt;

&lt;h4 id=&quot;3-novelty-effects&quot;&gt;3. Novelty effects&lt;/h4&gt;

&lt;p&gt;Short-term optimization can be bad, not only because it prevents discovery of emergent mountains, but also because some hills can be transient. One way a hill can disappear is through &lt;em&gt;novelty effects&lt;/em&gt;, where a shiny new feature can be engaging to customers in the short term, but uninteresting or even negative in the long term.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;div class=&quot;inner&quot; id=&quot;plot3&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h4 id=&quot;4-loss-of-differentiation&quot;&gt;4. Loss of differentiation&lt;/h4&gt;

&lt;p&gt;Another way a hill can disappear is through loss of differentiation from more dominant competitors. Your product may occupy a market niche. If you try to copy your competitor, you may initially see some benefits. But at some point, your customers may leave because not much separates you from your more dominant competitor. Differentiation matters in some dimensions more than others.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
  &lt;span id=&quot;plot4&quot;&gt;&lt;/span&gt;
  &lt;span id=&quot;plot5&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;You can think of an industry as a dynamic ecosystem where each company has its own reward function. When one company moves, it changes its own reward function as well as the reward functions of other companies. If this sounds like biology, you’re not mistaken. The dynamics here are similar to evolutionary &lt;a href=&quot;http://www.randalolson.com/2014/04/17/visualizing-evolution-in-action-dynamic-fitness-landscapes/&quot;&gt;fitness landscapes&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;While all of the criticisms of hill climbing have obvious validity, I think it is easy for people to overreact to them. Here are some caveats in defense of hill climbing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The plots above probably exaggerate the magnitude and frequency with which reward functions change.&lt;/li&gt;
  &lt;li&gt;There is huge uncertainty and disagreement about what future landscapes will look like. In most cases, it’s better to explore regions that increase (rather than decrease) reward, making sure to run long term experiments when needed.&lt;/li&gt;
  &lt;li&gt;The space is high dimensional. Even if your product is at a local maximum in one dimension, there are many other dimensions to explore and measure.&lt;/li&gt;
  &lt;li&gt;We may overestimate the causal relationship between bold product moves and company success. Investors often observe that companies who don’t make bold changes are doomed to fail. While I don’t doubt that there is some causation here, I think there is also some reverse causation. Bold changes require lots of resources. Maybe it’s mostly the success-bound companies who have enough resources to afford the bold changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Special thanks to Marika Inhoff, &lt;a href=&quot;https://twitter.com/johnvmcdonnell&quot;&gt;John McDonnell&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/alexrosner&quot;&gt;Alex Rosner&lt;/a&gt; for comments on a draft of this post.&lt;/p&gt;
&lt;style&gt;

circle {
  stroke: #fff;
  stroke-width: 1.5px;
}

circle.you {
  fill: #ED2685;
}

circle.competitor {
  fill: #3AC3F2;
}

.background {
  fill: #e7e7e7;
}

.axis .tick:nth-child(2n) {
  stroke-opacity: 0.5;
}

.axis text {
  font: 12px sans-serif;
}

.line path {
  fill: none;
  stroke: #000;
  stroke-width: 2px;
  stroke-linecap: round;
  stroke-linejoin: round;
}

}

&lt;/style&gt;

&lt;!-- I think Markdown doesn't like two script tags in a row --&gt;
&lt;div&gt;&lt;/div&gt;

&lt;script&gt;





function makeObjectiveGraph(id, get_dot_x, objective, title, is_dynamic, company) {

  // Dot radius
  var r = 11;

  var margin = {top: 20, right: 20, bottom: 20, left: 20},
      width = 270 - margin.left - margin.right,
      height = 270 - margin.top - margin.bottom;

  var tickFormat = d3.format(&quot;.1f&quot;);

  var y = d3.scaleLinear()
      .domain([0, 1])
      .range([height, 0]);

  var yAxisPitfalls = d3.axisLeft(y)

  var x = d3.scaleLinear()
      .domain([0, 1])
      .range([0, width]);

  var xAxisPitfalls = d3.axisBottom(x)

  var path = d3.line();

  var svg = d3.select(&quot;#&quot; + id).append(&quot;svg&quot;)
      .attr(&quot;width&quot;, width + margin.left + margin.right)
      .attr(&quot;height&quot;, height + margin.top + margin.bottom)
    .append(&quot;g&quot;)
      .attr(&quot;transform&quot;, &quot;translate(&quot; + margin.left + &quot;,&quot; + margin.top + &quot;)&quot;);

  svg.append(&quot;rect&quot;)
      .attr(&quot;class&quot;, &quot;background&quot;)
      .attr(&quot;width&quot;, width)
      .attr(&quot;height&quot;, height);

  var line = svg.append(&quot;g&quot;)
      .attr(&quot;class&quot;, &quot;line&quot;)
    .append(&quot;path&quot;);

  var dot = svg.append(&quot;circle&quot;)
      .attr(&quot;r&quot;, r-1)
      .attr(&quot;class&quot;, company);

  // For X- and Y- labels.
  if (company == &quot;competitor&quot;){
    var prefix = &quot;Competitor &quot;;
  } else {
    var prefix = &quot;&quot;;
  }

  // Title
  svg.append(&quot;text&quot;)
      .attr(&quot;class&quot;, &quot;x label&quot;)
      .attr(&quot;text-anchor&quot;, &quot;middle&quot;)
      .attr(&quot;x&quot;, width/2)
      .attr(&quot;y&quot;, -8)
      .style(&quot;font-size&quot;, &quot;14px&quot;)
      .style(&quot;font-weight&quot;, &quot;bold&quot;)
      .text(title);

  // X-Label
  svg.append(&quot;text&quot;)
      .attr(&quot;class&quot;, &quot;x label&quot;)
      .attr(&quot;text-anchor&quot;, &quot;middle&quot;)
      .attr(&quot;x&quot;, width/2)
      .attr(&quot;y&quot;, height + 14)
      .style(&quot;font-size&quot;, &quot;12px&quot;)
      .text(prefix + &quot;Product Space&quot;);

  // Y-Label
  svg.append(&quot;text&quot;)
      .attr(&quot;class&quot;, &quot;y label&quot;)
      .attr(&quot;text-anchor&quot;, &quot;middle&quot;)
      .attr(&quot;x&quot;, -width/2)
      .attr(&quot;y&quot;, -14)
      .attr(&quot;dy&quot;, &quot;.75em&quot;)
      .attr(&quot;transform&quot;, &quot;rotate(-90)&quot;)
      .style(&quot;font-size&quot;, &quot;12px&quot;)
      .text(prefix + &quot;Reward&quot;);


  if (!is_dynamic){
    line.attr(&quot;d&quot;, path(d3.range(0, 1, .002).concat(1).map(function(xo) {
      return [x(xo), y(objective(xo))];
    })));
  }


  // Provide pixel coordinates of small segment of curve.
  // Returns pixel coordinates just above segment.
  function get_adjusted_dot_coords(start_x, start_y, end_x, end_y) {
    var center_x = (start_x + end_x) / 2;
    var center_y = (start_y + end_y) / 2;
    var rise = (end_y - start_y);
    var run = (end_x - start_x);
    var k = Math.sqrt(Math.pow(r, 2)/(Math.pow(run, 2) + Math.pow(rise, 2)));
    var dx = k * rise;
    var dy = k * run;

    return [center_x + dx, center_y - dy]

  };

  // Functions x and y return pixel units.
  // Var xo is in graph coordinates from 0 to 1.
  // Var t is from 0 to 1.
  d3.timer(function(elapsed) {
    var t = (elapsed % 3000) / 3000; // will range from 0 to 1.
    var dot_xo = get_dot_x(t) // make this a function

    if (is_dynamic) {

      dot_coords = get_adjusted_dot_coords(
                                           x(dot_xo-.001),
                                           y(objective(t, dot_xo-.001)),
                                           x(dot_xo+.001),
                                           y(objective(t, dot_xo+.001))
                                           );

      // dot_coords = [x(dot_xo), y(objective(t, dot_xo))]

      dot.attr(&quot;cx&quot;, dot_coords[0]).attr(&quot;cy&quot;, dot_coords[1]);

      // dot.attr(&quot;cx&quot;, x(t)).attr(&quot;cy&quot;, y(objective(t, t))-12); // The 12 is in pixel world. Subtracting because pixel 0 is at top.

      line.attr(&quot;d&quot;, path(d3.range(0, 1, .002).concat(1).map(function(xo) {
        return [x(xo), y(objective(t, xo))];
      })));
    } else {

      dot_coords = get_adjusted_dot_coords(x(dot_xo-.001), y(objective(dot_xo-.001)), x(dot_xo+.001), y(objective(dot_xo+.001)))
      dot.attr(&quot;cx&quot;, dot_coords[0]).attr(&quot;cy&quot;, dot_coords[1]);
    }


  });

  d3.select(self.frameElement).style(&quot;height&quot;, height + margin.top + margin.bottom + &quot;px&quot;);
}

var left_pos  = 0.20;
var mid_pos   = 0.50;
var right_pos = 0.80;
var mu = 0.25


// Provides soft trajectory, given starting and stopping times and locations.
// Generally the first agrument will be time t, but using x
// internally here so I can think about the functions visually.
var soft_motion = function(x, start_x, start_y, end_x, end_y) {
  var period = 2 * (end_x - start_x);
  var scale = (end_y - start_y)/2;
  var intercept = start_y + scale;

  if (x &lt; start_x) { return start_y } else
  if (x &lt; end_x) { return -scale * Math.cos(2*Math.PI/period * (x-start_x)) + intercept} else
  return end_y
};

// The horizontal position of your dot
var get_dot_x = function(t) {
  return soft_motion(t, .2, .2, .8, .8)
};

var get_dot_x_local_max = function(t) {
  return soft_motion(t, .2, .35, .8, .2)
};

// The horizontal position of your competitor's dot
var get_competitor_dot_x = function(t) {
  return 0.8;
};

// Gaussian function. For dynamics, call repeatedly with different scale values.
function gauss(x, scale, mu, sig) {
  var coef = scale / (sig* Math.sqrt(2 * Math.PI));
  var numer = Math.pow((x-mu), 2)
  var denom = 2*Math.pow(sig, 2)
  return coef * Math.exp(-numer/denom);
};


var objective1 = function(x){
  var left_scale = 0.2;
  var right_scale = 0.3;
  var sd = 0.2
  return gauss(x, left_scale, left_pos, sd) + gauss(x, right_scale, right_pos, sd)
}

makeObjectiveGraph(&quot;plot1&quot;, get_dot_x_local_max, objective1, &quot;&quot;, false, &quot;you&quot;)


var objective2 = function(t, x){
  var left_scale  = soft_motion(t, 0.6, 0.2, 0.95, 0.10);
  var right_scale = soft_motion(t, 0.6, 0.0, 0.95, 0.3);
  var sd = 0.2

  return gauss(x, left_scale, left_pos, sd) + gauss(x, right_scale, right_pos, sd)
};

makeObjectiveGraph(&quot;plot2&quot;, get_dot_x, objective2, &quot;&quot;, true, &quot;you&quot;)


var objective3 = function(t, x){
  var left_scale  = soft_motion(t, 0.6, 0.15, 0.95, 0.15);
  var right_scale = soft_motion(t, 0.6, 0.30, 0.95, 0.15);

  return gauss(x, left_scale, left_pos, .25) + gauss(x, right_scale, right_pos, .3)
};

makeObjectiveGraph(&quot;plot3&quot;, get_dot_x, objective3, &quot;&quot;, true, &quot;you&quot;)

var objective4 = function(t, x){
  var left_scale  = soft_motion(t, 0.6, 0.15, 0.95, 0.15);
  var right_scale = soft_motion(t, 0.6, 0.30, 0.95, 0.15);

  return gauss(x, left_scale, left_pos, .25) + gauss(x, right_scale, right_pos, .3)
};

makeObjectiveGraph(&quot;plot4&quot;, get_dot_x, objective4, &quot;Losing differentiation&quot;, true, &quot;you&quot;)

// Competitor
var objective5 = function(t, x){
  var left_scale  = soft_motion(t, 0.6, 0.05, 0.95, 0.07);
  var right_scale = soft_motion(t, 0.6, 0.4,  0.95, 0.50);

  return gauss(x, left_scale, left_pos, .15) + gauss(x, right_scale, right_pos, .3)
};

makeObjectiveGraph(&quot;plot5&quot;, get_competitor_dot_x, objective5, &quot;Dominant competitor wins&quot;, true, &quot;competitor&quot;)


&lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>How to make polished Jupyter presentations with optional code visibility</title>
   <link href="http://localhost:4000/2016/02/13/how-to-make-polished-jupyter-presentations-with-optional-code-visibility/"/>
   <updated>2016-02-13T00:00:00-08:00</updated>
   <id>http://localhost:4000/2016/02/13/how-to-make-polished-jupyter-presentations-with-optional-code-visibility</id>
   <content type="html">&lt;p&gt;Jupyter notebooks are great because they allow you to easily present interactive figures. In addition, these notebooks include the figures and code in a single file, making it easy for others to reproduce your results. Sometimes though, you may want to present a cleaner report to an audience who may not care about the code. This blog post shows how to make code visibility optional, and how to remove various Jupyter elements to get a clean presentation.&lt;/p&gt;

&lt;p&gt;On the top is a typical Jupyter presentation with code and some extra elements. Below that is a more polished version that removes some of the extra elements and makes code visibility optional with a button.&lt;/p&gt;

&lt;div class=&quot;wrapper&quot;&gt;
    &lt;a class=&quot;inner&quot; href=&quot;http://nbviewer.jupyter.org/github/csaid/polished_notebooks/blob/master/notebook_original.ipynb&quot; target=&quot;_blank&quot;&gt;
        &lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_unpolished_small.png&quot; alt=&quot;&quot; border=&quot;2px&quot; /&gt;&lt;/figure&gt;

    &lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;wrapper&quot;&gt;
    &lt;a class=&quot;inner&quot; href=&quot;http://nbviewer.jupyter.org/github/csaid/polished_notebooks/blob/master/notebook_polished.ipynb&quot; target=&quot;_blank&quot;&gt;
        &lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_polished_small.png&quot; alt=&quot;&quot; border=&quot;2px&quot; /&gt;&lt;/figure&gt;

    &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;To make the code optionally visible, available at the click of a button, include a raw cell at the beginning of your notebook containing the JavaScript and HTML below. This code sample is inspired by a &lt;a href=&quot;http://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer&quot;&gt;Stack Overflow post&lt;/a&gt;, but makes a few improvements such as using a raw cell so that the button position stays fixed, changing the button text depending on state, and displaying gradual transitions so the user understands what is happening.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;script&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;code_toggle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;code_shown&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;div.input&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;hide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;#toggleButton&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Show Code&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;div.input&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;#toggleButton&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Hide Code&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;code_shown&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;code_shown&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;code_shown&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;div.input&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;hide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;form&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;action=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;javascript:code_toggle()&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;input&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;submit&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;toggleButton&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Show Code&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/form&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;It’s pretty straightforward to remove the extra elements like the header, footer, and prompt numbers. That being said, you may want to still include some attribution to the Jupyter project and to your free hosting service. To do all of this, just include a raw cell at the end of your notebook with some more JavaScript.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;script&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;div.prompt&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;hide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;div.back-to-top&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;hide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;nav#menubar&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;hide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;.breadcrumb&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;hide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;.hidden-print&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;hide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;footer&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;attribution&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;style=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;float:right; color:#999; background:#fff;&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
Created with Jupyter, delivered by Fastly, rendered by Rackspace.
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/footer&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;One shortcoming with what we have so far is that users may still see some code or other unwanted elements while the page is loading. This can be especially problematic if you have a long presentation with many plots. To avoid this problem, add a raw cell at the very top of your notebook containing a preloader. This example preloader includes an animation that signals to users that the page is still loading. It heavily inspired by &lt;a href=&quot;http://codepen.io/mimoYmima/pen/fisgL&quot;&gt;this preloader&lt;/a&gt; created by &lt;a href=&quot;https://twitter.com/@mimoYmima&quot;&gt;@mimoYmima&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;script&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;jQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;

  &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;#preloader&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;fadeOut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;slow&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();});&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;style &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text/css&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;div&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;#preloader&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nl&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;fixed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;nl&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;nl&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;nl&quot;&gt;z-index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;999&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;nl&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;nl&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;nl&quot;&gt;overflow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;visible&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;nl&quot;&gt;background&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;#fff&lt;/span&gt; &lt;span class=&quot;sx&quot;&gt;url('http://preloaders.net/preloaders/720/Moving%20line.gif')&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;no-repeat&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;center&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;/style&amp;gt;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;preloader&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;meta charset=&quot;utf-8&quot; /&gt;

&lt;p&gt;To work with these notebooks, you can clone my &lt;a href=&quot;https://github.com/csaid/polished_notebooks&quot;&gt;GitHub repository&lt;/a&gt;. While the notebooks render correctly on nbviewer (&lt;a href=&quot;http://nbviewer.jupyter.org/github/csaid/polished_notebooks/blob/master/notebook_original.ipynb&quot;&gt;unpolished&lt;/a&gt;, &lt;a href=&quot;http://nbviewer.jupyter.org/github/csaid/polished_notebooks/blob/master/notebook_polished.ipynb&quot;&gt;polished&lt;/a&gt;), they do not render correctly on the GitHub viewer.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>New Blog Address</title>
   <link href="http://localhost:4000/2015/09/16/new-blog-address/"/>
   <updated>2015-09-16T00:00:00-07:00</updated>
   <id>http://localhost:4000/2015/09/16/new-blog-address</id>
   <content type="html">&lt;p&gt;Welcome to the new location for The File Drawer! This blog is now hosted on &lt;a href=&quot;https://pages.github.com/&quot;&gt;Github Pages&lt;/a&gt; and powered by &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;. My old blog at &lt;a href=&quot;http://filedrawer.wordpress.com&quot;&gt;filedrawer.wordpress.com&lt;/a&gt; will be shutting down soon.&lt;/p&gt;

&lt;p&gt;I actually really liked WordPress, but I wanted to have a little bit more control over what I can put in my posts. In particular, I wanted to be able to insert my own JavaScript animations, for example in &lt;a href=&quot;http://chris-said.io/2015/09/17/presidential-debates/&quot;&gt;this post&lt;/a&gt; on the recent presidential debates.&lt;/p&gt;

&lt;p&gt;It was pretty fun to migrate everything over from WordPress – which is not to say there weren’t some hiccups along the way – but I was able to do so by following some nice instructions in &lt;a href=&quot;http://joshualande.com/jekyll-github-pages-poole/&quot;&gt;this post by Joshua Lande&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For those of you using RSS, the feed for &lt;a href=&quot;http://chris-said.io/&quot;&gt;www.chris-said.io&lt;/a&gt; should be searchable in your RSS readers, but please let me know if it’s not.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>10 classic dialogues you can find on the internet</title>
   <link href="http://localhost:4000/2015/08/08/10-classic-dialogues/"/>
   <updated>2015-08-08T15:52:24-07:00</updated>
   <id>http://localhost:4000/2015/08/08/10-classic-dialogues</id>
   <content type="html">&lt;p&gt;Some videos on the internet are so good that I’ve watched them twice. Below is a list of 10 of my favorite interviews and dialogues. Obviously this isn’t an endorsement of all the positions taken. I just think they are very well done and fun to watch. The last four are best watched on 1.4x speed.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.dailymotion.com/video/x16z2ff_parkinson-interviews-muhammad-ali-1971-full_news&quot;&gt;1971&lt;/a&gt; Michael Parkinson interviews Muhammad Ali.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://archive.org/details/MuhammadAliParkinsonInterview1974betterSound&quot;&gt;1974&lt;/a&gt; Michael Parkinson interviews Muhammad Ali again, with better sound.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=WxOp5mBY9IY&quot;&gt;1997&lt;/a&gt; Steve Jobs interacting with the audience when announcing the Microsoft deal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FF-tKLISfPE&quot;&gt;1997&lt;/a&gt; Steve Jobs interacting with the audience at WWDC.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://thecolbertreport.cc.com/videos/6quypd/better-know-a-district---district-of-columbia---eleanor-holmes-norton&quot;&gt;2006&lt;/a&gt; Stephen Colbert interviews Eleanor Holmes Norton.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://bloggingheads.tv/videos/2326&quot;&gt;2009&lt;/a&gt; Robert Wright and Joel Achenbach&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://bloggingheads.tv/videos/2022&quot;&gt;2009&lt;/a&gt; Tyler Cowen and Peter Singer&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://bloggingheads.tv/videos/2841&quot;&gt;2010&lt;/a&gt; Robert Wright and Mickey Kaus&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://bloggingheads.tv/videos/3254&quot;&gt;2011&lt;/a&gt; Robert Wright and Mickey Kaus&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://bloggingheads.tv/videos/14073&quot;&gt;2012&lt;/a&gt; Glenn Loury and Ann Althouse&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If I had to recommend just one, it would be &lt;a href=&quot;http://bloggingheads.tv/videos/2022&quot;&gt;Cowen/Singer&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Across industries, we’re getting better at picking metrics</title>
   <link href="http://localhost:4000/2015/05/31/were-getting-better-at-picking-metrics-to-optimize/"/>
   <updated>2015-05-31T12:21:07-07:00</updated>
   <id>http://localhost:4000/2015/05/31/were-getting-better-at-picking-metrics-to-optimize</id>
   <content type="html">&lt;p&gt;Everywhere you look, people are optimizing bad metrics. Sometimes people optimize metrics that aren’t in their self interest, like when startups focus entirely on signup counts while forgetting about retention rates. In other cases, people optimize metrics that serve their immediate short term interest but which are bad for social welfare, like when California corrections officers &lt;a href=&quot;http://mic.com/articles/41531/union-of-the-snake-how-california-s-prison-guards-subvert-democracy&quot;&gt;lobby for longer prison sentences&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The good news is that as we become a more data-driven society, there seems to be a broad trend — albeit a very slow one — towards better metrics. Take the media economy, for example. A few years ago, media companies optimized for clicks, and companies like Upworthy thrived by producing low quality content with clickbaity headlines. But now, thanks to a more sustainable &lt;a href=&quot;https://stratechery.com/2015/buzzfeed-important-news-organization-world/&quot;&gt;business model&lt;/a&gt;, companies like Buzzfeed are &lt;a href=&quot;http://www.buzzfeed.com/bensmith/why-buzzfeed-doesnt-do-clickbait&quot;&gt;optimizing for shares rather than clicks&lt;/a&gt;. It’s not perfect, but overall it’s better for consumers.&lt;/p&gt;

&lt;p&gt;In science, researchers used to optimize for publication counts and citation counts, which biased them towards publishing surprising and interesting results that were unlikely to be true. These metrics &lt;a href=&quot;http://www.talyarkoni.org/blog/2013/03/12/the-truth-is-not-optional-five-bad-reasons-and-one-mediocre-one-for-defending-the-status-quo/&quot;&gt;still loom large&lt;/a&gt;, but increasingly scientists are beginning to optimize for other metrics like &lt;a href=&quot;https://twitter.com/lakens/status/603617310298001410&quot;&gt;open data badges&lt;/a&gt; and &lt;a href=&quot;http://sometimesimwrong.typepad.com/wrong/2014/12/why-i-am-optimistic.html&quot;&gt;reproducibility&lt;/a&gt;, although we still have a long way to go before quality metrics are effectively &lt;a href=&quot;http://journal.frontiersin.org/researchtopic/beyond-open-access-visions-for-open-evaluation-of-scientific-papers-by-post-publication-peer-review-137&quot;&gt;measured&lt;/a&gt; and &lt;a href=&quot;/2012/04/17/its-the-incentives-structure-people-why-science-reform-must-come-from-the-granting-agencies/&quot;&gt;incentivized&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In health care, hospitals used to profit by maximizing the quantity of care. Perversely, hospitals benefited whenever patients were readmitted due to infections acquired in the hospital or due to lack of adequate follow-up plan. Now, with &lt;a href=&quot;http://healthaffairs.org/blog/2014/07/24/examining-medicares-hospital-readmissions-reduction-program/&quot;&gt;ACA&lt;/a&gt; &lt;a href=&quot;http://www.hhs.gov/asl/testify/2013/09/t20130924.html&quot;&gt;policies&lt;/a&gt; that penalize hospitals for avoidable readmissions, hospitals are taking &lt;a href=&quot;https://innovations.ahrq.gov/profiles/statewide-all-payer-financial-incentives-significantly-reduce-hospital-acquired-conditions&quot;&gt;real steps&lt;/a&gt; to improve follow-up care and to reduce hospital-acquired infections. While the metrics should &lt;a href=&quot;http://en.wikipedia.org/wiki/Value-added_modeling&quot;&gt;be&lt;/a&gt; &lt;a href=&quot;https://www.aamc.org/download/382516/data/thehospitalreadmissionsprogramaccuracyandaccountabilityactbills.pdf&quot;&gt;adjusted&lt;/a&gt; so that they don’t unfairly penalize low income hospitals, the overall emphasis on quality rather than quantity is moving things &lt;a href=&quot;http://www.cdc.gov/hai/progress-report/index.html&quot;&gt;in the right direction&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We still are light years from where we need to be, and bad incentives continue to plague everything from government to finance to education. But slowly, as we get better at measuring and storing data, I think we are getting at picking the right metrics.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Independent t-tests and the 83% confidence interval: A useful trick for eyeballing your data.</title>
   <link href="http://localhost:4000/2014/11/30/independent-t-tests-and-the-83-confidence-interval-a-useful-trick-for-eyeballing-your-data/"/>
   <updated>2014-11-30T19:47:37-08:00</updated>
   <id>http://localhost:4000/2014/11/30/independent-t-tests-and-the-83-confidence-interval-a-useful-trick-for-eyeballing-your-data</id>
   <content type="html">&lt;p&gt;Like most people who have analyzed data using frequentist statistics, I have often found myself staring at error bars and trying to guess whether my results are significant. When comparing two independent sample means, this practice is confusing and difficult. The conventions that we use for testing differences between sample means are not aligned with the conventions we use for plotting error bars. As a result, it’s fair to say that there’s a lot of confusion about this issue.&lt;/p&gt;

&lt;p&gt;Some people believe that two independent samples have significantly different means if and only if their standard error bars (68% confidence intervals for large samples) don’t overlap. This belief is incorrect. Two samples can have nonoverlapping standard error bars and still fail to reach statistical significance at &lt;script type=&quot;math/tex&quot;&gt;\alpha=.05&lt;/script&gt;. Other people believe that two means are significantly different if and only if their 95% confidence intervals overlap. This belief is also incorrect. For one sample t-tests, it is true that significance is reached when the 95% confidence interval crosses the test parameter &lt;script type=&quot;math/tex&quot;&gt;\mu_0&lt;/script&gt;. But for two-sample t-tests, which are more common in research, statistical significance can occur with overlapping 95% confidence intervals.&lt;/p&gt;

&lt;p&gt;If neither the 68% confidence interval nor the 95% confidence interval tells us anything about statistical significance, what does? In most situations, the answer is the 83.4% confidence interval. This can be seen in the figure below, which shows two samples with a barely significant difference in means (p=.05). Only the 83.4% confidence intervals shown in the third panel are barely overlapping, reflecting the barely significant results.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_errorbars1.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;To understand why, let’s start by defining the t-statistic for two independent samples:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
t = \frac{\overline{X_1} - \overline{X_2}}{\sqrt{se_1^2 + se_2^2}}
\end{align}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\overline{X_1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\overline{X_2}&lt;/script&gt; are the means of the two samples, and &lt;script type=&quot;math/tex&quot;&gt;se_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;se_2&lt;/script&gt; are their standard errors. By rearranging, we can see that significant results will be barely obtained (&lt;script type=&quot;math/tex&quot;&gt;p=.05&lt;/script&gt;) if the following condition holds:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\overline{X_1} - \overline{X_2} = 1.96\times\sqrt{se_1^2 + se_2^2}
\end{align}&lt;/script&gt;

&lt;p&gt;where 1.96 is the large sample &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; cutoff for significance. Assuming equal standard errors (more on this later), the equation simplifies to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\overline{X_1} - \overline{X_2} = 1.96\times{\sqrt{2}}\times{se}
\end{align}&lt;/script&gt;

&lt;p&gt;On a graph, the quantity &lt;script type=&quot;math/tex&quot;&gt;\overline{X_1} - \overline{X_2}&lt;/script&gt; is the distance between the means. If we want our error bars to just barely touch each other, we should set the length of the half-error bar to be exactly half of this, or:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
1.386\times{se}
\end{align}&lt;/script&gt;

&lt;p&gt;This corresponds to an 83.4% confidence interval on the normal distribution. While this result assumes a large sample size, it remains quite useful for sample sizes as low as 20. The 83.4% confidence interval can also become slightly less useful when the samples have strongly different standard errors, which can stem from very unequal sample sizes or variances. If you really want a solution that generalizes to this situation, you can set your half-error bar on your first sample to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{1.96\times{\sqrt{se_1^2 + se_2^2}}\times{se_1}}{se_1^2 + se_2^2}
\end{align}&lt;/script&gt;

&lt;p&gt;and make the appropriate substitutions to compute the half-error bar in your second sample. However, this solution has the undesirable property that the error bar for one sample depends on the standard error of the other sample. For most purposes, it’s probably better to just plot the 83% confidence interval. If you are eyeballing data for a project that requires frequentist statistics, it is arguably more useful than plotting the standard error or the 95% confidence interval.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: &lt;a href=&quot;http://www.twitter.com/jeffrouder&quot;&gt;Jeff Rouder&lt;/a&gt; helpfully points me to &lt;a href=&quot;http://www.researchgate.net/publication/11578767_Evaluating_statistical_difference_equivalence_and_indeterminacy_using_inferential_confidence_intervals_an_integrated_alternative_method_of_conducting_null_hypothesis_statistical_tests/file/5046351b1018420f84.pdf&quot;&gt;Tryon and Lewis (2008)&lt;/a&gt;, which presents an error bar that generalizes both to unequal standard errors and small samples. Like the last equation presented above, it has the undesirable property that the size of the error bar around a particular sample depends on both samples. But on the plus side, it’s guaranteed to tell you about significance.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Jumping quickly between deep directories</title>
   <link href="http://localhost:4000/2014/10/16/jumping-quickly-between-deep-directories/"/>
   <updated>2014-10-16T14:11:00-07:00</updated>
   <id>http://localhost:4000/2014/10/16/jumping-quickly-between-deep-directories</id>
   <content type="html">&lt;p&gt;I often need to jump between different directories with very deep paths, like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;some/very/deep/directory/project1
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# do stuff in Project 1&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;different/very/deep/directory/project2
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# do stuff in Project 2&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;While it only takes a handful of seconds to switch directories, the extra mental effort often derails my train of thought. Some solutions exist, but they all have their limitations. For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pushd&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;popd&lt;/code&gt; don’t work well for directories you haven’t visited in a while. Aliases require you to manually add a new alias to your .bashrc every time you want to save a new directory.&lt;/p&gt;

&lt;p&gt;I recently found a solution, inspired by &lt;a href=&quot;http://jeroenjanssens.com/2013/08/16/quickly-navigate-your-filesystem-from-the-command-line.html&quot;&gt;this post&lt;/a&gt; from &lt;a href=&quot;https://twitter.com/jeroenhjanssens&quot;&gt;Jeroen Janssens&lt;/a&gt;, that works great and feels totally natural. All it takes is a one-time change to your .bashrc that will allow you to easily save directories and switch between them. To save a directory, just use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mark&lt;/code&gt; function:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd
&lt;/span&gt;some/very/deep/directory/project1
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;mark project1&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To navigate to a saved directory, just use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cdd&lt;/code&gt; function:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;cdd project1
&lt;span class=&quot;c&quot;&gt;# do stuff in Project 1&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;cdd project2
&lt;span class=&quot;c&quot;&gt;# do stuff in Project 2&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You can display a list of your saved directories with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;marks&lt;/code&gt; function, and you can remove a directory from the list with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unmark&lt;/code&gt; function:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;unmark project1&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For any of this to work, you’ll need to add this to your .bashrc, assuming you have a Mac and use the bash shell.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;    &lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;cdd &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-P&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$MARKPATH&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; 2&amp;gt;/dev/null &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;No such mark: &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;mark &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$MARKPATH&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ln&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$MARKPATH&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;unmark &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$MARKPATH&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;function &lt;/span&gt;marks &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;se&quot;&gt;\l&lt;/span&gt;s &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$MARKPATH&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; +2 | &lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'s/  / /g'&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;cut&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;' '&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f9-&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;' -&amp;gt; '&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'{printf &quot;%-10s -&amp;gt; %s\n&quot;, $1, $2}'&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

    _cdd&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;local &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;cur&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;COMP_WORDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[COMP_CWORD]&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;COMPREPLY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=(&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compgen&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-W&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$MARKPATH&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$cur&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;complete&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt; _cdd cdd&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This differs from Jeroen’s &lt;a href=&quot;http://jeroenjanssens.com/2013/08/16/quickly-navigate-your-filesystem-from-the-command-line.html&quot;&gt;original&lt;/a&gt; code in a couple of ways. First, to be more brain-friendly, it names the function “cdd” instead of “jump”. Second, the tab completion works &lt;a href=&quot;https://news.ycombinator.com/item?id=6229291&quot;&gt;better&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Update: &lt;a href=&quot;https://twitter.com/johnvmcdonnell&quot;&gt;John McDonnell&lt;/a&gt; points me to &lt;a href=&quot;https://github.com/joelthelion/autojump&quot;&gt;autojump&lt;/a&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>How failed replications change our effect size estimates</title>
   <link href="http://localhost:4000/2014/07/10/how-failed-replications-change-our-effect-size-estimates/"/>
   <updated>2014-07-10T12:10:32-07:00</updated>
   <id>http://localhost:4000/2014/07/10/how-failed-replications-change-our-effect-size-estimates</id>
   <content type="html">&lt;p&gt;Yesterday I posted a very unscientific &lt;a href=&quot;https://www.surveymonkey.com/s/H7NM96W&quot;&gt;survey&lt;/a&gt; asking researchers to describe how failed replications changed their subjective estimates of effect sizes. The main survey asked for “ballpark estimates” of effect sizes, but an &lt;a href=&quot;http://csaid.shinyapps.io/survey/&quot;&gt;alternative interactive version&lt;/a&gt; allowed researchers to also report their uncertainty by specifying both the mean and variance of their posterior distributions. Thanks to everyone who participated. I won’t be analyzing any new data after this, but it’s never too late to publicly share your estimates!&lt;/p&gt;

&lt;p&gt;Here are the questions. &lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Question 1&lt;/strong&gt;. A 2009 experiment with 50 subjects (25 per cell) is published in Psych Science. The experiment does not require any special equipment other than a questionnaire. It is not pre-registered. The results show an effect size of d=0.5. Let’s define the true effect size to be the average effect size of an infinite number of replications that the original experimenter would deem “reasonably exact” in advance. Based on this information alone, what is your ballpark subjective estimate of the true effect size?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Question 2&lt;/strong&gt;. What if the experiment had been pre-registered? &lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Question 3&lt;/strong&gt;. Assume again that the experiment was not pre-registered. Now imagine that a pre-registered replication attempt with the same sample size estimated the effect size to be d=0.0. At the time of pre-registration, the original experimenter deemed it “reasonably exact”. Based on this replication and the original experiment, what is your ballpark subjective estimate of the true effect size?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Question 4&lt;/strong&gt;. What if the replication attempt had 300 subjects per cell?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here are the results.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/results.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;Keeping in mind all the caveats about sampling bias and other issues, here are a few observations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The original study reported an effect size of d=0.5, but the results for Question 1 tell us that most researchers believed the true effect size was closer to d=0.2, which is roughly in line with my own estimate. Had I allowed researchers to &lt;a href=&quot;http://csaid.shinyapps.io/survey/&quot;&gt;state their uncertainty&lt;/a&gt;, I suspect that many would find it quite possible that even the &lt;em&gt;sign&lt;/em&gt; of the effect was wrong. This isn’t really surprising to me, but I think we should take a moment to reflect on what this means. When a scientist reports a result, most other researchers believe it is massively overstated. I know that there are still some researchers who want little or no changes to the status quo, but I’d like to live in a world where people actually believe the claims that scientists make. That’s why I’m a strong supporter of all the attempts to &lt;a href=&quot;http://centerforopenscience.org/&quot;&gt;fundamentally&lt;/a&gt; &lt;a href=&quot;https://pubpeer.com/&quot;&gt;change&lt;/a&gt; &lt;a href=&quot;http://www.talyarkoni.org/blog/2013/03/12/the-truth-is-not-optional-five-bad-reasons-and-one-mediocre-one-for-defending-the-status-quo/&quot;&gt;how&lt;/a&gt; scientists do research.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you want people to have more confidence in your findings, pre-registration can make a big difference.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While it’s not apparent from the plot, almost all respondents reduced their effect size estimate upon hearing about failed replications (Question 3 and 4 compared to Question 1).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As some have pointed out, the original experiment falls a bit short of statistical significance. This was an oversight, as I forgot to check the p-value after changing some of the values. I don’t think this is a huge deal, since posterior estimates shouldn’t really depend too much on whether the results cross an arbitrary threshold. But apologies for the error.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;My estimates &lt;a href=&quot;https://twitter.com/Chris_Said/status/487041593422016512&quot;&gt;were&lt;/a&gt; .25, .40, .10, .05.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I wish I included another question asking what people would have thought of the original study if it was conducted in 2014.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Jason Mitchell's essay</title>
   <link href="http://localhost:4000/2014/07/07/jason-mitchells-essay/"/>
   <updated>2014-07-07T11:16:51-07:00</updated>
   <id>http://localhost:4000/2014/07/07/jason-mitchells-essay</id>
   <content type="html">&lt;p&gt;As of yesterday I thought the debate about replication in psychology was converging on consensus in at least one respect. While there was still some disagreement about tone, &lt;a href=&quot;http://www.spspblog.org/simone-schnall-on-her-experience-with-a-registered-replication-project/&quot;&gt;basically&lt;/a&gt; &lt;a href=&quot;http://www.talyarkoni.org/blog/2013/03/12/the-truth-is-not-optional-five-bad-reasons-and-one-mediocre-one-for-defending-the-status-quo/&quot;&gt;everyone&lt;/a&gt; agreed that there was value in failed replications. But then this morning, Jason Mitchell posted &lt;a href=&quot;http://wjh.harvard.edu/~jmitchel/writing/failed_science.htm&quot;&gt;this essay&lt;/a&gt;, in which he describes his belief that failed replication attempts can contain errors and therefore “cannot contribute to a cumulative understanding of scientific phenomena”. It’s hard to know where to begin when someone comes from a worldview so different from one’s own. Since there’s clearly a communication problem here, I’ll just give two examples to illustrate how I think about science.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt;. A rigorous lab conducts an experiment using a measurement device that requires special care. The effect size is d=0.5. Later, a different lab with no experience using the device tries to quickly replicate the experiment and computes an effect size of d=0.0.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;. A small sample experiment in a field with a history of p-hacking shows an effect size of d=0.5. Another lab tries to replicate the study with a much larger sample and computes an effect size of d=0.0.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In both cases, I’d have subjective beliefs about the true effect size. For the first example, my posterior distribution might peak around d=0.4. For the second example, my posterior distribution might peak around d=0.1. In both cases, the replication would influence my posterior, but to varying degrees. In the first example, it would cause a small shift. In the second, it would cause a big shift. Reasonable people can disagree on the exact positions of the posteriors, but basically everyone ought to agree that our posteriors should incrementally adjust as we acquire new information, and that the size of these shifts should depend on a variety of factors, including the possibility of errors in either the original experiment or in the replication attempt. Maybe it’s because I’m stuck in a worldview, but none of this even seems very hard to understand. &lt;/p&gt;

&lt;p&gt;Jason Mitchell &lt;a href=&quot;http://wjh.harvard.edu/~jmitchel/writing/failed_science.htm&quot;&gt;sees&lt;/a&gt; things differently. For him, all failed replications contain “no meaningful evidentiary value” and “do not constitute scientific output”. I don’t doubt the sincerity of his beliefs, but I suspect that most scientists and nonscientists alike will find these assertions to be pretty bizarre. NHST isn’t the only thing causing the crisis in psychology, but it’s pretty clear that this is what happens when people get too immersed in it. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How I use Twitter</title>
   <link href="http://localhost:4000/2014/05/14/how-i-use-twitter/"/>
   <updated>2014-05-14T07:31:13-07:00</updated>
   <id>http://localhost:4000/2014/05/14/how-i-use-twitter</id>
   <content type="html">&lt;p&gt;Next week I’m going to start a new job as a data scientist at Twitter and I am thrilled. Aside from Google search, no other website has had a more positive impact on my life than Twitter. Twitter is just so much fun, and I have learned so much from it. &lt;/p&gt;

&lt;p&gt;Because my experience has been so good, it saddens me to hear that some people don’t really “get” Twitter. Some people who try it feel frustrated and stop using it. Others use it occasionally but don’t really see what all the fuss is about.&lt;/p&gt;

&lt;p&gt;I want to share my approach to using Twitter so that others can try. There are probably other ways to enjoy it, but this approach has worked well for me:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;I don’t necessarily follow my friends, and I don’t expect them to follow me. I use Twitter for a limited set of interests, and not all of my friends tweet about those interests.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I generally don’t follow organizations. They tend to tweet too much and their content is often too promotional.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Instead, I follow opinionated people who tweet about a small set of topics that I’m interested in.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I make sure that my tweet stream is slow enough that I can read every tweet. I do this by limiting the number of people I follow and by making sure that I don’t follow people who tweet too much, even if they have good content.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That’s it. Follow opinionated strangers who tweet about topics you are interested in. Maybe you have a different approach that works well for you. But if you are still trying to figure out the incredible appeal of Twitter, you might want to give my approach a shot.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>High School STEM curriculum wishlist</title>
   <link href="http://localhost:4000/2014/02/22/high-school-stem-curriculum-wishlist/"/>
   <updated>2014-02-22T14:38:59-08:00</updated>
   <id>http://localhost:4000/2014/02/22/high-school-stem-curriculum-wishlist</id>
   <content type="html">&lt;p&gt;If I had a chance to remake the high school STEM curriculum to reflect the skills that are actually needed in today’s world, my changes might look something like this:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/screen-shot-2014-02-23-at-8-36-37-am.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

</content>
 </entry>
 
 <entry>
   <title>Is your job in another state?</title>
   <link href="http://localhost:4000/2014/02/08/is-your-job-in-another-state/"/>
   <updated>2014-02-08T14:49:08-08:00</updated>
   <id>http://localhost:4000/2014/02/08/is-your-job-in-another-state</id>
   <content type="html">&lt;p&gt;National unemployment is high, but business is booming in some states. Vermont needs teachers. Nevada needs bartenders. North Dakota needs truck drivers and just about everything else.&lt;/p&gt;

&lt;p&gt;Despite these opportunities, Americans &lt;a href=&quot;http://www.washingtonmonthly.com/magazine/november_december_2013/features/stay_put_young_man047332.php?page=all&quot;&gt;aren’t moving much&lt;/a&gt; and unemployment remains high.  One reason for this is that moving can be expensive and disruptive, especially for those with families and roots in their communities. But another reason may just be lack of awareness about the opportunities in other states. That’s why I have made a new website: &lt;a href=&quot;http://www.ismyjobinanotherstate.com&quot;&gt;www.ismyjobinanotherstate.com&lt;/a&gt;. Enter your job skills, and the website will provide an interactive map showing where you are most in demand.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ismyjobinanotherstate.com&quot;&gt;&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/securityofficer.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;
&lt;/a&gt;
States are ranked by their &lt;a href=&quot;https://www.fas.org/sgp/crs/misc/R42943.pdf&quot;&gt;ratio of job postings to unemployment&lt;/a&gt;. This is a pretty good metric, but it isn’t perfect. To understand why, imagine two states with the same posting/unemployed ratio for a particular job. If you are trained for the job, you might have better luck applying in a state where the unemployed population is either untrained or unwilling to take that type of job, even though the two states have the same ratio. There also may be differences across states in the use of Indeed.com. Still, I think my results have reasonably good face validity, and the results for many jobs are close to what you would except. If you average across jobs, you get something pretty close to an independently created measure called the &lt;a href=&quot;http://www.washingtonmonthly.com/magazine/november_december_2013/features/the_2013_opportunity_index047357.php&quot;&gt;“Opportunity Index”&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Job posting data was collected using the &lt;a href=&quot;https://ads.indeed.com/jobroll/xmlfeed&quot;&gt;Indeed.com api&lt;/a&gt;. Unemployment numbers came from the &lt;a href=&quot;http://www.bls.gov/news.release/laus.t03.htm&quot;&gt;Bureau of Labor Statistics&lt;/a&gt;. For more information about how this works, see my the &lt;a href=&quot;https://github.com/csaid/IsMyJobInAnotherState&quot;&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>My Insight Data Science Project</title>
   <link href="http://localhost:4000/2013/10/12/my-insight-data-science-project/"/>
   <updated>2013-10-12T13:59:14-07:00</updated>
   <id>http://localhost:4000/2013/10/12/my-insight-data-science-project</id>
   <content type="html">&lt;p&gt;I just finished an excellent fellowship at &lt;a href=&quot;http://insightdatascience.com/&quot;&gt;Insight Data Science&lt;/a&gt;. During our first few weeks there, each of us designed a website to demo at Insight’s sponsor companies. My website is called &lt;a href=&quot;http://www.dealspotter.info&quot;&gt;DealSpotter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This all started earlier this summer when I went to Craigslist to find a used car. There were lots of good deals on Craigslist, but it took way too long to find them. When I searched for a particular model, I got hundreds of hits, but only a few of the hits included the mileage in the posting title. Since I needed the mileage to know whether I was getting a good deal, I had to click on each of the hundreds of listings. Pretty time consuming.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.dealspotter.info&quot;&gt;&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/listings.png&quot; alt=&quot;&quot; border=&quot;2px&quot; /&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A larger problem was that even if I clicked on every post, I didn’t always have a sense for what was the best deal. For example, if I had $3,000, was it better to spend it on a 2001 model with 100K miles, or a 2003 model with 140K miles?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.dealspotter.info&quot;&gt;&lt;em&gt;DealSpotter&lt;/em&gt;&lt;/a&gt; is a proof-of-concept website that shows how these problems could be solved. DealSpotter grabs all the Craigslist car postings in the San Francisco Bay Area and automatically shows you the best deals. It knows how much each car should be priced, based on the model, year, and mileage. Cars that are priced lower than DealSpotter expects them to be are shown at the top of the list. DealSpotter also presents the same information in a visual format called “Graph” mode, where the best deals are highlighted in blue.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.dealspotter.info&quot;&gt;&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/full.png&quot; alt=&quot;&quot; border=&quot;2px&quot; /&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To determine how much each car should be priced, DealSpotter doesn’t use Kelley Blue Book, which tends to overprice cars, especially newer models. Instead, DealSpotter builds its own pricing model based on the actual Craigslist market. In particular, it uses a &lt;a href=&quot;http://en.wikipedia.org/wiki/Random_forest&quot;&gt;Random Forest&lt;/a&gt; pricing model because, unlike smooth parametric models, Random Forests are able to detect sharp discontinuities in prices that may be caused by factors such as manufacturer design overhauls.&lt;/p&gt;

&lt;p&gt;By selecting cars that are priced much lower than would be expected based on year and mileage, DealSpotter picks out some incredible deals, as well as the occasional clunker with an accident history. A more elaborate service might find a way to filter for accident history, but for now DealSpotter remains useful because it greatly narrows down the scope of the search for users. Once users are dealing with a handful of posts, they can easily inspect the text of the ad to determine which cars are good deals, and which have a history of accidents.&lt;/p&gt;

&lt;p&gt;If you are in the San Francisco Bay Area and are looking for a used car, you should definitely check out my website &lt;em&gt;right now&lt;/em&gt;. Many cars are underpriced by thousands of dollars. In the future though, I won’t be updating the listings, which will soon become outdated. Craigslist has a &lt;a href=&quot;http://news.cnet.com/8301-1023_3-57479344-93/craigslist-sues-padmapper-for-mass-harvesting-listings/&quot;&gt;history of suing&lt;/a&gt; other services that try to improve on how their data is presented. Craigslist’s litigiousness is understandable – they curated the data after all. But it apparently has also &lt;a href=&quot;http://bits.blogs.nytimes.com/2012/07/29/when-craigslist-blocks-innovations-disruptions/?_r=0&quot;&gt;stifled innovation&lt;/a&gt;. Craigslist users spend many hours of their time clicking on blue links because the website’s search and UI tools are still stuck in the 90’s. Users are also at higher risk of scams because there is no reputation system. Normally, issues like this would put a company out of business, but a combination of lawsuits and network lock-in effects have kept Craigslist at the top of classifieds services. Hopefully, we will one day get a better Craigslist. In the meantime, if you want to find an incredible deal on a car while the postings are still fresh, you should do so &lt;a href=&quot;http://www.dealspotter.info&quot;&gt;now&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FAQ for Which Famous Economist</title>
   <link href="http://localhost:4000/2013/08/19/which-famous-economist/"/>
   <updated>2013-08-19T11:44:46-07:00</updated>
   <id>http://localhost:4000/2013/08/19/which-famous-economist</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;[Continuously updated. Last update December 14th, 2015]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I made a new webpage: &lt;a href=&quot;http://www.whichfamouseconomistareyoumostsimilarto.com/&quot;&gt;http://www.whichfamouseconomistareyoumostsimilarto.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here’s an FAQ for it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q.&lt;/strong&gt; What is this and why did you make it?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; There is a &lt;a href=&quot;http://www.npr.org/blogs/money/2012/07/19/157047211/six-policies-economists-love-and-politicians-hate&quot;&gt;surprising amount of consensus&lt;/a&gt; among economists on many issues. Progressive consumption taxes and carbon taxes are good. Personal income taxes and corporate taxes are bad. Congestion pricing is good. The mortgage deduction is bad. Marijuana should be legalized. These positions are endorsed by almost every economist, both from the left and the right, but politicians in Washington tend to support the opposite.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://www.igmchicago.org/igm-economic-experts-panel&quot;&gt;IGM Forum&lt;/a&gt; surveys an ideologically diverse group of top economists on these and other issues. I wish more people knew about their website. My new webpage, &lt;a href=&quot;http://www.whichfamouseconomistareyoumostsimilarto.com/&quot;&gt;http://www.whichfamouseconomistareyoumostsimilarto.com&lt;/a&gt;, collects responses from the IGM forum and allows users to compare it to their own responses.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q.&lt;/strong&gt; Why is the economist closest to me on the graph different from the economist who actually is closest to me, according to the text below the graph?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; Each economist can be thought of as a point in a 30 dimensional space, where each dimension corresponds to a question, and unfortunately it’s only possible to display 2 dimensions. While you may appear close to an economist on those 2 dimensions, you may be far apart on the 28 other dimensions that you can’t see.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q.&lt;/strong&gt; I don’t have the expertise to answer some of these questions. Should I leave them blank or should I click “Neutral”?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; You should leave them blank so that they do not enter the calculations. “Neutral” indicates that you have a real opinion somewhere between “Agree” and “Disagree”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q.&lt;/strong&gt; Every question I answer makes me move very far on the graph. This seems unreliable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; Do not take your graph position seriously until you have answered at least 20 questions. Your position will gradually converge as you answer more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q.&lt;/strong&gt; Responses that “strongly deviate from expert consensus” are highlighted in yellow. What does that mean?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; It means that your response deviated more that two standard deviations from the IGM panel average.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q.&lt;/strong&gt; I just answered a question the exact same way as Economist X. But my position on the graph moved &lt;em&gt;away&lt;/em&gt; from him/her. Why?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; This is a natural consequence of projecting multiple dimensions onto two dimensions. To see why, take a cube-shaped object and trace your finger along the edges from one corner to the opposite corner. Viewed from some angles, your finger might sometimes appear to move away from the destination corner.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q.&lt;/strong&gt; What do the two principal components represent?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; It’s hard so say exactly, but the horizontal axis corresponds pretty closely to the left-right political axis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q.&lt;/strong&gt; Why were some IGM panel economists excluded from your webpage?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; Economists who answered less than 75% of the questions were excluded.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q.&lt;/strong&gt; Has there been any academic study on the responses to the IGM poll questions?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A.&lt;/strong&gt; Yes, the responses to the IGM poll have been analyzed in a &lt;a href=&quot;http://faculty.chicagobooth.edu/luigi.zingales/papers/research/Economic-Experts-vs-Average-Americans.pdf&quot;&gt;paper&lt;/a&gt; by Paola Sapienza and Luigi Zingales, and in another &lt;a href=&quot;http://econweb.ucsd.edu/~gdahl/papers/views-among-economists.pdf&quot;&gt;paper&lt;/a&gt; by Roger Gordon and Gordon Dahl. In addition, the polls were &lt;a href=&quot;https://www.aeaweb.org/webcasts/2013/kashyap/NewStandardPlayer.html?plugin=Silverlight&quot;&gt;discussed&lt;/a&gt; during the 2013 American Economic Association annual meeting.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>New model of binocular rivalry</title>
   <link href="http://localhost:4000/2013/03/30/new-model-of-binocular-rivalry/"/>
   <updated>2013-03-30T08:23:03-07:00</updated>
   <id>http://localhost:4000/2013/03/30/new-model-of-binocular-rivalry</id>
   <content type="html">&lt;p&gt;Binocular rivalry is a visual illusion that occurs when the two eyes are presented with incompatible images. Instead of perceiving a mixture of the two images, most people experience alternations in which only one image is visible at a time. Binocular rivalry works best under controlled laboratory conditions with prisms or mirrors, but if you are lucky you might be able to experience it in the figure below. Try crossing your eyes to align the left boxes and right boxes, so that three boxes are observed rather than two. If you can keep your eyes stable, you might perceive alternations between the two different gratings in the middle box. It helps if you first try to merge the “Merge me!” phrase and then, once that it is stable, focus on the middle box. If you can’t stabilize your eyes enough, don’t worry. You are not alone.&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_dichop1.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;Binocular rivalry is more than just an interesting illusion: it reflects actual inhibitory competition between neurons in the brain, and therefore provides a rare window into neural dynamics. To help us understand these mechanisms, researchers have developed several models of the phenomenon. Yet surprisingly, all of these models make a big incorrect prediction about a type of stimulus known as “binocular plaids”. You can view some binocular plaids by crossing your eyes on the boxes below, or simply by looking at one of the boxes normally.&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_binoc.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;As you can see, a plaid is composed of two gratings, a rightward pointing grating and a leftward pointing grating. The big, incorrect prediction made by previous models of rivalry is that the leftward pointing grating should alternate with the rightward pointing grating, just as it would in the traditional rivalry stimuli shown above. This prediction – which follows because the same neural inhibition that creates competition in the first figure must necessarily also create competition in the second figure – is clearly wrong: When viewing the binocular plaid, you probably perceive that the rightward grating remains just as strong as the leftward grating, without any alternations. This failed prediction extends far beyond these toy stimuli. Plaid perception is typically explained by the broad theory of &lt;a href=&quot;http://www.nature.com/nrn/journal/v13/n1/full/nrn3136.html&quot;&gt;divisive normalization&lt;/a&gt;, which also covers a whole host of other inhibitory interactions in cortex. Models of the inhibitory processes in rivalry are thus in tension with models of inhibitory processes that use divisive normalization.&lt;/p&gt;

&lt;p&gt;Together with my advisor &lt;a href=&quot;http://www.cns.nyu.edu/~david/&quot;&gt;David Heeger&lt;/a&gt;, I developed a &lt;a href=&quot;http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002991&quot;&gt;new model&lt;/a&gt; of rivalry that is able to accommodate plaids, and which I hope reconciles models of rivalry with models of normalization. Finding a solution was not as easy as you might think. When we presented the problem to colleagues, everyone immediately had intuitions for how to solve it, but amazingly none of them worked. We found only one solution that worked, and it is one that I later discovered was once proposed by Randolph Blake. The model makes novel predictions that we confirmed with psychophysical tests. If you want to read more about it, you can find the paper below. The Matlab code is available &lt;a href=&quot;http://www.cns.nyu.edu/~csaid/code/SaidAndHeeger_model_code.zip&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002991&quot;&gt;Said CP &amp;amp; Heeger DJ (2013). A model of binocular rivalry and cross-orientation suppression. &lt;em&gt;PLOS Computational Biology.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Canadian funding models for all</title>
   <link href="http://localhost:4000/2013/03/21/165/"/>
   <updated>2013-03-21T06:08:46-07:00</updated>
   <id>http://localhost:4000/2013/03/21/165</id>
   <content type="html">&lt;p&gt;The US and Canada have very different systems for funding science. To compare them, I found some of the publicly available data on &lt;a href=&quot;http://report.nih.gov/success_rates/Success_ByIC.cfm&quot;&gt;NIH R01s&lt;/a&gt; (USA) and &lt;a href=&quot;http://www.nserc-crsng.gc.ca/_doc/Funding-Financement/DGStat2012-SDStat2012_eng.pdf&quot;&gt;NSERC Individual Discovery Grants&lt;/a&gt; (Canada), and plotted them below. Before describing the results, I should say that comparing NIH to NSERC is a bit like comparing apples to oranges, since NSERC is probably closer to the NSF than to the NIH. Nevertheless, the cross-country trends hold up across agencies, and in any case my goal is not to compare countries (as much as I would like to) but to compare funding models.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig2.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;There are two things to notice about the plots. First, the funding rates are clearly higher at NSERC than at NIH. The catch, of course, is that higher rates mean smaller awards. NSERC typically provides $35,000/year, far less than the big awards from NIH. Canadian scientists &lt;a href=&quot;http://oikosjournal.wordpress.com/2011/05/16/should-granting-agencies-fund-projects-or-people/&quot;&gt;love&lt;/a&gt; their system, valuing the stability it provides more than the possibility of large awards. Quality of life issues aside, a separate question is: Does the NSERC system produce better science? Or do the high success rates waste too much money on low-quality projects? My feeling is that the NSERC system is much better. High-quality NIH proposals are routinely rejected for arbitrary reasons, and the sink-or-swim culture is &lt;a href=&quot;http://www.nytimes.com/2012/04/17/science/rise-in-scientific-journal-retractions-prompts-calls-for-reform.html?pagewanted=1&amp;amp;_r=2&quot;&gt;directly contributing&lt;/a&gt; to bad research practices. We should move toward a higher rate / smaller award system. And for those who see value in large awards, we can still adjust the size of the award based on the quality of the proposal.&lt;/p&gt;

&lt;p&gt;The second thing to notice about the plots is the trends over time. At NIH, more so than at NSERC, the decline in success rates is driven by an increase in the number of applicants, not by a decrease in the number of awards. I don’t think the solution is just “more funding”, especially in the current fiscal climate. We have a denominator problem, not a numerator problem. We should fix the system that rewards programs for producing more PhDs than the system can accommodate. I’ll leave it to actual experts to decide how to do this. But as with most public policy questions, a good place to start is to just copy whatever the Canadians are doing.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Eight Lessons from the Reproducibility Crisis </title>
   <link href="http://localhost:4000/2013/01/15/8-lessons-from-the-reproducibility-crisis/"/>
   <updated>2013-01-15T16:04:46-08:00</updated>
   <id>http://localhost:4000/2013/01/15/8-lessons-from-the-reproducibility-crisis</id>
   <content type="html">&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;There is a reproducibility crisis in psychology.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Outright fraud is rare. Soft forms of bad practice are the bigger problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most scientists are honest, but soft forms of bad practice emerge through self-deception or lack of awareness. &lt;a href=&quot;http://psr.sagepub.com/content/2/3/196.abstract&quot;&gt;
&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The problem is worse in medical research, but that is no excuse for psychologists to resist reforms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1850704&quot;&gt;Lists of new regulations&lt;/a&gt; are fine, but the core issue is that career incentive structures are not always aligned with truth discovery.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some data outcomes are rewarded more than other data outcomes. This is bad.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Journals have little incentive to change this incentive structure themselves.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2012/04/17/its-the-incentives-structure-people-why-science-reform-must-come-from-the-granting-agencies/&quot;&gt;But granting agencies can help&lt;/a&gt;, by increasing the grant award probability to scientists who submit to &lt;a href=&quot;http://neurochambers.blogspot.co.uk/2012/10/changing-culture-of-scientific.html&quot;&gt;good&lt;/a&gt; &lt;a href=&quot;http://talyarkoni.org/papers/Yarkoni_FCN_2012.pdf&quot;&gt;practice&lt;/a&gt; &lt;a href=&quot;http://www.plosone.org&quot;&gt;journals&lt;/a&gt;. Can someone at NIH/NSF please do something about this?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you have comments, they might already be addressed in my &lt;a href=&quot;/2012/04/18/faq/&quot;&gt;FAQ&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>In defense of correlation/causation blowhards</title>
   <link href="http://localhost:4000/2013/01/09/in-defense-of-correlationcausation-blowhards/"/>
   <updated>2013-01-09T07:19:10-08:00</updated>
   <id>http://localhost:4000/2013/01/09/in-defense-of-correlationcausation-blowhards</id>
   <content type="html">&lt;p&gt;Let’s get one thing out of the way first: There is not a single scientist or science journalist who doesn’t know that correlation does not equal causation. Most have probably known it since high school.&lt;/p&gt;

&lt;p&gt;That’s why there has been a bit of a backlash against internet commenters who keep pointing it out. The phrase is “common and irritating”, writes Slate’s Daniel Engber in his article &lt;a href=&quot;http://www.slate.com/articles/health_and_science/science/2012/10/correlation_does_not_imply_causation_how_the_internet_fell_in_love_with_a_stats_class_clich_.single.html&quot;&gt;The Internet Blowhard’s Favorite Phrase&lt;/a&gt;. To Engber, correlation≠causation is a “freshman platitude” that professional scientists and journalists don’t need to be reminded of. Scientists are merely &lt;em&gt;suggesting&lt;/em&gt; a causal relationship. They are not claiming it is proven.&lt;/p&gt;

&lt;p&gt;I can see why writers and researchers find it frustrating to be scolded about something they already know. But correlation≠causation is one of those things that has a way of sliding onto the back burner of one’s mental awareness, even among scientists. On one day a research group is acknowledging the limits to their correlational study, but the next day they are advancing policy arguments that depend on a causal relationship. Or more commonly, they are preparing to run yet another correlational study.&lt;/p&gt;

&lt;p&gt;Nowhere does this seem more of an issue than in nutrition science and in education research. In nutrition science, it is much easier to conduct a simple survey on health and eating habits than to organize a large-scale longitudinal randomized control study. Is it any wonder then, that after 50 years of nutrition science &lt;a href=&quot;http://grist.org/scary-food/2011-03-04-low-fat-diet-fad/&quot;&gt;we still don’t know&lt;/a&gt; whether saturated fat is good for you or bad for you? And in education research, it is much easier to run a correlational study on class size and achievement than it is to run a &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/1468-0297.00586/abstract&quot;&gt;randomized control study&lt;/a&gt;. Is it any wonder then, that the public policy debate about class size is so muddled?&lt;/p&gt;

&lt;p&gt;Don’t get me wrong – There is some fantastic causal research coming out of the nutrition and education fields. And there is nothing wrong with running a correlational study either. Correlational studies are a great way for researchers to identify variables that are promising enough to investigate with causal methods.&lt;/p&gt;

&lt;p&gt;But could it be that we have struck the wrong balance between correlational studies and causal studies? Could we be allocating too many resources towards easy but inconclusive studies, and not enough towards costly but more definitive research? I think the answer might be yes, and that internet comment blowhards are an important voice for this point of view.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>New paper on autism</title>
   <link href="http://localhost:4000/2012/12/29/new-paper-on-autism/"/>
   <updated>2012-12-29T14:20:55-08:00</updated>
   <id>http://localhost:4000/2012/12/29/new-paper-on-autism</id>
   <content type="html">&lt;p&gt;If there is one thing I have learned recently, it is that autism is a really, really complicated disorder. Autism is best known for causing repetitive behaviors and problems with social communication, but it is also known to cause issues in sensory perception. Many hypotheses for the underlying neurophysiological basis have been proposed. Among these is the excitation/inhibition (E/I) imbalance hypothesis, which states that levels of cortical excitation and inhibition are disrupted in autism. An imbalance like this could be caused by unusual levels of certain neurotransmitters, such as glutamate and GABA, or by an unusual distribution of synaptic connections. Together with my collaborators (David Heeger, Marlene Behrmann, Nancy Minshew, and Ryan Egan), we tested this theory and report the results in a new &lt;a href=&quot;http://dx.doi.org/10.1016/j.visres.2012.11.002&quot;&gt;paper&lt;/a&gt; published in &lt;em&gt;Vision Research&lt;/em&gt;.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/network1.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;We chose to test the theory in the visual system because vision is one of the better understood systems in neuroscience and because the E/I imbalance theory has been proposed to explain hypersensitivity to sensory stimuli in autism. Specifically, we conducted two experiments on &lt;a href=&quot;http://en.wikipedia.org/wiki/Binocular_rivalry&quot;&gt;binocular rivalry&lt;/a&gt;, a well-studied phenomenon that depends critically on excitation and inhibition levels in cortex. Using a very simple computational model (a schematic is shown above), we made predictions about how imbalances in excitation and inhibition would affect perception during rivalry. Contrary to our expectations, we found no significant differences between autistic individuals and controls, and no evidence for a relationship between these measurements and the severity of autism. Of course, these results do not conclusively rule out an E/I imbalance in the visual system of those with autism. There are many alternative explanations that we describe in the Discussion section. But these results do seem to suggest that an E/I imbalance, if it exists, is likely to be small in magnitude.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://dx.doi.org/10.1016/j.visres.2012.11.002&quot;&gt;http://dx.doi.org/10.1016/j.visres.2012.11.002&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Why are contrast response functions linear in fMRI and nonmonotonic in EEG?</title>
   <link href="http://localhost:4000/2012/10/04/why-are-contrast-response-functions-linear-in-fmri-and-nonmonotonic-in-eeg/"/>
   <updated>2012-10-04T08:43:41-07:00</updated>
   <id>http://localhost:4000/2012/10/04/why-are-contrast-response-functions-linear-in-fmri-and-nonmonotonic-in-eeg</id>
   <content type="html">&lt;p&gt;Contrast response functions (CRFs) describe how a neuron’s firing rate depends on the contrast, or intensity, of a visual stimulus. CRFs are really important for testing theories about how the visual system works, and I’ve spent a lot of time over the past few years trying to indirectly measure them in humans, using EEG and fMRI. The problem, however, is that EEG and fMRI give me very different CRFs.&lt;/p&gt;

&lt;p&gt;Typically, individual neurons show sigmoidal CRFs. They don’t fire at all for very low contrasts, but they rapidly increase their firing rate at a middle range of contrast. The midpoint of this range is sometimes called the ‘semisaturation constant’. After that, the firing rate tends to level off, or ‘saturate’.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_intro_neuron2.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;With fMRI, however, I tend to find that the CRFs are mostly linear, not sigmoidal.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_intro_fmri1.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;And with EEG, I often find that the CRFs are actually nonmonotonic!&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_intro_eeg1.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;This is weird, and I’m not the first person to notice it. Linear CRFs in fMRI are quite common (e.g. &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/9535979&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/22153378&quot;&gt;here&lt;/a&gt;), as are nonmonotic CRFs in EEG (e.g. &lt;a href=&quot;http://www.nature.com/nature/journal/v321/n6067/abs/321235a0.html&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://www.jneurosci.org/content/21/12/4530.short&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1192067/&quot;&gt;here&lt;/a&gt;,  and &lt;a href=&quot;http://www.jneurosci.org/content/32/8/2783.short&quot;&gt;here&lt;/a&gt; ). While not all papers show these effects, they certainly seem like real trends. How it is possible that an underlying sigmoidal function in neurons could give rise to such completely different functions measured by fMRI and EEG?&lt;/p&gt;

&lt;p&gt;To explain the linear fMRI responses, one idea is that fMRI takes an average over many different neurons, each with a different semisaturation constant. Some neurons saturate early, some saturate late, and when you average them all together you just get a line.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_fmri1.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;That seems to make sense. But what about the nonmonotonic CRFs in EEG? One idea is that EEG CRFs might reflect the true contrast response functions of individual neurons, many of which are themselves &lt;a href=&quot;http://ww.w.journalofvision.org/content/7/6/13.short&quot;&gt;nonmonotonic&lt;/a&gt;. While I don’t doubt that this could be a contributing factor, I have trouble believing that this explains all, or even most, of the nonmonotonicity in EEG. Only a small minority of visual cells exhibit this property, and I have seen some _massive _nonmonotonicity in some of my most reliable EEG subjects.&lt;/p&gt;

&lt;p&gt;A different hypothesis is that nonmonotonicity is caused by the cancellation of dipoles across cortical sulci or gyri. EEG dipoles are oriented normal to the cortical surface and, since the cortex has so many folds, many of the electrical signals simply &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/19639553&quot;&gt;cancel&lt;/a&gt; each other out.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_dipole1.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;Here’s where it gets interesting. Different cortical areas (e.g. V1 and V2) have different average semisaturation constants. (They tend to get lower the higher you move up in the visual hierarchy). Imagine that two cortical areas with different semisaturation constants live on opposite sides of a sulcus. At medium contrasts, one area will be active and the other will be mostly silent. But at higher contrasts, &lt;em&gt;both&lt;/em&gt; areas will have kicked in, and so the overall signal (due to cancellation) will be actually _lower _that at medium contrast. This isn’t my idea, but it makes a lot of sense and I think it deserves more attention.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_eeg2.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;But wait, the fMRI explanation makes sense, and the EEG explanation makes sense, but how can they both be true? After all, the EEG cancellation story only works if each area’s average CRF is nonlinear: If the average responses were linear (as demonstrated by the fMRI explanation), cancellation of opposite CRFs would just result in more straight lines, right?&lt;/p&gt;

&lt;p&gt;Well, yes and no. Using simulations I have found it quite easy to capture both effects. In the fMRI simulation, a perfectly linear CRF emerges only if I assume that the underlying neural semisaturation constants are uniformly distributed, which is quite an unlikely assumption. With any other reasonable distribution (e.g. normal distribution) I get fMRI CRFs that are mostly linear but with a touch of sigmoid. And critically, that touch of sigmoid will drive the dipole cancellation in a way that results in nonmonotonic EEG CRFs.&lt;/p&gt;

&lt;p&gt;Below is some Matlab code showing how this works, just a proof of concept. It makes some very, very simplifying assumptions, and I’m sure that the truth is far more complicated.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;crfmodel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%number of neurons&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%contrast levels&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c50_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%stddev of semisaturation constants&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c50_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;70&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%for two visual areas;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c50&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%loop through two banks of sulcus (two visual areas)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;idcs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;c50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idcs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c50_std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c50_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;meshgrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;makeLogistic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%Each row is a CRF for a particular neuron&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;figure&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;%Single neurons&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%loop through 2 areas&lt;/span&gt;
   &lt;span class=&quot;nb&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makeLogistic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c50_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)));&lt;/span&gt;
   &lt;span class=&quot;nb&quot;&gt;hold&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Single Neuron Response'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;%fMRI&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%loop through 2 areas&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;CRF_fMRI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idcs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},:),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%average all neurons in each area&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CRF_fMRI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;hold&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'fMRI Response'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;%EEG&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%Cancellation. Scale factor isn't necessary but reflects that fact that&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%some areas are likely to activate more than others.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;R_EEG&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idcs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},:)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idcs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},:));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;CRF_EEG&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R_EEG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CRF_EEG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'EEG Response'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;makeLogistic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))));&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And here are the results.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/fig_all2.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;It would be nice if we could go in the opposite direction and reconstruct underlying neural responses from the fMRI and EEG measurements. This seems like a pretty tricky inverse problem, but it might be possible with accurate assumptions about current propagation and the distribution of semisaturation constants.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>National labs for all the sciences</title>
   <link href="http://localhost:4000/2012/05/29/national-labs-for-all-the-sciences/"/>
   <updated>2012-05-29T08:26:00-07:00</updated>
   <id>http://localhost:4000/2012/05/29/national-labs-for-all-the-sciences</id>
   <content type="html">&lt;p&gt;Harvard, MIT, Stanford, and several other elite universities have all recently &lt;a href=&quot;http://www.nytimes.com/2012/05/03/education/harvard-and-mit-team-up-to-offer-free-online-courses.html&quot;&gt;announced&lt;/a&gt; that they will be be offering free online courses. The courses will be massively open, taught by star professors, and supplemented with video lessons, embedded testing, and realtime feedback. This is surely good news for students who might not be able to access these resources otherwise, and it is an overall positive development for education. But what are the implications for scientists who conduct research in universities? And how will these developments affect the progress of scientific research?&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;&lt;img src=&quot;/assets/youregonnahaveabadtime.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;Unless we correctly anticipate the changes, the implications for science might not be good, at least through the medium term. When elite universities offer classes for free, other universities will have difficulty convincing new students to enroll in their own traditional programs. Why sit through an expensive 9AM lecture at your local university when you could get the Harvard lecture for free, whenever you want? Yes, yes, students benefit from being physically present at a university: interactions with professors and other students are often irreplaceable. And yes, many universities will manage to survive in their traditional form. But as a matter of degree the future trends are clear: Online alternatives will become better, and in many cases they will offer real diplomas. As this happens, demand for traditional education at second-tier universities will decline, tuition revenue will dry up, and many excellent scientists could be laid off. In some cases, entire universities may shut down, just as many local newspapers have &lt;a href=&quot;http://newspaperdeathwatch.com/&quot;&gt;succumbed&lt;/a&gt; to competition from online journalism.&lt;/p&gt;

&lt;p&gt;With so many good scientists out of work, the progress of science will slow down. While scientists who work on applied research may find employment in private industry, those who do basic research will not fare as well. Nonprofit research institutes may emerge as places for basic research, but donor-based funding is never guaranteed. Moreover, these nonprofits might only begin operations after a painful transition period.&lt;/p&gt;

&lt;p&gt;This is where government can step in, anticipating the problem and preparing research institutes where scientists can work outside of a university setting. For the biological sciences, let’s start building more &lt;a href=&quot;http://irp.nih.gov/about-us/research-campus-locations&quot;&gt;regional NIH campuses&lt;/a&gt; throughout America. In the physical sciences, let’s expand the &lt;a href=&quot;http://en.wikipedia.org/wiki/United_States_Department_of_Energy_National_Laboratories&quot;&gt;national labs&lt;/a&gt; system. And for all the other sciences, let’s call for regional NSF campuses. Ideally, these initiatives will be paid for by increases in top-line budgets for science agencies. Or, if that is not an option, we could use the money saved in extramural research budgets, which will surely be trimmed as university scientists are laid off. Whatever the details, the larger scientific community must start preparing now for the coming &lt;a href=&quot;http://www.nytimes.com/2012/05/04/opinion/brooks-the-campus-tsunami.html?partner=rssnyt&amp;amp;emc=rss&quot;&gt;tsunami&lt;/a&gt; of online education, before it’s too late.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>FAQ</title>
   <link href="http://localhost:4000/2012/04/18/faq/"/>
   <updated>2012-04-18T10:29:46-07:00</updated>
   <id>http://localhost:4000/2012/04/18/faq</id>
   <content type="html">&lt;p&gt;&lt;em&gt;This is a continuously updated list of responses to questions I get about the &lt;a href=&quot;/2012/04/17/its-the-incentives-structure-people-why-science-reform-must-come-from-the-granting-agencies/&quot;&gt;main post&lt;/a&gt; and &lt;a href=&quot;/2013/01/16/8-lessons-from-the-reproducibility-crisis/&quot;&gt;Top 8 List&lt;/a&gt;, which should be read first.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; To solve the replication crisis, scientists need to do X.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; There’s a good chance ‘X’ is a &lt;a href=&quot;http://en.wikipedia.org/wiki/Collective_action%23Collective_action_problem&quot;&gt;collective action problem&lt;/a&gt;. If all scientists did it, the field as a whole would benefit. But if a single scientist did it alone, he or she would not benefit. These problems can only be fixed if an outside force (e.g. the NIH) adjusts the incentives so that individuals would benefit from doing the action alone.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Journals need to do X.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Again, this is probably a collective action problem. See the response above. The NIH can’t tell journals what to do, but it can reward scientists who submit to good-practice journals. This will encourage other journals to change their behavior.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What about citation count metrics? Aren’t they biased towards surprising and interesting results?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Citation count metrics should not be used as a factor in grant decisions and should be replaced by one of the quality-based metrics proposed by &lt;a href=&quot;http://futureofscipub.wordpress.com/2009/11/12/open-post-publication-peer-review/&quot;&gt;Niko Kriegeskorte&lt;/a&gt; or &lt;a href=&quot;http://talyarkoni.org/papers/Yarkoni_open_evaluation_03132012.pdf&quot;&gt;Tal Yarkoni&lt;/a&gt;, or some of the other metrics proposed in the &lt;a href=&quot;http://www.frontiersin.org/Journal/SpecialTopicDetail.aspx?name=computational_neuroscience&amp;amp;st=137&amp;amp;sname=Beyond_open_access_visions_for&quot;&gt;special issue&lt;/a&gt; of Frontiers. My only addition to Niko’s proposed metrics is that I would emphasize importance of the research &lt;em&gt;question&lt;/em&gt;, rather than importance of the &lt;em&gt;outcome.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: &lt;/strong&gt;Couldn’t quality-based metrics or other aspects of your proposal be gameable, just like the current system?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;Yes. All systems are gameable, but some systems are better than others. A more “outcome-unbiased” system will be far better than the current one, which is dysfunctional.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Won’t it be hard for granting agencies to determine whether a journal’s incentive structure encourages good practices?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Government agencies make qualitative  judgments all the time. Even a rough first-order approximation would have a huge positive effect on research quality. The status quo is dysfunctional. Moreover, some journals that specialize in simple experiments (e.g. clinical trials) might demonstrate that they are outcome-unbiased by adopting an &lt;a href=&quot;http://www.overcomingbias.com/2010/11/results-blind-peer-review.html&quot;&gt;outcome-&lt;/a&gt;_&lt;a href=&quot;http://www.overcomingbias.com/2010/11/results-blind-peer-review.html&quot;&gt;blind&lt;/a&gt; _review system, as Robin Hanson has proposed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What about post-publication review?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; In the current system, the only signal of a paper’s quality is the journal’s impact factor. Readers need more information than this. I am generally supportive of post-publication review and would recommend reading the proposals of &lt;a href=&quot;http://futureofscipub.wordpress.com/&quot;&gt;Niko Kriegeskorte&lt;/a&gt; and some of the proposals in the &lt;a href=&quot;http://www.frontiersin.org/Journal/SpecialTopicDetail.aspx?name=computational_neuroscience&amp;amp;st=137&amp;amp;sname=Beyond_open_access_visions_for&quot;&gt;special issue&lt;/a&gt; of Frontiers. Still, I wonder: Will any of these ideas actually be put into practice? Or will scientists just continue to talk about them as they have since the 1970s? It seem like a classic &lt;a href=&quot;http://en.wikipedia.org/wiki/Collective_action%23Collective_action_problem&quot;&gt;collective action problem&lt;/a&gt;. Granting agencies may be needed to provide a nudge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Null results can easily obtained with sloppy research. Won’t outcome-unbiased journals encourage sloppy research?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, it is admittedly a complicated issue. We need to strike a balance between being completely outcome-unbiased on the one hand, and valuing significant results on the other hand. At the moment, the wrong balance has been struck. Null results are disincentivized far too much.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q: &lt;/strong&gt;Isn’t it good to do exploratory analysis?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;Absolutely, but only if it identified as such. &lt;a href=&quot;http://psr.sagepub.com/content/2/3/196.abstract&quot;&gt;HARKing&lt;/a&gt; is misleading, and inflates Type I error.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>It’s the incentive structure, people! Why science reform must come from the granting agencies.</title>
   <link href="http://localhost:4000/2012/04/17/its-the-incentives-structure-people-why-science-reform-must-come-from-the-granting-agencies/"/>
   <updated>2012-04-17T13:35:49-07:00</updated>
   <id>http://localhost:4000/2012/04/17/its-the-incentives-structure-people-why-science-reform-must-come-from-the-granting-agencies</id>
   <content type="html">&lt;p&gt;Another day, another &lt;em&gt;New York Times&lt;/em&gt; &lt;a href=&quot;http://www.nytimes.com/2012/04/17/science/rise-in-scientific-journal-retractions-prompts-calls-for-reform.html?pagewanted=1&amp;amp;_r=2&quot;&gt;report&lt;/a&gt; on bad practice in biomedical science. The growing problems with scientific research are by now well known: Many results in the top journals are cherry picked, methodological weaknesses and other important caveats are often swept under the rug, and a large fraction of findings cannot be replicated. In some rare cases, there is even outright fraud. This waste of resources is unfair to the general public that pays for most of the research.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Times&lt;/em&gt; article places the blame for this trend on the sharp competition for grant money and on the increasing pressure to publish in high impact journals. While both of these factors certainly play contributing roles, the &lt;em&gt;Times&lt;/em&gt; article misses the root cause of the problem. The cause is not simply that the competition is too steep. The cause is that the competition is shaped to point scientists in the wrong direction.&lt;/p&gt;

&lt;p&gt;As many other observers have already noted, scientific journals favor surprising, interesting, and statistically significant experimental results. When journal editors give preferences to these types of results, it is not surprising that more false positives will be published by simple &lt;a href=&quot;http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124&quot;&gt;selection effects&lt;/a&gt;, and sadly it is not surprising that unscrupulous scientists will manipulate their data to show these types of results. These manipulations include selection from &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/3661589&quot;&gt;multiple&lt;/a&gt; &lt;a href=&quot;http://people.psych.cornell.edu/~jec7/pcd%20pubs/simmonsetal11.pdf&quot;&gt;analyses&lt;/a&gt;, selection from &lt;a href=&quot;http://www.talyarkoni.org/blog/tag/file-drawer-problem/&quot;&gt;multiple experiments&lt;/a&gt; (the “file drawer” problem), and the formulation of ‘a priori’ hypotheses after the results are known. While the vast majority of scientists are honest individuals, these biases still emerge in subtle and often subconscious ways.&lt;/p&gt;

&lt;p&gt;Scientists have known about these problems for decades, and there have been several well-intentioned efforts to fix them. &lt;a href=&quot;http://www.jasnh.com/&quot;&gt;The Journal of Articles in Support of the Null Hypothesis&lt;/a&gt; (JASNH) is specifically dedicated to null results. &lt;a href=&quot;http://psychfiledrawer.org/&quot;&gt;The Psych File Drawer&lt;/a&gt; is a nicely designed online archive for failed replications. &lt;a href=&quot;http://www.plosone.org&quot;&gt;PLoS ONE&lt;/a&gt; publishes papers based on the quality of the methods, and allows post-publication commenting so that readers may be alerted about study flaws. Finally, Simmons and colleagues (2011) have proposed &lt;a href=&quot;http://people.psych.cornell.edu/~jec7/pcd%20pubs/simmonsetal11.pdf&quot;&gt;lists of regulations&lt;/a&gt; for other journals to enforce, including minimum sample sizes and requirements for the disclosure of all variables and analyses.&lt;/p&gt;

&lt;p&gt;As well-intentioned as these important (and necessary) initiatives may be, they have all failed to catch on. JANSH publishes a &lt;a href=&quot;http://www.jasnh.com/&quot;&gt;handful&lt;/a&gt; of papers a year, The Psych File Drawer only has &lt;a href=&quot;http://psychfiledrawer.org/view_article_list.php&quot;&gt;nine&lt;/a&gt; submissions, and &lt;a href=&quot;http://www.plosone.org/article/browse.action?field=date&amp;amp;day=1&quot;&gt;hardly anyone&lt;/a&gt; comments on PLoS ONE papers. To my knowledge, no journals have begun enforcing the &lt;a href=&quot;http://people.psych.cornell.edu/~jec7/pcd%20pubs/simmonsetal11.pdf&quot;&gt;lists of regulations&lt;/a&gt; proposed by Simmons et al.&lt;/p&gt;

&lt;p&gt;What is most frustrating is that all of these outcomes were completely predictable. As any economist will tell you, it’s the incentive structure, people! The reason nobody publishes in JASNH is that the rewards for publishing in high-impact journals are larger. The reason nobody puts their failed replications on Psych File Drawer or comments on PLoS ONE is that online archive posts can’t be put on CVs. And the reason individual journals don’t tighten their standards is that scientists can just submit their papers elsewhere. Even if the journals did manage to impose the regulations, wouldn’t it be better if the career incentives of scientists were aligned with the interests of good science? Wouldn’t a more sensible incentive structure make the list of regulations unnecessary?&lt;/p&gt;

&lt;p&gt;This is where the funding agencies need to come in. Or, more to the point, where we as scientists need to ask the funding agencies to come in. Granting agencies should reward scientists who publish in journals that have acceptance criteria that are aligned with good science. In particular, the agencies should favor journals that devote special sections to replications, including failures to replicate. More directly, the agencies should devote more grant money to submissions that specifically propose replications. And finally, I would like to see some preference given to fully “outcome-unbiased” journals that make decisions based on the quality of the experimental design and the importance of the scientific &lt;em&gt;question&lt;/em&gt;, not the &lt;em&gt;outcome&lt;/em&gt; of the experiment. This type of policy naturally eliminates the temptation to manipulate data towards desired outcomes.&lt;/p&gt;

&lt;p&gt;The mechanism could start with granting agencies making modest adjustments to grant scores for scientists who submit to good-practice journals. Over time, as scientists compete to submit to these journals, more of these journals will emerge by market forces. Journals that currently encourage bad practices may adjust their policies if they wish. Under the current system, there is simply no incentive for journals to adjust their policies. Will this transition be easy? No. Will the granting agencies manage this perfectly? Probably not. But it is obvious to me that scientists alone cannot solve problem of publication bias, and that a push from the outside is needed. The proposed system may not be perfect, but it will be vastly better than the dysfunctional system we are working in now.&lt;/p&gt;

&lt;p&gt;If you agree that the cause of bad science is a perverse incentive structure, and if you agree that reform attempts can only work if there is pressure from granting agencies, please pass this article around and contact your funding agency. Within each agency, reform will require coordination among several sub-agencies, so it might make most sense to contact the director. Also, please see the &lt;a href=&quot;/2012/04/18/faq/&quot;&gt;FAQ&lt;/a&gt;, above, for continuously updated answers to questions.&lt;/p&gt;
</content>
 </entry>
 

</feed>
