<!DOCTYPE html>
<html lang="en-us">

<!--
It's bad to import d3 in every post separately (https://groups.google.com/forum/#!topic/d3-js/bwdNirt2uEU).
Importing it globally here.
Putting this up at the top, according to this controversial stack overflow answer
http://stackoverflow.com/questions/7169370/d3-js-and-document-onready
 -->
<script src="https://d3js.org/d3.v5.min.js"></script>

<link rel="stylesheet" href="/public/font-awesome-4.4.0/css/font-awesome.min.css">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
    <meta name="description" content="A surprisingly simple formula for optimizing your experiment">
  

  <!-- To get a link preview image, just set the image attribute in your _post -->
  
    <meta content="http://localhost:4000/assets/2020_optimizing_sample_sizes/product_matrix.png" property="og:image"/>
  

  <!-- twitter standard -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@chris_said" />
  <meta name="twitter:title" content="Optimizing sample sizes in A/B testing, Part II&#58; Aggregate time-discounted lift" />
  <meta name="twitter:description" content="A surprisingly simple formula for optimizing your experiment" />


  <title>
    
      Optimizing sample sizes in A/B testing, Part II&#58; Aggregate time-discounted lift &middot; The File Drawer
    
  </title>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-67746868-1', 'auto');
    ga('send', 'pageview');
  </script>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

<!--   <body class="theme-base-cps">
 -->
  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          The File Drawer
        </a>
      </h1>
      <p class="lead">My name is Chris Said and I am a data scientist at Opendoor. This blog is mostly about tech, stats, and science.</p>

    </div>

    <nav class="sidebar-nav">

      
        <a class="sidebar-nav-item" href="/">Home</a>
      
        <a class="sidebar-nav-item" href="/archive">Archive</a>
      
        <a class="sidebar-nav-item" href="/atom.xml">Feed</a>
      

    </nav>

    <div class="wrapper">
      <div class="inner">
        <a href = "http://www.twitter.com/Chris_Said" class="contact-button"><i class="fa fa-twitter fa-2x"></i></a>
        <a href = "https://www.linkedin.com/pub/chris-said/6b/86b/979" class="contact-button"><i class="fa fa-linkedin-square fa-2x"></i></a>
        <a href = "mailto:chris.said@gmail.com" class="contact-button"><i class="fa fa-envelope fa-2x"></i></a>
      </div>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Optimizing sample sizes in A/B testing, Part II&#58; Aggregate time-discounted lift</h1>
  <span class="post-date">01 Jan 2020</span>
  <p>This is Part II of a two-part blog post on how to optimize your sample size in A/B testing. As in <a href="/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-I/">Part I</a>, the focus will be on choosing a sample size at the beginning of the experiment and committing to it, not on dynamically updating the sample size as the experiment proceeds.</p>

<p>In <a href="/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-I/">Part I</a>, we learned how before the experiment starts we can estimate $\hat{L}$, the expected post-experiment lift, a probability weighted average of outcomes.</p>

<p>In Part II, we’ll discuss how to estimate what is perhaps the most important per-unit cost of epxerimentation: the forfeited benefits that could come from shipping the winning bucket earlier. This leads to something I think is incredibly cool: A formula for $\hat{L}_a$, the aggregate time-discounted post-experiment lift, as a function of sample size. The formula allows you to pick optimal sample sizes specific to your business circumstances, and leads us to three lessons for practioners.</p>

<ol>
  <li>You should run “underpowered” experiments if you have a very high discount rate.</li>
  <li>You should run “underpowered” experiments if you have a small user base.</li>
  <li>That said, it’s far better to run your experiment too long than too short.</li>
</ol>

<h2 id="a-quick-modification-from-part-i">A quick modification from Part I</h2>

<p>In <a href="/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-I/">Part I</a>, we saw that if you run an experiment comparing the current version of your product (A) to a new alternative (B), and if you ship whichever version does best in the experiment, your business will on average experience a post-experiment per-user lift of</p>

<script type="math/tex; mode=display">\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{n})}}</script>

<p>where $\sigma_\Delta^2$ is the variance on your normally distributed zero-mean prior for $\mu_B - \mu_A$, $\sigma_X^2$ is the within-group variance, and $n$ is the per-bucket sample size.</p>

<p>Because Part II is primarily concerned with the the duration of the experiment, we’re going to modify the formula to be time-dependent. As a simplifying assumption we’re going to make <em>sessions</em>, rather then <em>users</em>, the unit of analysis. We’ll also assume that you have a constant number of sessions per day. This changes the definition of $\hat{L}$ to a <em>post-experiment per-session lift</em>, and the formula becomes</p>

<script type="math/tex; mode=display">\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{mt})}}</script>

<p>where $m$ is the sessions per day for each bucket, and $t$ is duration of the experiment in days.</p>

<h2 id="time-discounting">Time Discounting</h2>

<p>The formula above shows that larger sample sizes result in higher $ \hat{L} $, since larger samples make it more likely you will ship the better version. But as with all things in life, there are costs to increasing your sample size. In particular, the larger your sample size, the longer you have to wait to ship the winning bucket. In a fast moving startup, there’s often good reason to accrue your wins as soon as possible. Lift today is much more valuable than the same lift a year from now.</p>

<p>How much more valuable is lift today versus lift a year from now? A common way to quantify this is with exponential discounting, such that weights (or “discount factors”) on future lift follow the form:</p>

<script type="math/tex; mode=display">w = e^{-rt}</script>

<p>where $ r $ is a discount rate. For teams shipping product at startups, the annual discount rate might be quite large, like 0.5 or even 1.0, which would correspond to a daily discount rate $r$ of 0.5/365 or 1.0/365, respectively.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/discount_function.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 1</strong>.
  </div>
</div>
<p><br /></p>

<h2 id="aggregate-time-discounted-lift-visual-intuition">Aggregate time-discounted lift: Visual Intuition</h2>

<p>Take a look at Figure 2, below. It shows an experiment that is planned to run for $\tau = 60$ days. The top panel shows $\hat{L}$. While the experiment is running, $\hat{L} = 0$, since our prior is that $\Delta$ is sampled from a normal distribution with mean zero. But once the experiment finishes and we launch the winning bucket, we should begin to reap our expected per-session lift.</p>

<p>The middle panel shows our discount function.</p>

<p>The bottom panel shows our time-discounted lift, defined as the product of the lift in the top panel and the time discount in the middle panel. (We can also multiply it by $M$, the number of post-experiment sessions per day, which for simplicity we set to 1 here.) The aggregate time-discounted lift, $\hat{L}_a$, is the area under the curve.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/discounted_lift_static.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 2</strong>.
  </div>
</div>
<p><br /></p>

<p>Now let’s see what happens with different experiment durations. Figure 3 shows that the longer you plan to run your experiment, the higher $\hat{L}$ will be (top panel). But due to time discounting, (middle panel), the area under the time-discounted lift curve (bottom panel) is low for overly large sample sizes. There is an optimal duration of the experiment (in this case, $\tau = 24$ days), that maximizes $\hat{L}_a$, the area under the curve.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/discounted_lift_dynamic.gif" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 3</strong>.
  </div>
</div>
<p><br /></p>

<h2 id="aggregate-time-discounted-lift-formula">Aggregate time-discounted lift: Formula</h2>
<p>The aggregate time-discounted lift $\hat{L}_a$, i.e. the area under the curve in the bottom panel of Figure 3, is:</p>

<script type="math/tex; mode=display">\hat{L}_a = \frac{\sigma_\Delta^2 M e^{-r\tau}}{r \sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{m\tau})}}</script>

<p>where $ \tau $ is the duration of the experiment and $M$ is the number of post-experiment sessions per day. See the Appendix for a derivation.</p>

<p>There’s two things to note about this formula.</p>
<ol>
  <li>Increasing the number of bucketed sessions per day, $m$, always increases $\hat{L}_a$.</li>
  <li>Increasing the duration of the experiment, $\tau$, may or may not help and is the result of competing forces in the numerator and denominator. In the numerator, higher $\tau$ decreases $\hat{L}_a$ by delaying shipment. In the denominator, higher $\tau$ increases $\hat{L}_a$ by making it more likely you will ship the superior version.</li>
</ol>

<h2 id="optimizing-sample-size">Optimizing sample size</h2>

<p>At long last, we can answer the question, “How long should we run this experiment?”. A nice way to do it is to plot $\hat{L}_a$ as a function of $\tau$. Below we see what this looks like for one set of parameters. Here the optimal duration is 18 days.</p>
<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/time_aggregated_lift_by_tau.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 4</strong>.
  </div>
</div>
<p><br />
Note also that a set of simulated experiment and post-experiment periods (in blue) confirm the predictions of the closed form solution (in gray). See the <a href="https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb">notebook</a> for details.</p>

<h2 id="three-lessons-for-practioners">Three lessons for practioners</h2>
<p>I played around with the formula for $\hat{L}_a$ and came across three lessons I think will be of interest to practitioners.</p>

<h4 id="1-you-should-run-underpowered-experiments-if-you-have-a-very-high-discount-rate">1. You should run “underpowered” experiments if you have a very high discount rate</h4>
<p>Take a look at Figure 5, which shows some recommendations for conversion rate experiments where $p=0.10$ and $\sigma_\Delta=0.01$. On the left panel we plot the optimal duration as a function of the annual discount rate. If you have a high discount rate, you care a lot more about the near future than the distant future and therefore it is critical that you ship any potentially winning version as soon as possible. In this scenario, the optimal duration is low (left panel), and the probability you will find a statistically significant result is low (right panel). For many of these cases, the optimal duration would traditionally be considered “underpowered”.</p>
<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/optimal_tau_and_sig_rate_by_r.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 5</strong>.
  </div>
</div>
<p><br /></p>

<h4 id="2-you-should-run-underpowered-experiments-if-you-have-a-small-user-base">2. You should run ‘underpowered’ experiments if you have a small user base</h4>
<p>Now let’s plot these curves as a function of $m$, our daily sessions per bucket. If we only have a small number of sessions to work with, we’ll need to run the experiment for longer (left panel). What’s especially interesting is that the optimal duration for low $m$ scenarios still results in a low rate of statistical significance. That is, if you don’t have a lot of users to work with, it’s optimal to run an “underpowered” experiment. Waiting to get a large number of sessions just causes too much time-discounting loss.</p>
<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/optimal_tau_and_sig_rate_by_m.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 6</strong>.
  </div>
</div>
<p><br /></p>

<h4 id="3-that-said-its-far-better-to-run-your-experiment-too-long-than-too-short">3. That said, it’s far better to run your experiment too long than too short</h4>
<p>Below we plot the aggregate time-discounted lift, $\hat{L}_a$ as a function of duration, for various combinations of $m$ and $r$. In all plots the left shoulder is steeper than the right shoulder. This means that it’s really bad if your experiment is shorter than optimal, but it’s kind of ok if your experiment is longer than optimal. This is true for basically all parameter regimes, unless you have an insanely high discount rate (not shown).</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/L_a_by_tau_for_m_and_r.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 7</strong>.
  </div>
</div>
<p><br /></p>

<p>Side note: It’s instructive to look at the bottom right panel of Figure 7, where there is a very high number of sessions per day and a relatively steep discount curve. Because of the high number of sessions, all of the benefits of the experiment are immediately obtained on the first day, and the $\hat{L}_a$ curve smashes up at its ceiling. After that, nothing else can be gained by extending the experiment, and any additional time spent is lost to discounting.</p>

<h2 id="examples-in-python">Examples in Python</h2>
<h3 id="example-1-continuous-variable-metric">Example 1: Continuous variable metric</h3>
<p>Let’s say you want to run an experiment comparing two different versions of a website, and your main metric is revenue per session. You know in advance that the within-group variance of this metric is $\sigma_X^2 = 100$. You don’t know which version is better but you have a prior that the true difference in means is normally distributed with variance $\sigma_\Delta^2 = 1$. You have 200 sessions per day and plan to bucket 100 sessions into Version A and 100 sessions into Version B, running the experiment for $\tau=20$ days. Your discount rate is fairly aggressive at 1.0 annually, or $r = 1/365$ per day. Using the function in the <a href="https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb">notebook</a>, you can find $\hat{L}_a$ with ths command:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">get_agg_lift_via_closed_form</span><span class="p">(</span><span class="n">var_D</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">var_X</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">365</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="c1"># returns 26298
</span></code></pre></div></div>

<p>You can also use the <code class="language-plaintext highlighter-rouge">find_optimal_tau</code> function to determine the optimal duration, which in this case is $\tau=18$.</p>

<h3 id="example-2-conversion-rates">Example 2: Conversion rates</h3>
<p>Let’s say your main metric is conversion rate. You think that on average conversion rates will be about 10%, and that the difference in conversion rates between buckets will be normally distributed with variance 1%. Using the normal approximation of the binomial distribution, you can use <code class="language-plaintext highlighter-rouge">p*(1-p)</code> for <code class="language-plaintext highlighter-rouge">var_X</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">get_agg_lift_via_closed_form</span><span class="p">(</span><span class="n">var_D</span><span class="o">=</span><span class="mf">0.01</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">var_X</span><span class="o">=</span><span class="n">p</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">),</span> <span class="n">m</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">365</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="c1"># returns 207
</span></code></pre></div></div>

<p>You can also use the <code class="language-plaintext highlighter-rouge">find_optimal_tau</code> function to determine the optimal duration, which in this case is $\tau=49$.</p>

<h2 id="faq">FAQ</h2>
<p><strong>Q:</strong> Has there been any similar work on this?</p>

<p><strong>A:</strong> As I was writing this, I came across a <a href="https://arxiv.org/pdf/1811.00457.pdf">fantastic in-press paper</a> by <a href="https://drexel.edu/now/experts/Overview/Feit-Elea/">Elea Feit</a> and <a href="https://www.ron-berman.com/">Ron Berman</a>. The paper is exceptionally clear and I would recommend reading it. Like this blog post, Feit and Berman argue that it doesn’t make any sense to pick sample sizes based on statistical significance and power thresholds. Instead they recommend profit-maximizing sample sizes. They independently come to the same formula for $ \hat{L} $ as I do (see right addend in their Equation 9, making sure to substitute my $\frac{\sigma_\Delta^2}{2}$ for their $\sigma^2)$. Where they differ is that they assume there is a fixed pool of $N$ users that can only experience the product once. In their setup, you can allocate $n_1$ users to Bucket A and $n_2$ users to Bucket B. Once you have identified the winning bucket, you ship that version to the remaining $N-n_1-n_2$ users. Your expected profit is determined by the expected lift from those users. My experience in industry differs from this setup. In my experience there is no constraint that you can only show the product once to a fixed set of users. Instead, there is often an indefinitely increasing pool of new users, and once you ship the winning bucket you can ship it to everyone, including users who already participated in the experiment. To me, the main constraint in industry is therefore time discounting, rather than a finite pool of users.</p>

<p><strong>Q:</strong> In addition to the lift from shipping a winning bucket, doesn’t experimentation also help inform you about the types of products that might work in the future? And if so, doesn’t this mean we should run experiments longer than recommended by your formula for $\hat{L}_a$?</p>

<p><strong>A:</strong> Yes, experimentation can teach lessons that are generalizable beyond the particular product being tested. This is an advantage of high powered experimentation not included in my framework.</p>

<p><strong>Q:</strong> If some users can show up in multiple sessions, doesn’t bucketing by session violate independence assumptions?</p>

<p><strong>A:</strong> Yeah, so this is tricky. For many companies, there is a distribution of user activity, where some users come for many sessions per week and other users come for only one session at most. Modeling this would make the framework significantly more complicated, so I tried to simplify things by making sessions the unit of analysis.</p>

<p><strong>Q:</strong> Is there anything else on your blog related to this topic?</p>

<p><strong>A:</strong> I’m glad you asked!</p>
<ul>
  <li><a href="/2016/02/28/four-pitfalls-of-hill-climbing/">Four pitfalls of hill climbing</a> discusses some product-focused issues in A/B testing</li>
  <li><a href="/2018/02/04/hyperbolic-discounting/">Hyperbolic discounting — The irrational behavior that might be rational after all</a> is about time discounting, although not in the constext of experimentation.</li>
</ul>

<h2 id="appendix">Appendix</h2>
<p>The aggregate time-discounted lift $\hat{L}_a$ is</p>

<script type="math/tex; mode=display">\hat{L}_a = \int_{\tau}^{\infty} \hat{L} M e^{-rt} \,dt</script>

<p>where $\hat{L}$ is the expected per-session lift, $M$ is the number of post-experiment sessions per day, $r$ is the discount rate, and $ \tau $ is the duration of the experiment. Solving the integral gives:</p>

<script type="math/tex; mode=display">\hat{L}_a = \frac{\hat{L} M e^{-r\tau}}{r}</script>

<p>Plugging in our previously solved value of $\hat{L}$ gives</p>

<script type="math/tex; mode=display">\hat{L}_a = \frac{\sigma_\Delta^2 M e^{-r\tau}}{r \sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{m\tau})}}</script>

  <div>
    <a class="twitter-share-button"
      href="https://twitter.com/share"
      data-url=""
      data-via="Chris_Said"
      data-size="large"
      data-count="none">
    Tweet
    </a>
</div>

  <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'csaid81';
    var disqus_identifier = 'http://localhost:4000/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-II/';
    var disqus_url = 'http://localhost:4000/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-II/';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

</div>


    </div>

  </body>
</html>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


<script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
