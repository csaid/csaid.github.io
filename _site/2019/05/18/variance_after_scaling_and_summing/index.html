<!DOCTYPE html>
<html lang="en-us">

<!--
It's bad to import d3 in every post separately (https://groups.google.com/forum/#!topic/d3-js/bwdNirt2uEU).
Importing it globally here.
Putting this up at the top, according to this controversial stack overflow answer
http://stackoverflow.com/questions/7169370/d3-js-and-document-onready
 -->
<script src="https://d3js.org/d3.v5.min.js"></script>

<link rel="stylesheet" href="/public/font-awesome-4.4.0/css/font-awesome.min.css">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
    <meta name="description" content="One of the most useful facts from statistics">
  

  <!-- To get a link preview image, just set the image attribute in your _post -->
  
    <meta content="http://localhost:4000/assets/2019_variance/inverse_variance/main_fig.png" property="og:image"/>
  

  <!-- twitter standard -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@chris_said" />
  <meta name="twitter:title" content="Variance after scaling and summing&#58; One of the most useful facts from statistics" />
  <meta name="twitter:description" content="One of the most useful facts from statistics" />


  <title>
    
      Variance after scaling and summing&#58; One of the most useful facts from statistics &middot; The File Drawer
    
  </title>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-67746868-1', 'auto');
    ga('send', 'pageview');
  </script>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

<!--   <body class="theme-base-cps">
 -->
  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          The File Drawer
        </a>
      </h1>
      <p class="lead">My name is Chris Said and I am a data scientist at Opendoor. This blog is mostly about tech, stats, and science.</p>

    </div>

    <nav class="sidebar-nav">

      
        <a class="sidebar-nav-item" href="/">Home</a>
      
        <a class="sidebar-nav-item" href="/archive">Archive</a>
      
        <a class="sidebar-nav-item" href="/atom.xml">Feed</a>
      

    </nav>

    <div class="wrapper">
      <div class="inner">
        <a href = "http://www.twitter.com/Chris_Said" class="contact-button"><i class="fa fa-twitter fa-2x"></i></a>
        <a href = "https://www.linkedin.com/pub/chris-said/6b/86b/979" class="contact-button"><i class="fa fa-linkedin-square fa-2x"></i></a>
        <a href = "mailto:chris.said@gmail.com" class="contact-button"><i class="fa fa-envelope fa-2x"></i></a>
      </div>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Variance after scaling and summing&#58; One of the most useful facts from statistics</h1>
  <span class="post-date">18 May 2019</span>
  <p>What do $ R^2 $, laboratory error analysis, ensemble learning, meta-analysis, and financial portfolio risk all have in common? The answer is that they all depend on a fundamental principle of statistics that is not as widely known as it should be. Once this principle is understood, a lot of stuff starts to make more sense.</p>

<p>Here’s a sneak peek at what the principle is.</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij}</script>

<p>Don’t worry if the formula doesn’t yet make sense! We’ll work our way up to it slowly, taking pit stops along the way at simpler formulas are that useful on their own. As we work through these principles, we’ll encounter lots of neat applications and explainers.</p>

<p>This post consists of three parts:</p>

<ul>
  <li><strong>Part 1</strong>: Sums of uncorrelated random variables: Applications to social science and laboratory error analysis</li>
  <li><strong>Part 2</strong>: Weighted sums of uncorrelated random variables: Applications to machine learning and scientific meta-analysis</li>
  <li><strong>Part 3</strong>: Correlated variables and Modern Portfolio Theory</li>
</ul>

<h2 id="part-1-sums-of-uncorrelated-random-variables-applications-to-social-science-and-laboratory-error-analysis">Part 1: Sums of uncorrelated random variables: Applications to social science and laboratory error analysis</h2>

<p>Let’s start with some simplifying conditions and assume that we are dealing with <em>uncorrelated</em> random variables. If you take two of them and add them together, the variance of their sum will equal the sum of their variances. This is amazing!</p>

<p>To demonstrate this, I’ve written some Python code that generates three arrays, each of length 1 million. The first two arrays contain samples from two normal distributions with variances 9 and 16, respectively. The third array is the sum of the first two arrays. As shown in the simulation, its variance is 25, which is equal to the sum of the variances of the first two arrays (9 + 16).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c"># 1M samples from normal distribution with variance=9</span>
<span class="k">print</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c"># 9</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c"># 1M samples from normal distribution with variance=16</span>
<span class="k">print</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c"># 16</span>
<span class="n">xp</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span>
<span class="k">print</span><span class="p">(</span><span class="n">xp</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c"># 25</span>
</code></pre></div></div>

<p>This <a href="https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)">fact</a> was first discovered in 1853 and is known as <a href="https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)">Bienaymé’s Formula</a>. While the code example above shows the sum of two random variables, the formula can be extended to multiple random variables as follows:</p>

<div class="box">
If $ X_p $ is a sum of uncorrelated random variables $ X_1 .. X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum{\sigma^2_i} $$

where each $ X_i $ has variance $ \sigma_i^2 $.
</div>

<p>What does the $ p $ stand for in $ X_p $? It stands for <em>portfolio</em>, which is just one of the many applications we’ll see later in this post.</p>

<h3 id="why-this-is-useful">Why this is useful</h3>
<p>Bienaymé’s result is surprising and unintuitive. But since it’s such a simple formula, it is worth committing to memory, especially because it sheds light on so many other principles. Let’s look at two of them.</p>

<h4 id="understanding--r2--and-variance-explained">Understanding $ R^2 $ and “variance explained”</h4>
<p>Psychologists often talk about “within-group variance”, “between-group variance”, and “variance explained”. What do these terms mean?</p>

<p>Imagine a hypothetical study that measured the extraversion of 10 boys and 10 girls, where extraversion is measured on a 10-point scale (<em>Figure 1</em>. Orange bars). The boys have a mean extraversion of 4.4 and the girls have a mean extraversion 5.0. In addition, the overall variance of the data is 2.5.  We can decompose this variance into two parts:</p>

<ul>
  <li><strong>Between-group variance</strong>: Create a 20-element array where every boy is assigned to the mean boy extraversion of 4.4, and every girl is assigned to the mean girl extraversion of 5.0. The variance of this array is 0.9. (<em>Figure 1</em>. Blue bars).</li>
  <li><strong>Within-group variance</strong>: Create a 20-element array of the amount each child’s extraversion deviates from the mean value for their sex. Some of these values will be negative and some will be positive. The variance of this array is 1.6. (<em>Figure 1</em>. Pink bars).</li>
</ul>

<div class="wrapper">
  <img src="/assets/2019_variance/boy_girl_extraversion.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 1</strong>: Decomposition of extraversion scores (orange) into between-group variance (blue) and within-group variance (pink).
  </div>
</div>
<p><br /></p>

<p>If you add these arrays together, the resulting array will represent the observed data (<em>Figure 1</em>. Orange bars). The variance of the observed array is 2.5, which is exactly what is predicted by Bienaymé’s Formula. It is the sum of the variances of the two component arrays (0.9 + 1.6). Psychologists might say that sex “explains” 0.9/2.5 = 36% of the extraversion variance. Equivalently, a model of extraversion that uses sex as the only predictor would have an <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">$ R^2 $</a> of 0.36.</p>

<h4 id="error-propagation-in-laboratories">Error propagation in laboratories</h4>
<p>If you ever took a physics lab or chemistry lab back in college, you may remember having to perform <a href="http://ipl.physics.harvard.edu/wp-uploads/2013/03/PS3_Error_Propagation_sp13.pdf">error analysis</a>, in which you calculated how errors would propagate through one noisy measurement after another.</p>

<p>Physics textbooks often say that standard deviations add in “quadrature”, which just means that if you are trying to estimate some quantity that is the sum of two other measurements, and if each measurement has some error with standard deviation <script type="math/tex">\sigma_1</script> and <script type="math/tex">\sigma_2</script> respectively, the final standard deviation would be  <script type="math/tex">\sigma_{p} = \sqrt{\sigma^2_1 + \sigma^2_2}</script>. I think it’s probably easier to just use variances, as in the Bienaymé Formula, with <script type="math/tex">\sigma^2_{p} = \sigma^2_1 + \sigma^2_2</script>.</p>

<p>For example, imagine you are trying to estimate the height of two boxes stacked on top of each other (<em>Figure 2</em>). One box has a height of 1 meter with variance $ \sigma^2_1 $ = 0.01, and the other has a height of 2 meters with variance $ \sigma^2_2 $ = 0.01. Let’s further assume, perhaps optimistically, that these errors are independent. That is, if the measurement of the first box is too high, it’s not any more likely that the measurement of the second box will also be too high. If we can make these assumptions, then the total height of the two boxes will be 3 meters with variance $ \sigma^2_p $ = 0.02.</p>

<div class="wrapper">
  <img src="/assets/2019_variance/stacked_boxes.png" class="inner" style="position:relative border: #222 2px solid; max-width:50%;" />
  <div class="caption"><strong>Figure 2</strong>: Two boxes stacked on top of each other. The height of each box is measured with some variance (uncertainty). The total height is the the sum of the individual heights, and the total variance (uncertainty) is the sum of the individual variances.
  </div>
</div>
<p><br /></p>

<p>There is a key difference between the extraversion example and the stacked boxes example. In the extraversion example, we added two <em>arrays</em> that each had an observed sample variance. In the stacked boxes example, we added two <em>scalar measurements</em>, where the variance of these measurements refers to our measurement uncertainty. Since both cases have a meaningful concept of ‘variance’, the Bienaymé Formula applies to both.</p>

<h2 id="part-2-weighted-sums-of-uncorrelated-random-variables-applications-to-machine-learning-and-scientific-meta-analysis">Part 2: Weighted sums of uncorrelated random variables: Applications to machine learning and scientific meta-analysis</h2>

<p>Let’s now move on to the case of <em>weighted</em> sums of uncorrelated random variables. But before we get there, we first need to understand what happens to variance when a random variable is scaled.</p>

<div class="box">
If $ X_p $ is defined as $ X $ scaled by a factor of $ w $, then the variance $ X_p $ will be

$$ \sigma_{p}^{2} = w^2 \sigma^2 $$

where $ \sigma^2 $ is the variance of $ X $.
</div>

<p>This means that if a random variable is scaled, the scale factor on the variance will change <em>quadratically</em>. Let’s see this in code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">baseline_var</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">w</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">baseline_var</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c"># Array of 1M samples from normal distribution with variance=10</span>
<span class="k">print</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c"># 10</span>
<span class="n">xp</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x1</span> <span class="c"># Scale this by w=0.7</span>
<span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">baseline_var</span><span class="p">)</span> <span class="c"># 4.9 (predicted variance)</span>
<span class="k">print</span><span class="p">(</span><span class="n">xp</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c"># 4.9 (empirical variance) </span>
</code></pre></div></div>
<p>To gain some intuition for this rule, it’s helpful to think about outliers. We know that outliers have a huge effect on variance. That’s because the formula used to compute variance, $ \sum{\frac{(x_i - \bar{x})^2}{n-1}} $, squares all the deviations, and so we get really big variances when we square large deviations. With that as background, let’s think about what happens if we scale our data by 2. The outliers will spread out twice as far, which means they will have even more than twice as much impact on the variance. Similarly, if we multiply our data by 0.5, we will squash the most “damaging” part of the outliers, and so we will reduce our variance by more than a factor of two.</p>

<p>While the above principle is pretty simple, things start to get interesting when you combine it with the Bienaymé Formula in Part I:</p>

<div class="box">
If $ X_p $ is a weighted sum of uncorrelated random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be 

$$ \sigma_{p}^{2} = \sum{w^2_i \sigma^2_i} $$

where each $ w_i $ is a weight on $ X_i $, and each $ X_i $ has its own variance $ \sigma_i^2 $.
</div>

<p>The above formula shows what happens when you scale and then sum random variables. The final variance is the weighted sum of the original variances, where the weights are squares of the original weights. Let’s see how this can be applied to machine learning.</p>

<h3 id="an-ensemble-model-with-equal-weights">An ensemble model with equal weights</h3>

<p>Imagine that you have built two separate models to predict car prices. While the models are unbiased, they have variance in their errors. That is, sometimes a model prediction will be too high, and sometimes a model prediction will be too low. Model 1 has a mean squared error (MSE) of \$1,000 and Model 2 has an MSE of \$2,000.</p>

<p>A valuable insight from machine learning is that you can often create a better model by simply averaging the predictions of other models. Let’s demonstrate this with simulations below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">actual</span> <span class="o">=</span> <span class="mi">20000</span> <span class="o">+</span> <span class="mi">5000</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">errors1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">errors1</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c"># 1000</span>
<span class="n">errors2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">errors2</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c"># 2000</span>

<span class="c"># Note that this section could be replaced with </span>
<span class="c"># errors_ensemble = 0.5 * errors1 + 0.5 * errors2</span>
<span class="n">preds1</span> <span class="o">=</span> <span class="n">actual</span> <span class="o">+</span> <span class="n">errors1</span>
<span class="n">preds2</span> <span class="o">=</span> <span class="n">actual</span> <span class="o">+</span> <span class="n">errors2</span>
<span class="n">preds_ensemble</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">preds1</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">preds2</span>
<span class="n">errors_ensemble</span> <span class="o">=</span> <span class="n">preds_ensemble</span> <span class="o">-</span> <span class="n">actual</span>

<span class="k">print</span><span class="p">(</span><span class="n">errors_ensemble</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c"># 750. Lower than variance of component models!</span>
</code></pre></div></div>

<p>As shown in the code above, even though a good model (Model 1) was averaged with an inferior model (Model 2), the resulting Ensemble model’s MSE of \$750 is better than either of the models individually.</p>

<p>The benefits of ensembling follow directly from the weighted sum formula we saw above, <script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>. To understand why, it’s helpful to think of models not as generating predictions, but rather as generating errors. Since averaging the predictions of a model corresponds to averaging the errors of the model, we can treat each model’s array of errors as samples of a random variable whose variance can be plugged in to the formula. Assuming the models are unbiased (i.e. the errors average to about zero), the formula tells us the expected MSE of the ensemble predictions. In the example above, the MSE would be</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = 0.5^2 \times 1000 + 0.5^2 \times 2000 = 750</script>

<p>which is exactly what we observed in the simulations.</p>

<p>(For a totally different intuition of why ensembling works, see <a href="https://www.opendoor.com/w/blog/why-ensembling-works-the-intuition-behind-opendoors-home-pricing">this blog post</a> that I co-wrote for my company, Opendoor.)</p>

<h3 id="an-ensemble-model-with-inverse-variance-weighting">An ensemble model with Inverse Variance Weighting</h3>

<p>In the example above, we obtained good results by using an equally-weighted average of the two models. But can we do better?</p>

<p>Yes we can! Since Model 1 was better than Model 2, we should probably put more weight on Model 1. But of course we shouldn’t put all our weight on it, because then we would throw away the demonstrably useful information from Model 2. The optimal weight must be somewhere in between 50% and 100%.</p>

<p>An effective way to find the optimal weight is to <a href="http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/">build another model on top of these models</a>. However, if you can make certain assumptions (unbiased and uncorrelated errors), there’s an even simpler approach that is great for back-of-the envelope calculations and great for understanding the principles behind ensembling.</p>

<p>To find the optimal weights (assuming unbiased and uncorrelated errors), we need to minimize the variance of the ensemble errors
<script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>
with the constraint that 
<script type="math/tex">\sum{w_i} = 1</script>.</p>

<p>It <a href="https://en.wikipedia.org/wiki/Inverse-variance_weighting">turns out</a> that the variance-minimizing weight for a model should be proportional to the inverse of its variance.</p>

<script type="math/tex; mode=display">w_k = \frac{\frac{1}{\sigma^2_k}}{\sum{\frac{1}{\sigma^2_i}}}</script>

<p>When we apply this method, we obtain optimal weights of <script type="math/tex">w_1</script> = 0.67 and <script type="math/tex">w_2</script> = 0.33. These weights give us an ensemble error variance of</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = 0.67^2 \times 1000 + 0.33^2 \times 2000 = 666</script>

<p>which is significantly better than the $750 variance we were getting with equal weighting.</p>

<p>This method is called <a href="https://en.wikipedia.org/wiki/Inverse-variance_weightinghttps://en.wikipedia.org/wiki/Inverse-variance_weighting">Inverse Variance Weighting</a>, and allows you to assign the right amount of weight to each model, depending on its error.</p>

<p>Inverse Variance Weighting is not just useful as a way to understand Machine Learning ensembles. It is also one of the core principles in scientific <a href="https://en.wikipedia.org/wiki/Meta-analysis">meta-analysis</a>, which is popular in medicine and the social sciences. When multiple scientific studies attempt to estimate some quantity, and each study has a different sample size (and hence variance of their estimate), a meta-analysis should weight the high sample size studies more. Inverse Variance Weighting is used to determine those weights.</p>

<h2 id="part-3-correlated-variables-and-modern-portfolio-theory">Part 3: Correlated variables and Modern Portfolio Theory</h2>

<p>Let’s imagine we now have three unbiased models with the following MSEs:</p>

<ul>
  <li>Model 1: MSE = 1000</li>
  <li>Model 2: MSE = 1000</li>
  <li>Model 3: MSE = 2000</li>
</ul>

<p>By Inverse Variance Weighting, we should assign more weight to the first two models, with <script type="math/tex">w_1=0.4, w_2=0.4, w_3=0.2</script>.</p>

<p>But what happens if Model 1 and Model 2 have correlated errors? For example, whenever Model 2’s predictions are too high, Model 3’s predictions tend to also be too high. In that case, maybe we don’t want to give so much weight to Models 1 and 2, since they provide somewhat redundant information. Instead we might want to <em>diversify</em> our ensemble by increasing the weight on Model 3, since it provides new independent information.</p>

<p>To determine how much weight to put on each model, we first need to determine how much total variance there will be if the errors are correlated. To do this, we need to borrow a <a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory">formula</a> from the financial literature, which extends the formulas we’ve worked with before. This is the formula we’ve been waiting for.</p>

<div class="box">
If $ X_p $ is a weighted sum of (correlated or uncorrelated) random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij} $$

where each $ w_i $ and $ w_j $ are weights assigned to $ X_i $ and $ X_j $, where each $ X_i $ and $ X_j $ have standard deviations $ \sigma_i $ and $ \sigma_j $, and where the correlation between $ X_i $ and $ X_j $ is $ \rho_{ij} $.
</div>

<p>There’s a lot to unpack here, so let’s take this step by step.</p>

<ul>
  <li><script type="math/tex">\sigma_i \sigma_j \rho_{ij}</script> is a scalar quantity representing the covariance between <script type="math/tex">X_i</script> and <script type="math/tex">X_j</script>.</li>
  <li>If none of the variables are correlated with each other, then all the cases where $ i \neq j $ will go to zero, and the formula reduces to <script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>, which we have seen before.</li>
  <li>The more that two variables <script type="math/tex">X_i</script> and <script type="math/tex">X_j</script> are correlated, the more the total variance <script type="math/tex">\sigma_{p}^{2}</script> increases.</li>
  <li>If two variables <script type="math/tex">X_i</script> and <script type="math/tex">X_j</script> are anti-correlated, then the total variance decreases, since <script type="math/tex">\sigma_i \sigma_j \rho_{ij}</script> is negative.</li>
  <li>This formula can be rewritten in more compact notation as <script type="math/tex">\sigma_{p}^{2} = \vec{w}^T\Sigma \vec{w}</script>, where <script type="math/tex">\vec{w}</script> is the weight vector, and <script type="math/tex">\Sigma</script> is the covariance matrix (not a summation sign!)</li>
</ul>

<p>If you skimmed the bullet points above, go back and re-read them! They are super important.</p>

<p>To find the set of weights that minimize variance in the errors, you must minimize the above formula, with the constraint that <script type="math/tex">\sum{w_i} = 1</script>. One way to do this is to use a <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize">numerical optimization method</a>. In practice, however, it is more common to just find weights by <a href="http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/">building another model on top of the base models</a></p>

<p>Regardless of how the weights are found, it will usually be the case that if Models 1 and 2 are correlated, the optimal weights will reduce redundancy and put lower weight on these models than simple Inverse Variance Weighting would suggest.</p>

<h3 id="applications-to-financial-portfolios">Applications to financial portfolios</h3>

<p>The formula above was discovered by economist <a href="https://en.wikipedia.org/wiki/Harry_Markowitz">Harry Markowitz</a> in his <a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory">Modern Portfolio Theory</a>, which describes how an investor can optimally trade off between expected returns and expected risk, often measured as variance. In particular, the theory shows how to maximize expected return given a fixed variance, or minimize variance given a fixed expected return. We’ll focus on the latter.</p>

<p>Imagine you have three stocks to put in your portfolio. You plan to sell them at time $ T $, at which point you expect that Stock 1 will have gone up by 5%, with some uncertainty. You can describe your uncertainty as variance, and in the case of Stock 1, let’s say <script type="math/tex">\sigma_1^2</script> = 1. This stock, as well Stocks 2 and 3, are summarized in the table below:</p>

<table>
  <thead>
    <tr>
      <th>Stock ID</th>
      <th>Expected Return</th>
      <th>Expected Risk (<script type="math/tex">\sigma^2</script>)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>5.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>5.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>5.0</td>
      <td>2.0</td>
    </tr>
  </tbody>
</table>

<p>This financial example should remind you of ensembling in machine learning. In the case of ensembling, we wanted to minimize variance of the weighted sum of error arrays. In the case of financial portfolios, we want to minimize the variance of the weighted sum of scalar financial returns.</p>

<p>As before, if there are no correlations between the expected returns (i.e. if Stock 1 exceeding 5% return does not imply that Stock 2 or Stock 3 will exceed 5% return), then the total variance in the portfolio will be
<script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>
and we can use Inverse Variance Weighting to obtain weights $ w_1=0.4, w_2=0.4, w_3=0.2 $.</p>

<p>However, sometimes stocks have correlated expected returns. For example, if two of the stocks are in oil companies, then one stock exceeding 5% implies the other is also likely to exceed 5%. When this happens, the total variance becomes</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij}</script>

<p>as we saw before in the ensemble example. Since this includes an additional positive term for <script type="math/tex">w_1 w_2 \sigma_1 \sigma_2 \rho_{1,2}</script>, the expected variance is higher than in the uncorrelated case, assuming the correlations are positive. To reduce this variance, we should put less weight on Stocks 1 and 2 than we would otherwise.</p>

<p>While the example above focused on minimizing the variance of a financial portfolio, you might also be interested in having a portfolio with high return. Modern Portfolio Theory describes how a portfolio can reach any abitrary point on the <a href="https://en.wikipedia.org/wiki/Efficient_frontier">efficient frontier</a> of variance and return, but that’s outside the scope of this blog post. And as you might expect, financial markets can be more complicated than Modern Portfolio Theory suggests, but that’s also outside scope.</p>

<h2 id="summary">Summary</h2>

<p>That was a long post, but I hope that the principles described have been informative. It may be helpful to summarize them in <em>backwards</em> order, starting with the most general principle.</p>

<div class="box">

If $ X_p $ is a weighted sum of (correlated or uncorrelated) random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij} $$

where each $ w_i $ and $ w_j $ are weights assigned to $ X_i $ and $ X_j $, where each $ X_i $ and $ X_j $ have standard deviations $ \sigma_i $ and $ \sigma_j $, and where the correlation between $ X_i $ and $ X_j $ is $ \rho_{ij} $. The term $ \sigma_i \sigma_j \rho_{ij} $ is a scalar quantity representing the covariance between $ X_i $ and $ X_j $.

<br /><br />If none of the variables are correlated, then all the cases where $ i \neq j $ go to zero, and the formula reduces to 

$$ \sigma_{p}^{2} = \sum{w^2_i \sigma^2_i} $$

And finally, if we are computing a simple sum of random variables where all the weights are 1, then the formula reduces to

$$ \sigma_{p}^{2} = \sum{\sigma^2_i} $$
</div>

  <div>
    <a class="twitter-share-button"
      href="https://twitter.com/share"
      data-url=""
      data-via="Chris_Said"
      data-size="large"
      data-count="none">
    Tweet
    </a>
</div>

  <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'csaid81';
    var disqus_identifier = 'http://localhost:4000/2019/05/18/variance_after_scaling_and_summing/';
    var disqus_url = 'http://localhost:4000/2019/05/18/variance_after_scaling_and_summing/';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

</div>


    </div>

  </body>
</html>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


<script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
