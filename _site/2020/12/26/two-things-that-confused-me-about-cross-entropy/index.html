<!DOCTYPE html>
<html lang="en-us">

<!--
It's bad to import d3 in every post separately (https://groups.google.com/forum/#!topic/d3-js/bwdNirt2uEU).
Importing it globally here.
Putting this up at the top, according to this controversial stack overflow answer
http://stackoverflow.com/questions/7169370/d3-js-and-document-onready
 -->
<script src="https://d3js.org/d3.v5.min.js"></script>

<link rel="stylesheet" href="/public/font-awesome-4.4.0/css/font-awesome.min.css">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
    <meta name="description" content="Some warnings about unstandardized notation">
  

  <!-- To get a link preview image, just set the image attribute in your _post -->
  
    <meta content="http://localhost:4000/assets/2020_cross_entropy/H_p_q.png" property="og:image"/>
  

  <!-- twitter standard -->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:site" content="@chris_said" />
  <meta name="twitter:title" content="Things that confused me about cross-entropy" />
  <meta name="twitter:description" content="Some warnings about unstandardized notation" />


  <title>
    
      Things that confused me about cross-entropy &middot; Chris Said
    
  </title>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-67746868-1', 'auto');
    ga('send', 'pageview');
  </script>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/logo144.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

<!--   <body class="theme-base-cps">
 -->
  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Chris Said
        </a>
      </h1>
      <p class="lead">I am a data scientist at Stitch Fix. This blog is mostly about statistics, technology, and science.</p>

    </div>

    <!-- <nav class="sidebar-nav">

      
        <a class="sidebar-nav-item" href="/">Home</a>
      
        <a class="sidebar-nav-item" href="/atom.xml">Feed</a>
      

    </nav> -->

    <div class="wrapper">
      <div class="inner">
        <a href = "http://www.twitter.com/Chris_Said" class="contact-button"><i class="fa fa-twitter fa-2x"></i></a>
        <a href = "https://www.linkedin.com/pub/chris-said/6b/86b/979" class="contact-button"><i class="fa fa-linkedin-square fa-2x"></i></a>
        <a href = "mailto:chris.said@gmail.com" class="contact-button"><i class="fa fa-envelope fa-2x"></i></a>
      </div>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Things that confused me about cross-entropy</h1>
  <span class="post-date">26 Dec 2020</span>
  <p>Every once in a while, I try to better understand cross-entropy by skimming over some Medium posts and StackExchange answers. I always come away only half-understanding it.</p>

<p>A major cause of confusion is that different sources use different notations and conventions. Here are some that tripped me up.</p>

<h3 id="p-and-q-vs-y-and-p">p and q vs y and p</h3>

<p>In information theory, the cross-entropy for an event with \(M\) discrete outcome classes is</p>

\[H(p, q) = -\sum_{j=1}^M{p_j log (q_j)}\]

<p>… where \(p\) is the true distribution of outcomes and \(q\) is the approximating distribution.</p>

<p>But in machine learning, the cross-entropy is defined as:</p>

\[H(y, p) = -\sum_{j=1}^M{y_j log (p(y_j))}\]

<p>… where \(y\) are the true labels and \(p(y)\) is the approximating distribution, i.e. the classifier’s probability predictions.</p>

<p>Notice that the meaning of \(p\) has been reversed! In information theory, it is the true distribution. In machine learning, it is the approximating distribution.</p>

<p>This points to a key difference between generic information theory and machine learning applications. In information theory, the target distribution \(p\) is a probability distribution where the probability mass can be distributed broadly over the classes. In machine learning, the target distribution \(y\) has all of its mass allocated to a known label.</p>

<h3 id="multiple-classes-vs-multiple-trials-and-indicators-vs-labels">Multiple classes vs multiple trials, and indicators vs labels</h3>

<p>As shown above, the cross-entropy for a single event with \(M\) classes is:</p>

\[H(y, p) = -\sum_{j=1}^M{y_j log (p(y_j))}\]

<p>If there are multiple trials, you just need to sum up the cross-entropies from all the trials so that the total cross-entropy is:</p>

\[H(y, p) = -\sum_{i=1}^N \sum_{j=1}^M{y_j^{(i)} log (p(y_j^{(i)}))}\]

<p>… where \(N\) is the number of trials.</p>

<p>This all makes notational sense. Unfortunately, confusion arises when people use non-compact notation for summing over binary classes. When dealing with binary classification, it’s common to see cross-entropy defined as:</p>

\[H(y, p) = -\sum_{i=1}^N{y_i log (p(y_i)) + (1-y_i) log (1 -p(y_i))}\]

<p>Superficially, this looks a lot like the first formula, but it’s actually just a clever and somewhat confusing way of writing the second formula for binary classes. Its notation differs from previous formulas in a few ways:</p>
<ul>
  <li>First, \(p(y_i)\) always refers to the probability of a positive result. In previous formulas, \(p(y_j)\) referred to the probability of the particular class being considered in the summation loop.</li>
  <li>Second, \(y_i\) (when not wrapped by a \(p\)) is a <em>label</em>, not an indicator! When the event has an outcome of 1, \(y_i\) is 1. When it has an outcome of 0, \(y_i\) is 0. In previous formulas, as you loop through classes you would set \(y_j\) to be 1 whenever the outcome was the class being considered; otherwise you set it to 0.</li>
  <li>While not a notational difference, this formula sums over events (and then enumerates a sum over classes), whereas the first formula in this section only sums over classes.</li>
</ul>

<p>The image below summarizes the many confusing differences between these formulas.</p>

<p><img src="/assets/2020_cross_entropy/cross-entropy.png" /></p>

<h3 id="where-to-learn-about-cross-entropy">Where to learn about cross-entropy</h3>
<p>I don’t know if there is a single best place to learn about cross-entropy, but below are a few places that were helpful. If you read them make sure you think about (a) how they are notating the true distribution and the approximating distribution (b) whether they are writing about single trials or multiple trials (c) whether they are writing about binary classes or multi classes and (d) whether the \(y\)’s are indicators or labels.</p>

<ul>
  <li><a href="https://xcelab.net/rm/statistical-rethinking">Statistical Rethinking</a>. Information theory notation, single trial case, multiple classes.</li>
  <li><a href="http://www.awebb.info/probability/2017/05/18/cross-entropy-and-log-likelihood.html">Andrew Webb</a>. Both notations, both the single and multiple trial case, multiple classes.</li>
  <li><a href="https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83">Harshith</a>. ML notation, multiple trials, binary classes.</li>
</ul>


  <div>
    <a class="twitter-share-button"
      href="https://twitter.com/share"
      data-url=""
      data-via="Chris_Said"
      data-size="large"
      data-count="none">
    Tweet
    </a>
</div>

  <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'csaid81';
    var disqus_identifier = 'http://localhost:4000/2020/12/26/two-things-that-confused-me-about-cross-entropy/';
    var disqus_url = 'http://localhost:4000/2020/12/26/two-things-that-confused-me-about-cross-entropy/';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

</div>


    </div>

  </body>
</html>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
