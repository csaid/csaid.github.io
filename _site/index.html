<!DOCTYPE html>
<html lang="en-us">

<!--
It's bad to import d3 in every post separately (https://groups.google.com/forum/#!topic/d3-js/bwdNirt2uEU).
Importing it globally here.
Putting this up at the top, according to this controversial stack overflow answer
http://stackoverflow.com/questions/7169370/d3-js-and-document-onready
 -->
<script src="https://d3js.org/d3.v5.min.js"></script>

<link rel="stylesheet" href="/public/font-awesome-4.4.0/css/font-awesome.min.css">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
    <meta name="description" content="My name is Chris Said and I am a data scientist at Opendoor. This blog is mostly about tech, stats, and science.">
  

  <!-- To get a link preview image, just set the image attribute in your _post -->
  

  <!-- twitter standard -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@chris_said" />
  <meta name="twitter:title" content="Home" />
  <meta name="twitter:description" content="" />


  <title>
    
      The File Drawer &middot; A blog by Chris Said
    
  </title>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-67746868-1', 'auto');
    ga('send', 'pageview');
  </script>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

<!--   <body class="theme-base-cps">
 -->
  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          The File Drawer
        </a>
      </h1>
      <p class="lead">My name is Chris Said and I am a data scientist at Opendoor. This blog is mostly about tech, stats, and science.</p>

    </div>

    <nav class="sidebar-nav">

      
        <a class="sidebar-nav-item active" href="/">Home</a>
      
        <a class="sidebar-nav-item" href="/archive">Archive</a>
      
        <a class="sidebar-nav-item" href="/atom.xml">Feed</a>
      

    </nav>

    <div class="wrapper">
      <div class="inner">
        <a href = "http://www.twitter.com/Chris_Said" class="contact-button"><i class="fa fa-twitter fa-2x"></i></a>
        <a href = "https://www.linkedin.com/pub/chris-said/6b/86b/979" class="contact-button"><i class="fa fa-linkedin-square fa-2x"></i></a>
        <a href = "mailto:chris.said@gmail.com" class="contact-button"><i class="fa fa-envelope fa-2x"></i></a>
      </div>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-II/">
        Optimizing sample sizes in A/B testing, Part II&#58; Aggregate time-discounted lift
      </a>
    </h1>

    <span class="post-date">01 Jan 2020</span>

    <p>This is Part II of a two-part blog post on how to optimize your sample size in A/B testing. As in <a href="/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-I/">Part I</a>, the focus will be on choosing a sample size at the beginning of the experiment and committing to it, not on dynamically updating the sample size as the experiment proceeds.</p>

<p>In <a href="/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-I/">Part I</a>, we learned how before the experiment starts we can estimate $\hat{L}$, the expected post-experiment lift, a probability weighted average of outcomes.</p>

<p>In Part II, we’ll discuss how to estimate what is perhaps the most important per-unit cost of epxerimentation: the forfeited benefits that could come from shipping the winning bucket earlier. This leads to something I think is incredibly cool: A formula for $\hat{L}_a$, the aggregate time-discounted post-experiment lift, as a function of sample size. The formula allows you to pick optimal sample sizes specific to your business circumstances, and leads us to three lessons for practioners.</p>

<ol>
  <li>You should run “underpowered” experiments if you have a very high discount rate.</li>
  <li>You should run “underpowered” experiments if you have a small user base.</li>
  <li>That said, it’s far better to run your experiment too long than too short.</li>
</ol>

<h2 id="a-quick-modification-from-part-i">A quick modification from Part I</h2>

<p>In <a href="/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-I/">Part I</a>, we saw that if you run an experiment comparing the current version of your product (A) to a new alternative (B), and if you ship whichever version does best in the experiment, your business will on average experience a post-experiment per-user lift of</p>

<script type="math/tex; mode=display">\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{n})}}</script>

<p>where $\sigma_\Delta^2$ is the variance on your normally distributed zero-mean prior for $\mu_B - \mu_A$, $\sigma_X^2$ is the within-group variance, and $n$ is the per-bucket sample size.</p>

<p>Because Part II is primarily concerned with the the duration of the experiment, we’re going to modify the formula to be time-dependent. As a simplifying assumption we’re going to make <em>sessions</em>, rather then <em>users</em>, the unit of analysis. We’ll also assume that you have a constant number of sessions per day. This changes the definition of $\hat{L}$ to a <em>post-experiment per-session lift</em>, and the formula becomes</p>

<script type="math/tex; mode=display">\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{mt})}}</script>

<p>where $m$ is the sessions per day for each bucket, and $t$ is duration of the experiment in days.</p>

<h2 id="time-discounting">Time Discounting</h2>

<p>The formula above shows that larger sample sizes result in higher $ \hat{L} $, since larger samples make it more likely you will ship the better version. But as with all things in life, there are costs to increasing your sample size. In particular, the larger your sample size, the longer you have to wait to ship the winning bucket. In a fast moving startup, there’s often good reason to accrue your wins as soon as possible. Lift today is much more valuable than the same lift a year from now.</p>

<p>How much more valuable is lift today versus lift a year from now? A common way to quantify this is with exponential discounting, such that weights (or “discount factors”) on future lift follow the form:</p>

<script type="math/tex; mode=display">w = e^{-rt}</script>

<p>where $ r $ is a discount rate. For teams shipping product at startups, the annual discount rate might be quite large, like 0.5 or even 1.0, which would correspond to a daily discount rate $r$ of 0.5/365 or 1.0/365, respectively.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/discount_function.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 1</strong>.
  </div>
</div>
<p><br /></p>

<h2 id="aggregate-time-discounted-lift-visual-intuition">Aggregate time-discounted lift: Visual Intuition</h2>

<p>Take a look at Figure 2, below. It shows an experiment that is planned to run for $\tau = 60$ days. The top panel shows $\hat{L}$. While the experiment is running, $\hat{L} = 0$, since our prior is that $\Delta$ is sampled from a normal distribution with mean zero. But once the experiment finishes and we launch the winning bucket, we should begin to reap our expected per-session lift.</p>

<p>The middle panel shows our discount function.</p>

<p>The bottom panel shows our time-discounted lift, defined as the product of the lift in the top panel and the time discount in the middle panel. (We can also multiply it by $M$, the number of post-experiment sessions per day, which for simplicity we set to 1 here.) The aggregate time-discounted lift, $\hat{L}_a$, is the area under the curve.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/discounted_lift_static.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 2</strong>.
  </div>
</div>
<p><br /></p>

<p>Now let’s see what happens with different experiment durations. Figure 3 shows that the longer you plan to run your experiment, the higher $\hat{L}$ will be (top panel). But due to time discounting, (middle panel), the area under the time-discounted lift curve (bottom panel) is low for overly large sample sizes. There is an optimal duration of the experiment (in this case, $\tau = 24$ days), that maximizes $\hat{L}_a$, the area under the curve.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/discounted_lift_dynamic.gif" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 3</strong>.
  </div>
</div>
<p><br /></p>

<h2 id="aggregate-time-discounted-lift-formula">Aggregate time-discounted lift: Formula</h2>
<p>The aggregate time-discounted lift $\hat{L}_a$, i.e. the area under the curve in the bottom panel of Figure 3, is:</p>

<script type="math/tex; mode=display">\hat{L}_a = \frac{\sigma_\Delta^2 M e^{-r\tau}}{r \sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{m\tau})}}</script>

<p>where $ \tau $ is the duration of the experiment and $M$ is the number of post-experiment sessions per day. See the Appendix for a derivation.</p>

<p>There’s two things to note about this formula.</p>
<ol>
  <li>Increasing the number of bucketed sessions per day, $m$, always increases $\hat{L}_a$.</li>
  <li>Increasing the duration of the experiment, $\tau$, may or may not help and is the result of competing forces in the numerator and denominator. In the numerator, higher $\tau$ decreases $\hat{L}_a$ by delaying shipment. In the denominator, higher $\tau$ increases $\hat{L}_a$ by making it more likely you will ship the superior version.</li>
</ol>

<h2 id="optimizing-sample-size">Optimizing sample size</h2>

<p>At long last, we can answer the question, “How long should we run this experiment?”. A nice way to do it is to plot $\hat{L}_a$ as a function of $\tau$. Below we see what this looks like for one set of parameters. Here the optimal duration is 18 days.</p>
<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/time_aggregated_lift_by_tau.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 4</strong>.
  </div>
</div>
<p><br />
Note also that a set of simulated experiment and post-experiment periods (in blue) confirm the predictions of the closed form solution (in gray). See the <a href="https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb">notebook</a> for details.</p>

<h2 id="three-lessons-for-practioners">Three lessons for practioners</h2>
<p>I played around with the formula for $\hat{L}_a$ and came across three lessons I think will be of interest to practitioners.</p>

<h4 id="1-you-should-run-underpowered-experiments-if-you-have-a-very-high-discount-rate">1. You should run “underpowered” experiments if you have a very high discount rate</h4>
<p>Take a look at Figure 5, which shows some recommendations for conversion rate experiments where $p=0.10$ and $\sigma_\Delta=0.01$. On the left panel we plot the optimal duration as a function of the annual discount rate. If you have a high discount rate, you care a lot more about the near future than the distant future and therefore it is critical that you ship any potentially winning version as soon as possible. In this scenario, the optimal duration is low (left panel), and the probability you will find a statistically significant result is low (right panel). For many of these cases, the optimal duration would traditionally be considered “underpowered”.</p>
<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/optimal_tau_and_sig_rate_by_r.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 5</strong>.
  </div>
</div>
<p><br /></p>

<h4 id="2-you-should-run-underpowered-experiments-if-you-have-a-small-user-base">2. You should run ‘underpowered’ experiments if you have a small user base</h4>
<p>Now let’s plot these curves as a function of $m$, our daily sessions per bucket. If we only have a small number of sessions to work with, we’ll need to run the experiment for longer (left panel). What’s especially interesting is that the optimal duration for low $m$ scenarios still results in a low rate of statistical significance. That is, if you don’t have a lot of users to work with, it’s optimal to run an “underpowered” experiment. Waiting to get a large number of sessions just causes too much time-discounting loss.</p>
<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/optimal_tau_and_sig_rate_by_m.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 6</strong>.
  </div>
</div>
<p><br /></p>

<h4 id="3-that-said-its-far-better-to-run-your-experiment-too-long-than-too-short">3. That said, it’s far better to run your experiment too long than too short</h4>
<p>Below we plot the aggregate time-discounted lift, $\hat{L}_a$ as a function of duration, for various combinations of $m$ and $r$. In all plots the left shoulder is steeper than the right shoulder. This means that it’s really bad if your experiment is shorter than optimal, but it’s kind of ok if your experiment is longer than optimal. This is true for basically all parameter regimes, unless you have an insanely high discount rate (not shown).</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/L_a_by_tau_for_m_and_r.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 7</strong>.
  </div>
</div>
<p><br /></p>

<p>Side note: It’s instructive to look at the bottom right panel of Figure 7, where there is a very high number of sessions per day and a relatively steep discount curve. Because of the high number of sessions, all of the benefits of the experiment are immediately obtained on the first day, and the $\hat{L}_a$ curve smashes up at its ceiling. After that, nothing else can be gained by extending the experiment, and any additional time spent is lost to discounting.</p>

<h2 id="examples-in-python">Examples in Python</h2>
<h3 id="example-1-continuous-variable-metric">Example 1: Continuous variable metric</h3>
<p>Let’s say you want to run an experiment comparing two different versions of a website, and your main metric is revenue per session. You know in advance that the within-group variance of this metric is $\sigma_X^2 = 100$. You don’t know which version is better but you have a prior that the true difference in means is normally distributed with variance $\sigma_\Delta^2 = 1$. You have 200 sessions per day and plan to bucket 100 sessions into Version A and 100 sessions into Version B, running the experiment for $\tau=20$ days. Your discount rate is fairly aggressive at 1.0 annually, or $r = 1/365$ per day. Using the function in the <a href="https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb">notebook</a>, you can find $\hat{L}_a$ with ths command:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">get_agg_lift_via_closed_form</span><span class="p">(</span><span class="n">var_D</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">var_X</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">365</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="c1"># returns 26298
</span></code></pre></div></div>

<p>You can also use the <code class="language-plaintext highlighter-rouge">find_optimal_tau</code> function to determine the optimal duration, which in this case is $\tau=18$.</p>

<h3 id="example-2-conversion-rates">Example 2: Conversion rates</h3>
<p>Let’s say your main metric is conversion rate. You think that on average conversion rates will be about 10%, and that the difference in conversion rates between buckets will be normally distributed with variance 1%. Using the normal approximation of the binomial distribution, you can use <code class="language-plaintext highlighter-rouge">p*(1-p)</code> for <code class="language-plaintext highlighter-rouge">var_X</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">get_agg_lift_via_closed_form</span><span class="p">(</span><span class="n">var_D</span><span class="o">=</span><span class="mf">0.01</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">var_X</span><span class="o">=</span><span class="n">p</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">),</span> <span class="n">m</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">365</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="c1"># returns 207
</span></code></pre></div></div>

<p>You can also use the <code class="language-plaintext highlighter-rouge">find_optimal_tau</code> function to determine the optimal duration, which in this case is $\tau=49$.</p>

<h2 id="faq">FAQ</h2>
<p><strong>Q:</strong> Has there been any similar work on this?</p>

<p><strong>A:</strong> As I was writing this, I came across a <a href="https://arxiv.org/pdf/1811.00457.pdf">fantastic in-press paper</a> by <a href="https://drexel.edu/now/experts/Overview/Feit-Elea/">Elea Feit</a> and <a href="https://www.ron-berman.com/">Ron Berman</a>. The paper is exceptionally clear and I would recommend reading it. Like this blog post, Feit and Berman argue that it doesn’t make any sense to pick sample sizes based on statistical significance and power thresholds. Instead they recommend profit-maximizing sample sizes. They independently come to the same formula for $ \hat{L} $ as I do (see right addend in their Equation 9, making sure to substitute my $\frac{\sigma_\Delta^2}{2}$ for their $\sigma^2)$. Where they differ is that they assume there is a fixed pool of $N$ users that can only experience the product once. In their setup, you can allocate $n_1$ users to Bucket A and $n_2$ users to Bucket B. Once you have identified the winning bucket, you ship that version to the remaining $N-n_1-n_2$ users. Your expected profit is determined by the expected lift from those users. My experience in industry differs from this setup. In my experience there is no constraint that you can only show the product once to a fixed set of users. Instead, there is often an indefinitely increasing pool of new users, and once you ship the winning bucket you can ship it to everyone, including users who already participated in the experiment. To me, the main constraint in industry is therefore time discounting, rather than a finite pool of users.</p>

<p><strong>Q:</strong> In addition to the lift from shipping a winning bucket, doesn’t experimentation also help inform you about the types of products that might work in the future? And if so, doesn’t this mean we should run experiments longer than recommended by your formula for $\hat{L}_a$?</p>

<p><strong>A:</strong> Yes, experimentation can teach lessons that are generalizable beyond the particular product being tested. This is an advantage of high powered experimentation not included in my framework.</p>

<p><strong>Q:</strong> If some users can show up in multiple sessions, doesn’t bucketing by session violate independence assumptions?</p>

<p><strong>A:</strong> Yeah, so this is tricky. For many companies, there is a distribution of user activity, where some users come for many sessions per week and other users come for only one session at most. Modeling this would make the framework significantly more complicated, so I tried to simplify things by making sessions the unit of analysis.</p>

<p><strong>Q:</strong> Is there anything else on your blog related to this topic?</p>

<p><strong>A:</strong> I’m glad you asked!</p>
<ul>
  <li><a href="/2016/02/28/four-pitfalls-of-hill-climbing/">Four pitfalls of hill climbing</a> discusses some product-focused issues in A/B testing</li>
  <li><a href="/2018/02/04/hyperbolic-discounting/">Hyperbolic discounting — The irrational behavior that might be rational after all</a> is about time discounting, although not in the constext of experimentation.</li>
</ul>

<h2 id="appendix">Appendix</h2>
<p>The aggregate time-discounted lift $\hat{L}_a$ is</p>

<script type="math/tex; mode=display">\hat{L}_a = \int_{\tau}^{\infty} \hat{L} M e^{-rt} \,dt</script>

<p>where $\hat{L}$ is the expected per-session lift, $M$ is the number of post-experiment sessions per day, $r$ is the discount rate, and $ \tau $ is the duration of the experiment. Solving the integral gives:</p>

<script type="math/tex; mode=display">\hat{L}_a = \frac{\hat{L} M e^{-r\tau}}{r}</script>

<p>Plugging in our previously solved value of $\hat{L}$ gives</p>

<script type="math/tex; mode=display">\hat{L}_a = \frac{\sigma_\Delta^2 M e^{-r\tau}}{r \sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{m\tau})}}</script>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-II/#disqus_thread" data-disqus-identifier="http://localhost:4000/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-II/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-II/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Optimizing sample sizes in A/B testing, Part II&#58; Aggregate time-discounted lift">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-I/">
        Optimizing sample sizes in A/B testing, Part I&#58; Expected lift
      </a>
    </h1>

    <span class="post-date">01 Jan 2020</span>

    <p><em>A special thanks to the excellent <a href="https://twitter.com/johnvmcdonnell">John McDonnell</a>, who came up with the idea for this post.</em></p>

<hr />

<p>If you’re a data scientist, you’ve surely encountered the question, “How big should this A/B test be?”</p>

<p>The standard answer is to do a power analysis, typically aiming for 80% power at $\alpha$=5%. But if you think about it, this advice is pretty weird. Why is 80% power the best choice for your business? And doesn’t a 5% significance cutoff seem pretty arbitrary?</p>

<p>In most business decisions, you want to choose a policy that maximizes your benefits minus your costs. In experimentation, the benefit comes from learning information to drive future decisions and the cost comes from the experiment itself. The optimal sample size will therefore depend on the unique circumstances of your business, not on arbitrary statistical significance thresholds.</p>

<p>In this blog post (Part I), I describe what I think is an incredibly useful business-focused formula that quantifies how much you can benefit from increasing your sample size. It is, in short, an <em>average of the value of all possible outcomes of the experiment, weighted by their probabilities</em>. Python helper functions are <a href="https://github.com/csaid/optimizing-sample-sizes-in-AB-testing">available</a>.</p>

<p>In <a href="/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-II/">Part II</a>, I’ll show how you can determine the costs of the experiment, focusing specifically on the forfeited benefits of waiting to ship the winning bucket. In particular, we’ll see how the optimal duration of an experiment depends on the discount rate of your project: How much you value accruing its benefits now rather than later. We’ll see how there are are many business scenarios where it is optimal to run “underpowered” experiments.</p>

<p>Throughout Part I and II, the focus will be on choosing a sample size at the beginning of the experiment and committing to it, not on dynamically updating the sample size as the experiment proceeds.</p>

<p>With that out of the way, let’s get started!</p>

<h2 id="outcome-probabilities">Outcome probabilities</h2>
<p><em>[This is the driest section. But if you can make it through, the rest gets easier.]</em></p>

<p>Imagine you are comparing two versions of a website. You currently are on version A, but you would like to compare it to version B. And imagine you are measuring some random variable $X$, which might represent something like clicks per user or page views per user. The goal of the experiment is to determine which version of the website has a higher mean value of $X$.</p>

<p>This blog post aims to quantify the benefit of experimentation as an average of the value of all possible outcomes, weighted by their probabilities. To do that, we first need to describe the probabilities of all the different outcomes. An outcome consists of two parts: A <em>true</em> difference in means, $\Delta$, defined as</p>

<script type="math/tex; mode=display">\Delta = \mu_B - \mu_A</script>

<p>and an experimentally <em>observed</em> difference in means $\delta$, defined as</p>

<script type="math/tex; mode=display">\delta = \overline{X}_B - \overline{X}_A</script>

<p>Let’s start with $\Delta$. While you don’t yet know which version of the website is better (that’s what the experiment is for!), you have a sense for how important the product change is. You can therefore create a normally distributed prior on $\Delta$ with mean zero and variance $ \sigma_\Delta^2 $.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/univariate_normal.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 1</strong>.
  </div>
</div>
<p><br /></p>

<p>Next, let’s consider $\delta$, your experimentally observed difference in means. It will be a noisy estimate of $\Delta$. Let’s assume you have previously measured the variance of $X$ to be $ \sigma_X^2 $. It is reasonable to assume that within each group in the experiment, and for any particular $\Delta$, the variance of $X$ will still be $ \sigma_X^2$. You should therefore believe that for any particular $\Delta$, the observed difference in means $\delta$ will be sampled from a normal distribution $\mathcal{N}(\Delta, \sigma_c^2)$, where</p>

<script type="math/tex; mode=display">\sigma_c^2 = \frac{2\sigma_X^2}{n}</script>

<p>and where $n$ is the sample size in each of the two buckets.</p>

<p>Collectively, this all forms a bivariate normal distribution of outcomes, shown below.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/bivariate_normal.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 2</strong>. Probabilities of possible outcomes, based on your prior beliefs.
  </div>
</div>
<p><br /></p>

<p>To gain some more intuiton about this, take a look at Figure 3. As sample size increases, $ \sigma^2_c $ decreases.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/three_bivariate_normals.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 3</strong>.
  </div>
</div>
<p><br /></p>

<h2 id="outcome-lifts">Outcome lifts </h2>
<p>Now that we know the probabilities of all the different outcomes, we next need to estimate how much per-user lift, $l$, we will gain from each possible outcome, assuming we follow a policy of shipping whichever bucket (A or B) looked better in the experiment.</p>

<ul>
  <li>In cases where $\delta &gt; 0$ and $\Delta &gt; 0$, you would ship B and your post-experiment per-user lift will be positively valued at $l = \Delta$.</li>
  <li>In cases where $\delta &gt; 0$ and $\Delta &lt; 0$, you would ship B, but unfortunately your post-experiment per-user lift will be negatively valued at $l = \Delta$.</li>
  <li>In cases where $\delta &lt; 0$, you would keep A in production, and your post-experiment lift would be zero.</li>
</ul>

<p>A heatmap of the per-user lifts ($l$) for each outcome is shown in the plot below. Good outcomes, where shipping B was the right decision, are shown in blue. Bad outcomes, where shipping B was the wrong decision, are shown in red.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/lift_matrix.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 4</strong>. Heatmap of possible outcomes, where the color scale represents the lift, $l$. 
  </div>
</div>
<p><br /></p>

<h2 id="probability-weighted-outcome-lifts">Probability-weighted Outcome Lifts</h2>
<p>At this point, we know the probability of each outcome, and we know the post-experiment per-user lift of each outcome. To determine how much lift we can expect, on average, by shipping the winning bucket of an experiment, we need to compute a probability-weighted average of the outcome lifts. Let’s start by looking at this visually and then later we’ll get into the math.</p>

<p>In Figure 5, the probability-weighted lift of each outcome (right) can be seen by multiplying the bivariate normal distribution (left) by the lift map (center). </p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/product_stages.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 5</strong>.
  </div>
</div>
<p><br /></p>

<p>The good outcomes contribute more than the bad outcomes, simply because a good outcome is more likely than a bad outcome. To put it differently, experimentation will on average give you useful information.</p>

<p>To gain some more intuition on this, it is helpful to see this plot for different sample sizes. As sample size increases, the probability-weighted contribution of bad outcomes gets smaller and smaller. </p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/three_products.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 6</strong>.
  </div>
</div>
<p><br /></p>

<h3 id="computing-the-expected-post-experiment-per-user-lift">Computing the expected post-experiment per-user lift</h3>
<p>To determine the expected post-experiment lift from shipping the winning bucket, we need compute a probability-weighted average of all the post-experiment lifts. In other words, we need to sum up all the probability-weighted post-experiment lifts on the right panel of Figure 5. The formula for doing this is shown below. A derivation can be found in the Appendix.</p>

<script type="math/tex; mode=display">\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{n})}}</script>

<p>There’s three things to notice about this formula.</p>
<ul>
  <li>As $n$ increases, the $\hat{L}$ increases. This makes sense. The larger the sample size, the more likely it is that you’ll ship the winning bucket.</li>
  <li>As the within-group variance $\sigma_X^2$ increases, $\hat{L}$ increases. That’s because a high within-group variance makes experiments less informative – they’re more likely to give you the wrong answer.</li>
  <li>As the variance prior on $\Delta$ increases, $\hat{L}$ increases. This also make sense. The more impactful (positive or negative) you think the product change might be, the more value you will get from experimentation.</li>
</ul>

<p>You can try this out using the <code class="language-plaintext highlighter-rouge">get_lift_via_closed_form</code> formula in the <a href="https://github.com/csaid/optimizing-sample-sizes-in-AB-testing/blob/master/Optimizing%20sample%20sizes%20in%20AB%20testing.ipynb">Notebook</a></p>

<h2 id="demonstration-via-simulation">Demonstration via simulation</h2>
<p>In the previous section, we derived a formula for $\hat{L}$. Should you trust a formula you found on a random internet blog? Yes! Let’s put the formula to the test, by comparing its predictions to actual simulations.</p>

<p>First, let’s consider the case where the outcome is a continuous variable, such as the number of clicks. Let’s set $ \sigma_D^2 = 2 $ and $ \sigma_X^2 = 100 $. We then measure $\hat{L}$ for a range of sample sizes, using both the closed-form solution and simulations. In each simulated experiment, we do the following:</p>
<ol>
  <li>Sample a true group difference $\Delta$ from $\mathcal{N}(0, \sigma_D^2)$</li>
  <li>Sample an $X$ for each of the $n$ users in each bucket A and B, using Normal distributions $\mathcal{N}(\frac{\Delta}{2}, \sigma_X^2)$ and $\mathcal{N}(-\frac{\Delta}{2}, \sigma_X^2)$, respectively.</li>
  <li>Compute $ \delta = \overline{X}_B - \overline{X}_A $.</li>
  <li>If $\delta &lt;= 0$, stick with A and accrue zero lift.</li>
  <li>If $\delta &gt; 0$, ship B and accrue the per-user lift of $\Delta$, which will probably, but not necessarily, be positive.</li>
</ol>

<p>We run these experiments thousands of times, each time computing the per-user lift. Finally, we average all the per-user lifts together.</p>

<p>As seen in Figure 7, below, the results of the simulation closely match the closed-form solution.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/lift_by_n_continuous.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 7</strong>.
  </div>
</div>
<p><br /></p>

<p>Second, let’s consider the case where the variable is binary, as in conversion rates. For reasonably large values of $ n $, we can safely assume that the error variance is normally distributed with variance $ \sigma_X^2 = p(1-p) $, where $ p $ is the baseline conversion rate. For this example, let’s set the baseline conversion rate $p = 0.1$, and let’s set $ \sigma_\Delta^2 = 0.01^2 $. The results of the simulation closely match the closed-form solution.</p>

<div class="wrapper">
  <img src="/assets/2020_optimizing_sample_sizes/lift_by_n_binary.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 8</strong>.
  </div>
</div>
<p><br /></p>

<h2 id="thinking-about-costs-and-a-preview-of-part-ii">Thinking about costs, and a preview of Part II</h2>

<p>In this blog post, we saw how increasing the sample size improves the expected post-experiment per-user lift, $\hat{L}$. But to determine the <em>optimal</em> sample size, we need to think about costs.</p>

<p>The cost in dollars of an experiment can be described as $f + vn$, where $f$ is a fixed cost and $ v $ is the variable cost per participant. If you already know these costs, and if you already know the revenue increase $ u $ from each unit increase in lift, you can calculate the net revenue $R$ as</p>

<script type="math/tex; mode=display">R = u\hat{L}  - f - vn</script>

<p>and then find the sample size $ n $ that maximizes $ R $.</p>

<p>Unfortunately, these costs aren’t always readily available. The good news is that there is a really nice way to calculate the most important cost: the forfeited benefit that comes from prolonging your experiment. To read about that, and about how to optimize your sample size, please continue to <a href="/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-II/">Part II</a>.</p>

<h2 id="appendix">Appendix</h2>
<p>To determine $\hat{L}$, we start with the probability-weighted lifts on the right panel of Figure 5. This is a bivariate normal distribution over $ \Delta $ and $ \delta $, multiplied by $ \Delta $.</p>

<script type="math/tex; mode=display">f(\Delta, \delta) = \frac{\Delta}{2 \pi \sigma_\Delta \sigma_\delta \sqrt{1-\rho^2}} e^{-\frac{
\frac{\Delta^2}{\sigma_\Delta^2} - \frac{2 \rho \Delta \delta}{\sigma_\Delta \sigma_\delta} + \frac{\delta^2}{\sigma_\delta^2}
}{2(1-\rho^2)}
}</script>

<p>where the correlation coefficient $ \rho $, is <a href="http://athenasc.com/Bivariate-Normal.pdf">defined</a> as:</p>

<script type="math/tex; mode=display">\rho = \sqrt{1 - \frac{\sigma_c^2}{\sigma_\Delta^2 + \sigma_c^2}}</script>

<p>and $\sigma_\delta^2$, is the variance on $\delta$. By the <a href="/2019/05/18/variance_after_scaling_and_summing/">variance addition rules</a>, $\sigma_\delta^2$ is defined as</p>

<script type="math/tex; mode=display">\sigma_\delta^2 = \sigma_\Delta^2 + \sigma_c^2</script>

<p>We next need to sum up the probability-weighted values in $f(\Delta, \delta)$. We can use integration to obtain a closed form solution.</p>

<script type="math/tex; mode=display">\hat{L} = \int_{0}^{\infty} \int_{-\infty}^{\infty} {\frac{\Delta}{2 \pi \sigma_\Delta \sigma_\delta \sqrt{1-\rho^2}} e^{-\frac{

\frac{\Delta^2}{\sigma_\Delta^2} - \frac{2 \rho \Delta \delta}{\sigma_\Delta \sigma_\delta} + \frac{\delta^2}{\sigma_\delta^2}

}{2(1-\rho^2)}

}
\,d\Delta\,d\delta
}</script>

<p>The integration limits on $ \delta $ start at zero because the lift will always be zero if $ \delta &lt; 0 $ (i.e if the status quo bucket A wins the experiment).</p>

<p>Thanks to my 15-day free trial of <a href="https://www.wolfram.com/mathematica/">Mathematica</a>, I determined that this integral comes out to the surprisingly simple </p>

<script type="math/tex; mode=display">\hat{L} = \rho \frac{\sigma_\Delta}{\sqrt{2\pi}}</script>

<p>The command I used was:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Integrate[(t / (2*\[Pi]*s1*s2*Sqrt[1 - p^2]))*Exp[-((t^2/s1^2 - \
(2*p*t*d)/(s1*s2) + d^2/s2^2)/(2*(1 - p^2)))], {d, 0, \[Infinity]}, \
{t, -\[Infinity], \[Infinity]}, Assumptions -&gt; p &gt; 0 &amp;&amp; p &lt; 1 &amp;&amp; s1 &gt; \
0 &amp;&amp; s2 &gt; 0]
</code></pre></div></div>

<p>If we then substitute in previously defined formulas for $ \rho $ and $ \sigma_c^2 $, we can produce a formula that accepts more readily-available inputs.</p>

<script type="math/tex; mode=display">\hat{L} = \frac{\sigma_\Delta^2}{\sqrt{2\pi(\sigma_\Delta^2 + \frac{2\sigma_X^2}{n})}}</script>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-I/#disqus_thread" data-disqus-identifier="http://localhost:4000/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-I/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2020/01/01/optimizing-sample-sizes-in-ab-testing-part-I/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Optimizing sample sizes in A/B testing, Part I&#58; Expected lift">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2019/05/18/variance_after_scaling_and_summing/">
        Variance after scaling and summing&#58; One of the most useful facts from statistics
      </a>
    </h1>

    <span class="post-date">18 May 2019</span>

    <p>What do $ R^2 $, laboratory error analysis, ensemble learning, meta-analysis, and financial portfolio risk all have in common? The answer is that they all depend on a fundamental principle of statistics that is not as widely known as it should be. Once this principle is understood, a lot of stuff starts to make more sense.</p>

<p>Here’s a sneak peek at what the principle is.</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij}</script>

<p>Don’t worry if the formula doesn’t yet make sense! We’ll work our way up to it slowly, taking pit stops along the way at simpler formulas are that useful on their own. As we work through these principles, we’ll encounter lots of neat applications and explainers.</p>

<p>This post consists of three parts:</p>

<ul>
  <li><strong>Part 1</strong>: Sums of uncorrelated random variables: Applications to social science and laboratory error analysis</li>
  <li><strong>Part 2</strong>: Weighted sums of uncorrelated random variables: Applications to machine learning and scientific meta-analysis</li>
  <li><strong>Part 3</strong>: Correlated variables and Modern Portfolio Theory</li>
</ul>

<h2 id="part-1-sums-of-uncorrelated-random-variables-applications-to-social-science-and-laboratory-error-analysis">Part 1: Sums of uncorrelated random variables: Applications to social science and laboratory error analysis</h2>

<p>Let’s start with some simplifying conditions and assume that we are dealing with <em>uncorrelated</em> random variables. If you take two of them and add them together, the variance of their sum will equal the sum of their variances. This is amazing!</p>

<p>To demonstrate this, I’ve written some Python code that generates three arrays, each of length 1 million. The first two arrays contain samples from two normal distributions with variances 9 and 16, respectively. The third array is the sum of the first two arrays. As shown in the simulation, its variance is 25, which is equal to the sum of the variances of the first two arrays (9 + 16).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># 1M samples from normal distribution with variance=9
</span><span class="k">print</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 9
</span><span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># 1M samples from normal distribution with variance=16
</span><span class="k">print</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 16
</span><span class="n">xp</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span>
<span class="k">print</span><span class="p">(</span><span class="n">xp</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 25
</span></code></pre></div></div>

<p>This <a href="https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)">fact</a> was first discovered in 1853 and is known as <a href="https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)">Bienaymé’s Formula</a>. While the code example above shows the sum of two random variables, the formula can be extended to multiple random variables as follows:</p>

<div class="box">
If $ X_p $ is a sum of uncorrelated random variables $ X_1 .. X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum{\sigma^2_i} $$

where each $ X_i $ has variance $ \sigma_i^2 $.
</div>

<p>What does the $ p $ stand for in $ X_p $? It stands for <em>portfolio</em>, which is just one of the many applications we’ll see later in this post.</p>

<h3 id="why-this-is-useful">Why this is useful</h3>
<p>Bienaymé’s result is surprising and unintuitive. But since it’s such a simple formula, it is worth committing to memory, especially because it sheds light on so many other principles. Let’s look at two of them.</p>

<h4 id="understanding--r2--and-variance-explained">Understanding $ R^2 $ and “variance explained”</h4>
<p>Psychologists often talk about “within-group variance”, “between-group variance”, and “variance explained”. What do these terms mean?</p>

<p>Imagine a hypothetical study that measured the extraversion of 10 boys and 10 girls, where extraversion is measured on a 10-point scale (<em>Figure 1</em>. Orange bars). The boys have a mean extraversion of 4.4 and the girls have a mean extraversion 5.0. In addition, the overall variance of the data is 2.5.  We can decompose this variance into two parts:</p>

<ul>
  <li><strong>Between-group variance</strong>: Create a 20-element array where every boy is assigned to the mean boy extraversion of 4.4, and every girl is assigned to the mean girl extraversion of 5.0. The variance of this array is 0.9. (<em>Figure 1</em>. Blue bars).</li>
  <li><strong>Within-group variance</strong>: Create a 20-element array of the amount each child’s extraversion deviates from the mean value for their sex. Some of these values will be negative and some will be positive. The variance of this array is 1.6. (<em>Figure 1</em>. Pink bars).</li>
</ul>

<div class="wrapper">
  <img src="/assets/2019_variance/boy_girl_extraversion.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption"><strong>Figure 1</strong>: Decomposition of extraversion scores (orange) into between-group variance (blue) and within-group variance (pink).
  </div>
</div>
<p><br /></p>

<p>If you add these arrays together, the resulting array will represent the observed data (<em>Figure 1</em>. Orange bars). The variance of the observed array is 2.5, which is exactly what is predicted by Bienaymé’s Formula. It is the sum of the variances of the two component arrays (0.9 + 1.6). Psychologists might say that sex “explains” 0.9/2.5 = 36% of the extraversion variance. Equivalently, a model of extraversion that uses sex as the only predictor would have an <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">$ R^2 $</a> of 0.36.</p>

<h4 id="error-propagation-in-laboratories">Error propagation in laboratories</h4>
<p>If you ever took a physics lab or chemistry lab back in college, you may remember having to perform <a href="http://ipl.physics.harvard.edu/wp-uploads/2013/03/PS3_Error_Propagation_sp13.pdf">error analysis</a>, in which you calculated how errors would propagate through one noisy measurement after another.</p>

<p>Physics textbooks often say that standard deviations add in “quadrature”, which just means that if you are trying to estimate some quantity that is the sum of two other measurements, and if each measurement has some error with standard deviation <script type="math/tex">\sigma_1</script> and <script type="math/tex">\sigma_2</script> respectively, the final standard deviation would be  <script type="math/tex">\sigma_{p} = \sqrt{\sigma^2_1 + \sigma^2_2}</script>. I think it’s probably easier to just use variances, as in the Bienaymé Formula, with <script type="math/tex">\sigma^2_{p} = \sigma^2_1 + \sigma^2_2</script>.</p>

<p>For example, imagine you are trying to estimate the height of two boxes stacked on top of each other (<em>Figure 2</em>). One box has a height of 1 meter with variance $ \sigma^2_1 $ = 0.01, and the other has a height of 2 meters with variance $ \sigma^2_2 $ = 0.01. Let’s further assume, perhaps optimistically, that these errors are independent. That is, if the measurement of the first box is too high, it’s not any more likely that the measurement of the second box will also be too high. If we can make these assumptions, then the total height of the two boxes will be 3 meters with variance $ \sigma^2_p $ = 0.02.</p>

<div class="wrapper">
  <img src="/assets/2019_variance/stacked_boxes.png" class="inner" style="position:relative border: #222 2px solid; max-width:50%;" />
  <div class="caption"><strong>Figure 2</strong>: Two boxes stacked on top of each other. The height of each box is measured with some variance (uncertainty). The total height is the the sum of the individual heights, and the total variance (uncertainty) is the sum of the individual variances.
  </div>
</div>
<p><br /></p>

<p>There is a key difference between the extraversion example and the stacked boxes example. In the extraversion example, we added two <em>arrays</em> that each had an observed sample variance. In the stacked boxes example, we added two <em>scalar measurements</em>, where the variance of these measurements refers to our measurement uncertainty. Since both cases have a meaningful concept of ‘variance’, the Bienaymé Formula applies to both.</p>

<h2 id="part-2-weighted-sums-of-uncorrelated-random-variables-applications-to-machine-learning-and-scientific-meta-analysis">Part 2: Weighted sums of uncorrelated random variables: Applications to machine learning and scientific meta-analysis</h2>

<p>Let’s now move on to the case of <em>weighted</em> sums of uncorrelated random variables. But before we get there, we first need to understand what happens to variance when a random variable is scaled.</p>

<div class="box">
If $ X_p $ is defined as $ X $ scaled by a factor of $ w $, then the variance $ X_p $ will be

$$ \sigma_{p}^{2} = w^2 \sigma^2 $$

where $ \sigma^2 $ is the variance of $ X $.
</div>

<p>This means that if a random variable is scaled, the scale factor on the variance will change <em>quadratically</em>. Let’s see this in code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">baseline_var</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">w</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">baseline_var</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># Array of 1M samples from normal distribution with variance=10
</span><span class="k">print</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 10
</span><span class="n">xp</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x1</span> <span class="c1"># Scale this by w=0.7
</span><span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">baseline_var</span><span class="p">)</span> <span class="c1"># 4.9 (predicted variance)
</span><span class="k">print</span><span class="p">(</span><span class="n">xp</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 4.9 (empirical variance) 
</span></code></pre></div></div>
<p>To gain some intuition for this rule, it’s helpful to think about outliers. We know that outliers have a huge effect on variance. That’s because the formula used to compute variance, $ \sum{\frac{(x_i - \bar{x})^2}{n-1}} $, squares all the deviations, and so we get really big variances when we square large deviations. With that as background, let’s think about what happens if we scale our data by 2. The outliers will spread out twice as far, which means they will have even more than twice as much impact on the variance. Similarly, if we multiply our data by 0.5, we will squash the most “damaging” part of the outliers, and so we will reduce our variance by more than a factor of two.</p>

<p>While the above principle is pretty simple, things start to get interesting when you combine it with the Bienaymé Formula in Part I:</p>

<div class="box">
If $ X_p $ is a weighted sum of uncorrelated random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be 

$$ \sigma_{p}^{2} = \sum{w^2_i \sigma^2_i} $$

where each $ w_i $ is a weight on $ X_i $, and each $ X_i $ has its own variance $ \sigma_i^2 $.
</div>

<p>The above formula shows what happens when you scale and then sum random variables. The final variance is the weighted sum of the original variances, where the weights are squares of the original weights. Let’s see how this can be applied to machine learning.</p>

<h3 id="an-ensemble-model-with-equal-weights">An ensemble model with equal weights</h3>

<p>Imagine that you have built two separate models to predict car prices. While the models are unbiased, they have variance in their errors. That is, sometimes a model prediction will be too high, and sometimes a model prediction will be too low. Model 1 has a mean squared error (MSE) of \$1,000 and Model 2 has an MSE of \$2,000.</p>

<p>A valuable insight from machine learning is that you can often create a better model by simply averaging the predictions of other models. Let’s demonstrate this with simulations below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">actual</span> <span class="o">=</span> <span class="mi">20000</span> <span class="o">+</span> <span class="mi">5000</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">errors1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">errors1</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 1000
</span><span class="n">errors2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span> <span class="o">*</span> <span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">errors2</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 2000
</span>
<span class="c1"># Note that this section could be replaced with 
# errors_ensemble = 0.5 * errors1 + 0.5 * errors2
</span><span class="n">preds1</span> <span class="o">=</span> <span class="n">actual</span> <span class="o">+</span> <span class="n">errors1</span>
<span class="n">preds2</span> <span class="o">=</span> <span class="n">actual</span> <span class="o">+</span> <span class="n">errors2</span>
<span class="n">preds_ensemble</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">preds1</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">preds2</span>
<span class="n">errors_ensemble</span> <span class="o">=</span> <span class="n">preds_ensemble</span> <span class="o">-</span> <span class="n">actual</span>

<span class="k">print</span><span class="p">(</span><span class="n">errors_ensemble</span><span class="o">.</span><span class="n">var</span><span class="p">())</span> <span class="c1"># 750. Lower than variance of component models!
</span></code></pre></div></div>

<p>As shown in the code above, even though a good model (Model 1) was averaged with an inferior model (Model 2), the resulting Ensemble model’s MSE of \$750 is better than either of the models individually.</p>

<p>The benefits of ensembling follow directly from the weighted sum formula we saw above, <script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>. To understand why, it’s helpful to think of models not as generating predictions, but rather as generating errors. Since averaging the predictions of a model corresponds to averaging the errors of the model, we can treat each model’s array of errors as samples of a random variable whose variance can be plugged in to the formula. Assuming the models are unbiased (i.e. the errors average to about zero), the formula tells us the expected MSE of the ensemble predictions. In the example above, the MSE would be</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = 0.5^2 \times 1000 + 0.5^2 \times 2000 = 750</script>

<p>which is exactly what we observed in the simulations.</p>

<p>(For a totally different intuition of why ensembling works, see <a href="https://www.opendoor.com/w/blog/why-ensembling-works-the-intuition-behind-opendoors-home-pricing">this blog post</a> that I co-wrote for my company, Opendoor.)</p>

<h3 id="an-ensemble-model-with-inverse-variance-weighting">An ensemble model with Inverse Variance Weighting</h3>

<p>In the example above, we obtained good results by using an equally-weighted average of the two models. But can we do better?</p>

<p>Yes we can! Since Model 1 was better than Model 2, we should probably put more weight on Model 1. But of course we shouldn’t put all our weight on it, because then we would throw away the demonstrably useful information from Model 2. The optimal weight must be somewhere in between 50% and 100%.</p>

<p>An effective way to find the optimal weight is to <a href="http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/">build another model on top of these models</a>. However, if you can make certain assumptions (unbiased and uncorrelated errors), there’s an even simpler approach that is great for back-of-the envelope calculations and great for understanding the principles behind ensembling.</p>

<p>To find the optimal weights (assuming unbiased and uncorrelated errors), we need to minimize the variance of the ensemble errors
<script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>
with the constraint that 
<script type="math/tex">\sum{w_i} = 1</script>.</p>

<p>It <a href="https://en.wikipedia.org/wiki/Inverse-variance_weighting">turns out</a> that the variance-minimizing weight for a model should be proportional to the inverse of its variance.</p>

<script type="math/tex; mode=display">w_k = \frac{\frac{1}{\sigma^2_k}}{\sum{\frac{1}{\sigma^2_i}}}</script>

<p>When we apply this method, we obtain optimal weights of <script type="math/tex">w_1</script> = 0.67 and <script type="math/tex">w_2</script> = 0.33. These weights give us an ensemble error variance of</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = 0.67^2 \times 1000 + 0.33^2 \times 2000 = 666</script>

<p>which is significantly better than the $750 variance we were getting with equal weighting.</p>

<p>This method is called <a href="https://en.wikipedia.org/wiki/Inverse-variance_weightinghttps://en.wikipedia.org/wiki/Inverse-variance_weighting">Inverse Variance Weighting</a>, and allows you to assign the right amount of weight to each model, depending on its error.</p>

<p>Inverse Variance Weighting is not just useful as a way to understand Machine Learning ensembles. It is also one of the core principles in scientific <a href="https://en.wikipedia.org/wiki/Meta-analysis">meta-analysis</a>, which is popular in medicine and the social sciences. When multiple scientific studies attempt to estimate some quantity, and each study has a different sample size (and hence variance of their estimate), a meta-analysis should weight the high sample size studies more. Inverse Variance Weighting is used to determine those weights.</p>

<h2 id="part-3-correlated-variables-and-modern-portfolio-theory">Part 3: Correlated variables and Modern Portfolio Theory</h2>

<p>Let’s imagine we now have three unbiased models with the following MSEs:</p>

<ul>
  <li>Model 1: MSE = 1000</li>
  <li>Model 2: MSE = 1000</li>
  <li>Model 3: MSE = 2000</li>
</ul>

<p>By Inverse Variance Weighting, we should assign more weight to the first two models, with <script type="math/tex">w_1=0.4, w_2=0.4, w_3=0.2</script>.</p>

<p>But what happens if Model 1 and Model 2 have correlated errors? For example, whenever Model 2’s predictions are too high, Model 3’s predictions tend to also be too high. In that case, maybe we don’t want to give so much weight to Models 1 and 2, since they provide somewhat redundant information. Instead we might want to <em>diversify</em> our ensemble by increasing the weight on Model 3, since it provides new independent information.</p>

<p>To determine how much weight to put on each model, we first need to determine how much total variance there will be if the errors are correlated. To do this, we need to borrow a <a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory">formula</a> from the financial literature, which extends the formulas we’ve worked with before. This is the formula we’ve been waiting for.</p>

<div class="box">
If $ X_p $ is a weighted sum of (correlated or uncorrelated) random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij} $$

where each $ w_i $ and $ w_j $ are weights assigned to $ X_i $ and $ X_j $, where each $ X_i $ and $ X_j $ have standard deviations $ \sigma_i $ and $ \sigma_j $, and where the correlation between $ X_i $ and $ X_j $ is $ \rho_{ij} $.
</div>

<p>There’s a lot to unpack here, so let’s take this step by step.</p>

<ul>
  <li><script type="math/tex">\sigma_i \sigma_j \rho_{ij}</script> is a scalar quantity representing the covariance between <script type="math/tex">X_i</script> and <script type="math/tex">X_j</script>.</li>
  <li>If none of the variables are correlated with each other, then all the cases where $ i \neq j $ will go to zero, and the formula reduces to <script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>, which we have seen before.</li>
  <li>The more that two variables <script type="math/tex">X_i</script> and <script type="math/tex">X_j</script> are correlated, the more the total variance <script type="math/tex">\sigma_{p}^{2}</script> increases.</li>
  <li>If two variables <script type="math/tex">X_i</script> and <script type="math/tex">X_j</script> are anti-correlated, then the total variance decreases, since <script type="math/tex">\sigma_i \sigma_j \rho_{ij}</script> is negative.</li>
  <li>This formula can be rewritten in more compact notation as <script type="math/tex">\sigma_{p}^{2} = \vec{w}^T\Sigma \vec{w}</script>, where <script type="math/tex">\vec{w}</script> is the weight vector, and <script type="math/tex">\Sigma</script> is the covariance matrix (not a summation sign!)</li>
</ul>

<p>If you skimmed the bullet points above, go back and re-read them! They are super important.</p>

<p>To find the set of weights that minimize variance in the errors, you must minimize the above formula, with the constraint that <script type="math/tex">\sum{w_i} = 1</script>. One way to do this is to use a <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize">numerical optimization method</a>. In practice, however, it is more common to just find weights by <a href="http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/">building another model on top of the base models</a></p>

<p>Regardless of how the weights are found, it will usually be the case that if Models 1 and 2 are correlated, the optimal weights will reduce redundancy and put lower weight on these models than simple Inverse Variance Weighting would suggest.</p>

<h3 id="applications-to-financial-portfolios">Applications to financial portfolios</h3>

<p>The formula above was discovered by economist <a href="https://en.wikipedia.org/wiki/Harry_Markowitz">Harry Markowitz</a> in his <a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory">Modern Portfolio Theory</a>, which describes how an investor can optimally trade off between expected returns and expected risk, often measured as variance. In particular, the theory shows how to maximize expected return given a fixed variance, or minimize variance given a fixed expected return. We’ll focus on the latter.</p>

<p>Imagine you have three stocks to put in your portfolio. You plan to sell them at time $ T $, at which point you expect that Stock 1 will have gone up by 5%, with some uncertainty. You can describe your uncertainty as variance, and in the case of Stock 1, let’s say <script type="math/tex">\sigma_1^2</script> = 1. This stock, as well Stocks 2 and 3, are summarized in the table below:</p>

<table>
  <thead>
    <tr>
      <th>Stock ID</th>
      <th>Expected Return</th>
      <th>Expected Risk (<script type="math/tex">\sigma^2</script>)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>5.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>5.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>5.0</td>
      <td>2.0</td>
    </tr>
  </tbody>
</table>

<p>This financial example should remind you of ensembling in machine learning. In the case of ensembling, we wanted to minimize variance of the weighted sum of error arrays. In the case of financial portfolios, we want to minimize the variance of the weighted sum of scalar financial returns.</p>

<p>As before, if there are no correlations between the expected returns (i.e. if Stock 1 exceeding 5% return does not imply that Stock 2 or Stock 3 will exceed 5% return), then the total variance in the portfolio will be
<script type="math/tex">\sigma_{p}^{2} = \sum{w^2_i \sigma^2_i}</script>
and we can use Inverse Variance Weighting to obtain weights $ w_1=0.4, w_2=0.4, w_3=0.2 $.</p>

<p>However, sometimes stocks have correlated expected returns. For example, if two of the stocks are in oil companies, then one stock exceeding 5% implies the other is also likely to exceed 5%. When this happens, the total variance becomes</p>

<script type="math/tex; mode=display">\sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij}</script>

<p>as we saw before in the ensemble example. Since this includes an additional positive term for <script type="math/tex">w_1 w_2 \sigma_1 \sigma_2 \rho_{1,2}</script>, the expected variance is higher than in the uncorrelated case, assuming the correlations are positive. To reduce this variance, we should put less weight on Stocks 1 and 2 than we would otherwise.</p>

<p>While the example above focused on minimizing the variance of a financial portfolio, you might also be interested in having a portfolio with high return. Modern Portfolio Theory describes how a portfolio can reach any abitrary point on the <a href="https://en.wikipedia.org/wiki/Efficient_frontier">efficient frontier</a> of variance and return, but that’s outside the scope of this blog post. And as you might expect, financial markets can be more complicated than Modern Portfolio Theory suggests, but that’s also outside scope.</p>

<h2 id="summary">Summary</h2>

<p>That was a long post, but I hope that the principles described have been informative. It may be helpful to summarize them in <em>backwards</em> order, starting with the most general principle.</p>

<div class="box">

If $ X_p $ is a weighted sum of (correlated or uncorrelated) random variables $ X_1 ... X_n $, then the variance of $ X_p $ will be

$$ \sigma_{p}^{2} = \sum\limits_{i} \sum\limits_{j} w_i w_j \sigma_i \sigma_j \rho_{ij} $$

where each $ w_i $ and $ w_j $ are weights assigned to $ X_i $ and $ X_j $, where each $ X_i $ and $ X_j $ have standard deviations $ \sigma_i $ and $ \sigma_j $, and where the correlation between $ X_i $ and $ X_j $ is $ \rho_{ij} $. The term $ \sigma_i \sigma_j \rho_{ij} $ is a scalar quantity representing the covariance between $ X_i $ and $ X_j $.

<br /><br />If none of the variables are correlated, then all the cases where $ i \neq j $ go to zero, and the formula reduces to 

$$ \sigma_{p}^{2} = \sum{w^2_i \sigma^2_i} $$

And finally, if we are computing a simple sum of random variables where all the weights are 1, then the formula reduces to

$$ \sigma_{p}^{2} = \sum{\sigma^2_i} $$
</div>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2019/05/18/variance_after_scaling_and_summing/#disqus_thread" data-disqus-identifier="http://localhost:4000/2019/05/18/variance_after_scaling_and_summing/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2019/05/18/variance_after_scaling_and_summing/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Variance after scaling and summing&#58; One of the most useful facts from statistics">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2018/08/06/cone-of-confusion/">
        Using your ears and head to escape the Cone Of Confusion
      </a>
    </h1>

    <span class="post-date">06 Aug 2018</span>

    <p>One of coolest things I ever learned about sensory physiology is how the auditory system is able to locate sounds. To determine whether sound is coming from the right or left, the brain uses inter-ear differences in amplitude and timing. As shown in the figure below, if the sound is louder in the right ear compared to the left ear, it’s probably coming from the right side. The smaller that difference is, the closer the sound is to the midline (i.e the vertical plane going from your front to your back). Similarly, if the sound arrives at your right ear before the left ear, it’s probably coming from the right. The smaller the timing difference, the closer it is to the midline. There’s a <a href="https://en.wikipedia.org/wiki/Coincidence_detection_in_neurobiology#Sound_localization">fascinating</a> <a href="https://nba.uth.tmc.edu/homepage/cnjclub/2004spring/McAlpineGrothe2003.pdf">literature</a> on the neural mechanisms behind this.</p>
<div class="wrapper">
  <img src="/assets/2018_cone_of_confusion/right_and_front_right.png" class="inner" style="position:relative border: #222 2px solid; max-width:50%;" />
</div>
<p>Inter-ear loudness and timing differences are pretty useful, but unfortunately they still leave a lot of ambiguity. For example, a sound from your front right will have the exact same loudness differences and timing differences as a sound from your back right.</p>
<div class="wrapper">
  <img src="/assets/2018_cone_of_confusion/front_right_and_back_right.png" class="inner" style="position:relative border: #222 2px solid; max-width:50%;" />
</div>
<p>Not only does this system leave ambiguities between front and back, it also leaves ambiguities between top and down. In fact, there is an entire <em>cone of confusion</em> that cannot be disambiguated by this system. Sound from all points along the surface of the cone will have the same inter-ear loudness differences and timing differences.</p>
<div class="wrapper">
  <img src="/assets/2018_cone_of_confusion/cone_of_confusion.png" class="inner" style="position:relative border: #222 2px solid; max-width:50%;" />
</div>
<p>While this system leaves a cone of confusion, humans are still able to determine the location of sounds from different points on the cone, at least to some extent. How are we able to do this?</p>

<p>Amazingly, we are able to do this because of the shape of our ears and heads. When sound passes through our ears and head, certain frequencies are attenuated more than others. Critically, the attenuation pattern is highly dependent on sound direction.</p>

<p>This location-dependent attenuation pattern is called a <a href="https://en.wikipedia.org/wiki/Head-related_transfer_function">Head-related transfer function</a> (HRTF) and in theory this could be used to disambiguate locations along the cone of confusion. An example of someone’s HRTF is shown below, with frequency on the horizontal axis and polar angle on the vertical axis. Hotter colors represent less attenuation (i.e. more power). If your head and ears gave you this HRTF, you might decide a sound is coming from the front if it has more high frequency power than you’d expect.</p>
<div class="wrapper">
  <img src="/assets/2018_cone_of_confusion/cone_and_hrtf.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
  <div class="caption">
    HRTF image from Simon Carlile's <a href="https://sonification.de/handbook/download/TheSonificationHandbook-chapter3.pdf">Psychoacoustics chapter</a> in The Sonification Handbook.
  </div>
</div>
<p><br />
This system sounds good in theory, but do we actually use these cues in practice? In 1988, Frederic Wightman and Doris Kistler performed an ingenious set of experiments (<a href="http://public.vrac.iastate.edu/~charding/audio/Headphone%20simulation%20of%20free-field%20listening.%20I-%20Stimulus%20synthesis%20-%20J%20Acoust%20Soc%20Am%201989%20-%20Wightman.pdf">1</a>, <a href="http://public.vrac.iastate.edu/~charding/audio/Headphone%20simulation%20of%20free-field%20listening.%20II-%20Psychophysical%20validation%20-%20J%20Acoust%20Soc%20Am%201989%20-%20Wightman.pdf">2</a>) to show that that people really do use HRTFs to infer location. First, they measured the HRTF of each participant by putting a small microphone in their ears and playing sounds from different locations. Next they created a digital filter for each location and each participant. That is to say, these filters implemented each participant’s HRTF. Finally, they placed headphones on the listeners and played sounds to them, each time passing the sound through one of the digital filters. Amazingly, participants were able to correctly guess the “location” of the sound, depending on which filter was used, even though the sound was coming from headphones. They were also much better at sound localization when using their own HRTF, rather than someone else’s HRTF.</p>

<p>Further evidence for this hypothesis comes from <a href="https://doi.org/10.1038/1633">Hofman et al., 1998</a>, who showed that by using putty to reshape people’s ears, they were able to change the HRTFs and thus disrupt sound localization. Interestingly, people were able to quickly relearn how to localize sound with their new HRTFs.</p>
<div class="wrapper">
  <img src="/assets/2018_cone_of_confusion/putty.png" class="inner" style="position:relative border: #222 2px solid; max-width:50%;" />
  <div class="caption">
    Image from <a href="https://doi.org/10.1038/1633">Hofman et al., 1998</a>.
  </div>
</div>
<p><br /></p>

<p>A final fun fact: to improve the sound localization of humanoid robots, researchers in Japan attached <a href="https://doi.org/10.1007/s10489-014-0544-y">artificial ears to the robot heads</a> and implemented some sophisticated algorithms to infer sound location. Here are some pictures of the robots.</p>
<div class="wrapper">
  <img src="/assets/2018_cone_of_confusion/robots.png" class="inner" style="position:relative border: #222 2px solid; max-width:50%;" />
</div>
<p>Their paper is kind of ridiculous and has some questionable justifications for not just using microphones in multiple locations, but I thought it was fun to see these principles being applied.</p>



    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2018/08/06/cone-of-confusion/#disqus_thread" data-disqus-identifier="http://localhost:4000/2018/08/06/cone-of-confusion/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2018/08/06/cone-of-confusion/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Using your ears and head to escape the Cone Of Confusion">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2018/02/04/hyperbolic-discounting/">
        Hyperbolic discounting — The irrational behavior that might be rational after all
      </a>
    </h1>

    <span class="post-date">04 Feb 2018</span>

    <p></p>
<p>When I was in grad school I occasionally overheard people talk about how humans do something called “hyperbolic discounting”. Apparently, hyperbolic discounting was considered irrational under standard economic theory.</p>

<p>I recently decided to learn what hyperbolic discounting was all about, so I set out to write this blog post. I have to admit that hyperbolic discounting has been pretty hard for me to understand, but I think I now finally have a good enough handle on it to write about it. Along the way, I learned something interesting: Hyperbolic discounting might be rational after all.</p>

<h2 id="rational-and-irrational-discounting">Rational and irrational discounting</h2>
<p>If I offered you <span class="tex2jax_ignore">$</span>50 now or <span class="tex2jax_ignore">$</span>100 in 6 months, which would you pick? It’s not crazy to choose the <span class="tex2jax_ignore">$</span>50 now. One reason is that it’s a safer bet. If you had chosen the delayed <span class="tex2jax_ignore">$</span>100, there’s a risk that I might forget about the deal when the time came to pay <em>[personal note: I wouldn’t]</em>, and you would never get your money. Another reason is that if you invest the $50 now, you might be able to make up some of the remainder in interest.</p>

<p>Valuing immediate money more than future money is a rational behavior known as discounting. Everybody has their own discount factor. Some people might value money in 6 months at, say, 75% of what they’d value it today. Others might value it at 90%.</p>

<p>In the early 1980s, psychologist George Ainslie discovered something peculiar. He <a href="https://en.wikipedia.org/wiki/Hyperbolic_discounting#Observations">found</a> that while a lot of people would prefer <span class="tex2jax_ignore">$</span>50 immediately rather than <span class="tex2jax_ignore">$</span>100 in 6 months, they would <em>not</em> prefer <span class="tex2jax_ignore">$</span>50 in 3 months rather than <span class="tex2jax_ignore">$</span>100 in 9 months. These two different scenarios are shown in the diagram below, where the green checks indicate the options that people tended to choose.</p>

<div class="wrapper">
  <img src="/assets/2018_hyperbolic_discounting/timeline_diagram.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
</div>

<p>If you think about it, there’s something inconsistent about this behavior. The two scenarios are actually identical, just shifted by 3 months, and yet the same people behave differently depending on when the scenario would be presented. If we waited 3 months and then asked them again if they would prefer <span class="tex2jax_ignore">$</span>50 immediately or <span class="tex2jax_ignore">$</span>100 in 6 months, their original response to Scenario 1 implies they would prefer the fast money (i.e. they would have a high discount rate), but their original response to Scenario 2 implies they would prefer the delayed money (i.e. they would have a low discount rate). In other words, their present self today would make a decision in Scenario 2 that three months from now they will have regretted making. This behavior is <em>time-inconsistent</em> and is therefore considered irrational according to standard economic theory.</p>

<p>The usual way to achieve rational time-consistent discounting is with an exponential discount curve, where the value of receiving something at future time <script type="math/tex">t</script> is a fixed fraction <script type="math/tex">s(t)</script> of its present value, and where <script type="math/tex">\lambda</script> is the constant discount rate.</p>

<script type="math/tex; mode=display">s(t) = e^{-\lambda t}</script>

<p>With an exponential curve, a dollar delayed by six months is always worth the same fixed fraction of a dollar at the baseline date, no matter what the baseline date it is. If you are indifferent between <span class="tex2jax_ignore">$</span>100 now and <span class="tex2jax_ignore">$</span>50 in 6 months, you should also be indifferent between <span class="tex2jax_ignore">$</span>100 in 3 months and <span class="tex2jax_ignore">$</span>50 in 9 months. The discount rate is constant.</p>

<div class="wrapper">
  <img src="/assets/2018_hyperbolic_discounting/fig_exp_fracs.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
</div>

<p>In contrast to an exponential curve, humans tend to show a <a href="https://en.wikipedia.org/wiki/Hyperbolic_discounting">hyperbolic discount curve</a>, which is considered irrational according to standard economic theory. Confusingly, the hyperbolic discount curve is not defined by one of the <a href="https://en.wikipedia.org/wiki/Hyperbolic_function">hyperbolic functions</a> you may remember from high school. Instead, it is defined as follows, where $ \tau $ is the relative time from now:</p>

<script type="math/tex; mode=display">s(\tau) = \frac{1}{1+k \tau}</script>

<p>Whereas an exponential curve has a constant discount rate, a hyperbolic discount curve has a higher discount rate in the near future and lower discount rate in the distant future. That’s why the participants in Ainslie’s experiment cared more about the delay from 0 to 6 months than about the same delay from 3 to 9 months.</p>

<div class="wrapper">
  <img src="/assets/2018_hyperbolic_discounting/fig_exp_and_hyp.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
</div>

<p>The apparent rationality of an exponential function is often presented either with a hazard rate interpretation or an interest rate interpretation. It turns out, however, that both of these interpretations make implausible assumptions about the world. The rest of this blog post describes how if we make some more plausible assumptions, hyperbolic discount function becomes rational.</p>

<h2 id="hazard-rate-interpretation">Hazard Rate interpretation</h2>

<p>According to the hazard-based interpretation of discounting, you should prefer immediate money to future money because there is a risk that the future money will never be delivered. This interpretation is more common in the animal behavior literature. (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2648524/">Pigeons</a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2648524/">rats</a>, and <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3107575/">monkeys</a> all show hyperbolic discounting.)</p>

<h5 id="apparent-rationality-of-exponential-discounting">Apparent rationality of exponential discounting</h5>

<p>Imagine that there could be an event at some time that could cause you to no longer receive your reward. Perhaps the person who owes you the money could die or lose their assets. Assuming a constant hazard rate (i.e. assuming the event is equally likely to happen at any time), the probability that the event has not happened by time $ \tau $ is:</p>

<script type="math/tex; mode=display">s(\tau) = e^{-\lambda \tau}</script>

<p>The <script type="math/tex">s(\tau)</script> function is called a “survival” function because it describes the probability that the deal is still “alive” by time <script type="math/tex">\tau</script>. The <script type="math/tex">\lambda</script> parameter is the constant hazard rate.</p>

<div class="wrapper">
  <img src="/assets/2018_hyperbolic_discounting/fig_exp_survival.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
</div>

<p>A key insight here is that the survival function can also be interpreted as a discount function. If someone says you can receive your reward at time $ \tau $ if you’re willing to wait for it, and if there’s only a 60% chance you’ll actually receive it, you should value that offer at 60% of the current value of the reward. Since the survival function is exponential, and since the survival function <em>is</em> the discount function, your discount function is also exponential, at least according to standard economic theory.</p>

<h5 id="rationality-of-hyperbolic-discounting">Rationality of hyperbolic discounting</h5>

<p>The problem with assuming an exponential survival function is that it assumes you know the hazard rate $ \lambda $. In most situations, you don’t know exactly what the hazard rate is. Instead you have uncertainty around the parameter. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1689473/pdf/T9KA20YDP8PB1QP4_265_2015.pdf">Souza (2015)</a> proposes an exponential prior distribution over the hazard rate.</p>

<script type="math/tex; mode=display">p(\lambda) = \frac{1}{k} e^{-\frac{\lambda}{k}}</script>

<p>The exponential prior is shown in the graph below. Although this curve looks like a discount function, it is not. It is a distribution over a parameter.</p>

<div class="wrapper">
  <img src="/assets/2018_hyperbolic_discounting/fig_hazard_prior.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
</div>

<p>According to Souza, if you average over all the possible exponential survival functions generated by this prior, you get a hyperbolic survival function, or equivalently, a hyperbolic discount function. Let’s see this in action.</p>

<p>In the plot below, I’ve drawn 30 exponential survival functions in blue, each with a <script type="math/tex">\lambda</script> sampled from the <script type="math/tex">p(\lambda)</script> prior distribution defined above. The pink curve is the mean of all of them. Notice that whereas the individual survival curves are all exponential, their mean is hyperbolic, with a distant future characterized by a flat slope and relatively high survival values.</p>

<div class="wrapper">
  <img src="/assets/2018_hyperbolic_discounting/fig_mean_of_exponentials.png" class="inner" style="position:relative border: #222 2px solid; max-width:100%;" />
</div>

<p>As a consequence, the further you look into the future, the lower your discount rate. This is exactly what hyperbolic discounting is and exactly what humans do. For the choice between <span class="tex2jax_ignore">$</span>50 now and <span class="tex2jax_ignore">$</span>100 in 6 months, we apply a heavy discount rate. For the choice between <span class="tex2jax_ignore">$</span>50 in 3 months and <span class="tex2jax_ignore">$</span>100 in 9 months, we apply a lighter discount rate.</p>

<p>For a more qualitative intuition, you can think of it this way: When you make a deal with someone, you don’t know what the hazard rate is going to be. But if the deal is still alive after 80 months, then your hazard rate is probably favorable and thus the deal is likely to still be alive after 100 months. You can therefore have a light discount rate between 80 months and 100 months.</p>

<h2 id="interest-rate-interpretation">Interest Rate interpretation</h2>

<p>While hazard functions are one way to explain discounting, another common explanation involves interest rates. A dollar now is worth more than a dollar in a year, because if you take the dollar now and invest it, it will worth more in a year.</p>

<h5 id="apparent-rationality-of-exponential-discounting-1">Apparent rationality of exponential discounting</h5>
<p>If the interest rate is 5%, you should be indifferent between <span class="tex2jax_ignore">$</span>100 today and <span class="tex2jax_ignore">$</span>105 in a year, since you could just invest the <span class="tex2jax_ignore">$</span>100 today and get the same amount in a year. If the interest rate is constant, the value of the dollar will rise exponentially, according to $ v(\tau) = e^{0.05\tau} $, where $ v $ is the value. To rationally maintain indifference between a dollar and its equivalent value in the future after investment, your discount function should decay exponentially, according to $ s(\tau) = e^{-0.05\tau} $.</p>

<h5 id="rationality-of-hyperbolic-discounting-1">Rationality of hyperbolic discounting</h5>
<p>The problem with this story is that it only works if you assume that the interest rate is constant. In the real world, the interest rate fluctuates.</p>

<p>Before taking on the fluctuating interest rate scenario, let’s first take on a different assumption that is still somewhat simplified. Let’s assume that the interest rate is constant but we don’t know what it is, just as we didn’t know what the hazard rate was in the previous interpretation. With this assumption, the justification for hyperbolic discounting becomes similar to the explanation in the blue and pink plots above. When you do a probability-weighted average over these decaying exponential curves, you get a hyperbolic function.</p>

<p>The previous paragraph assumed that the interest rate was constant but unknown. In the real world, the interest rate is known but fluctuates over time. <a href="https://campuspress.yale.edu/johngeanakoplos/files/2017/07/Hyperbolic-Discounting-is-Rational-Valuing-the-Far-Future-with-Uncertain-Discount-Rates-1uc47ft.pdf">Farmer and Geanakoplos (2009)</a> showed that if you assume that interest rate fluctuations follow a <a href="http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/tutorials/sfehtmlnode21.html">geometric random walk</a>, hyperbolic discounting becomes optimal, at least asymptotically as <script type="math/tex">\tau \rightarrow \infty</script>. In the near future, you know the interest rate with reasonable certainty and should therefore discount with an exponential curve. But as you look further into the future, your uncertainty about the interest rate increases and you should therefore discount with a hyperbolic curve.</p>

<p>Is the geometric random walk a process that was cherry picked by the authors to produce this outcome? Not really. <a href="https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/7542/NewellPizerDiscountingPew.pdf?sequence=1">Newell and Pizer (2003)</a> studied US bond rates in the 19th and 20th century and found that the geometric random walk provided a better fit than any of the other interest rate models tested.</p>

<h2 id="summary">Summary</h2>
<p>When interpreting discounting as a survival function, a hyperbolic discounting function is rational if you introduce uncertainty into the hazard parameter via an exponential prior (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1689473/pdf/T9KA20YDP8PB1QP4_265_2015.pdf">Souza, 2015</a>). When interpreting the discount rate as an interest rate, a hyperbolic discounting function is asymptotically rational if you introduce uncertainty in the interest rate via a geometric random walk (<a href="https://campuspress.yale.edu/johngeanakoplos/files/2017/07/Hyperbolic-Discounting-is-Rational-Valuing-the-Far-Future-with-Uncertain-Discount-Rates-1uc47ft.pdf">Farmer and Geanakoplos, 2009</a>).</p>



    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2018/02/04/hyperbolic-discounting/#disqus_thread" data-disqus-identifier="http://localhost:4000/2018/02/04/hyperbolic-discounting/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2018/02/04/hyperbolic-discounting/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Hyperbolic discounting — The irrational behavior that might be rational after all">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>

<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'csaid81'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function () {
  var s = document.createElement('script'); s.async = true;
  s.type = 'text/javascript';
  s.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
  }());
</script>

    </div>

  </body>
</html>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


<script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
