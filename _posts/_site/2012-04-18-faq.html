<p><em>This is a continuously updated list of responses to questions I get about the <a href="/2012/04/17/its-the-incentives-structure-people-why-science-reform-must-come-from-the-granting-agencies/">main post</a> and <a href="/2013/01/16/8-lessons-from-the-reproducibility-crisis/">Top 8 List</a>, which should be read first.</em></p>

<p><strong>Q:</strong> To solve the replication crisis, scientists need to do X.</p>

<p><strong>A:</strong> There’s a good chance ‘X’ is a <a href="http://en.wikipedia.org/wiki/Collective_action%23Collective_action_problem">collective action problem</a>. If all scientists did it, the field as a whole would benefit. But if a single scientist did it alone, he or she would not benefit. These problems can only be fixed if an outside force (e.g. the NIH) adjusts the incentives so that individuals would benefit from doing the action alone.</p>

<p><strong>Q:</strong> Journals need to do X.</p>

<p><strong>A:</strong> Again, this is probably a collective action problem. See the response above. The NIH can’t tell journals what to do, but it can reward scientists who submit to good-practice journals. This will encourage other journals to change their behavior.</p>

<p><strong>Q:</strong> What about citation count metrics? Aren’t they biased towards surprising and interesting results?</p>

<p><strong>A:</strong> Citation count metrics should not be used as a factor in grant decisions and should be replaced by one of the quality-based metrics proposed by <a href="http://futureofscipub.wordpress.com/2009/11/12/open-post-publication-peer-review/">Niko Kriegeskorte</a> or <a href="http://talyarkoni.org/papers/Yarkoni_open_evaluation_03132012.pdf">Tal Yarkoni</a>, or some of the other metrics proposed in the <a href="http://www.frontiersin.org/Journal/SpecialTopicDetail.aspx?name=computational_neuroscience&amp;st=137&amp;sname=Beyond_open_access_visions_for">special issue</a> of Frontiers. My only addition to Niko’s proposed metrics is that I would emphasize importance of the research <em>question</em>, rather than importance of the <em>outcome.</em></p>

<p><strong>Q: </strong>Couldn’t quality-based metrics or other aspects of your proposal be gameable, just like the current system?</p>

<p><strong>A: </strong>Yes. All systems are gameable, but some systems are better than others. A more “outcome-unbiased” system will be far better than the current one, which is dysfunctional.</p>

<p><strong>Q:</strong> Won’t it be hard for granting agencies to determine whether a journal’s incentive structure encourages good practices?</p>

<p><strong>A:</strong> Government agencies make qualitative  judgments all the time. Even a rough first-order approximation would have a huge positive effect on research quality. The status quo is dysfunctional. Moreover, some journals that specialize in simple experiments (e.g. clinical trials) might demonstrate that they are outcome-unbiased by adopting an <a href="http://www.overcomingbias.com/2010/11/results-blind-peer-review.html">outcome-</a>_<a href="http://www.overcomingbias.com/2010/11/results-blind-peer-review.html">blind</a> _review system, as Robin Hanson has proposed.</p>

<p><strong>Q:</strong> What about post-publication review?</p>

<p><strong>A:</strong> In the current system, the only signal of a paper’s quality is the journal’s impact factor. Readers need more information than this. I am generally supportive of post-publication review and would recommend reading the proposals of <a href="http://futureofscipub.wordpress.com/">Niko Kriegeskorte</a> and some of the proposals in the <a href="http://www.frontiersin.org/Journal/SpecialTopicDetail.aspx?name=computational_neuroscience&amp;st=137&amp;sname=Beyond_open_access_visions_for">special issue</a> of Frontiers. Still, I wonder: Will any of these ideas actually be put into practice? Or will scientists just continue to talk about them as they have since the 1970s? It seem like a classic <a href="http://en.wikipedia.org/wiki/Collective_action%23Collective_action_problem">collective action problem</a>. Granting agencies may be needed to provide a nudge.</p>

<p><strong>Q:</strong> Null results can easily obtained with sloppy research. Won’t outcome-unbiased journals encourage sloppy research?</p>

<p><strong>A:</strong> Yes, it is admittedly a complicated issue. We need to strike a balance between being completely outcome-unbiased on the one hand, and valuing significant results on the other hand. At the moment, the wrong balance has been struck. Null results are disincentivized far too much.</p>

<p><strong>Q: </strong>Isn’t it good to do exploratory analysis?</p>

<p><strong>A: </strong>Absolutely, but only if it identified as such. <a href="http://psr.sagepub.com/content/2/3/196.abstract">HARKing</a> is misleading, and inflates Type I error.</p>
