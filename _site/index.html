<!DOCTYPE html>
<html lang="en-us">

<!--
It's bad to import d3 in every post separatly (https://groups.google.com/forum/#!topic/d3-js/bwdNirt2uEU).
Importing it globally here.
Putting this up at the top, according to this controversial stack overflow answer
http://stackoverflow.com/questions/7169370/d3-js-and-document-onready
 -->
<script src="http://d3js.org/d3.v3.min.js"></script>

<link rel="stylesheet" href="/public/font-awesome-4.4.0/css/font-awesome.min.css">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
    <meta name="description" content="My name is Chris Said and I am a data scientist at Opendoor. This blog is mostly about tech, stats, and science.">
  

  <!-- To get a link preview image, just set the image attribute in your _post -->
  
  
  <title>
    
      The File Drawer &middot; A blog by Chris Said
    
  </title>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-67746868-1', 'auto');
    ga('send', 'pageview');
  </script>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

<!--   <body class="theme-base-cps">
 -->
  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          The File Drawer
        </a>
      </h1>
      <p class="lead">My name is Chris Said and I am a data scientist at Opendoor. This blog is mostly about tech, stats, and science.</p>

    </div>

    <nav class="sidebar-nav">

      
        <a class="sidebar-nav-item active" href="/">Home</a>
      
        <a class="sidebar-nav-item" href="/archive">Archive</a>
      
        <a class="sidebar-nav-item" href="/atom.xml">Feed</a>
      

    </nav>

    <div class="wrapper">
      <div class="inner">
        <a href = "http://www.twitter.com/Chris_Said" class="contact-button"><i class="fa fa-twitter fa-2x"></i></a>
        <a href = "https://www.linkedin.com/pub/chris-said/6b/86b/979" class="contact-button"><i class="fa fa-linkedin-square fa-2x"></i></a>
        <a href = "mailto:chris.said@gmail.com" class="contact-button"><i class="fa fa-envelope fa-2x"></i></a>
      </div>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2017/06/19/three-questions-for-social-scientists/">
        Three questions for social scientists&#58; Internet virtue edition
      </a>
    </h1>

    <span class="post-date">19 Jun 2017</span>

    <p>This isn’t news to anybody, but the internet is changing our culture. Recently, I’ve been thinking about how it has changed our moral culture, and I realized that most of our beliefs on this topic are weirdly in tension with one another. Below are three questions that I feel are very much unresolved. I don’t have any good answers to them, and so I think they might be good topics for social science research.</p>

<h4 id="1-moral-substitution-and-moral-licensing-versus-moral-contagion">1. Moral Substitution and Moral Licensing versus Moral Contagion</h4>
<p>When people do the Ice Bucket Challenge or put a Pride symbol on their profile avatar, they are sometimes accused of <em>virtue signalling</em>, a derogatory term akin to <em>moral grandstanding</em>. Virtue signallers are said to care more about showcasing their virtue than about creating real change.</p>

<p>Virtue signalling is bad, allegedly, for two reasons:
First, instead of performing truly impactful moral acts, virtue signallers spend more time performing easy and symbolic acts. This could be called <em>moral substitution</em>.
Second, after doing something good, people often feel like they’ve earned enough virtue points that they can get away with doing something bad. This well-studied phenomenon is called <em><a href="https://en.wikipedia.org/wiki/Self-licensing">moral licensing</a></em>.</p>

<div class="wrapper">
  <img src="/assets/2017_three_questions/fig_kony.jpg" height="250" class="inner" style="position:relative; border: #ccc 1px solid;" />
</div>

<p>While there are some clear ways that virtue signalling can be bad, there is another way in which it is good. Doing good things makes other people more likely to do good things. This process, known as <em>moral contagion</em>, was famously demonstrated in the Milgram experiments. Participants in those experiments who saw other participants behave morally were <a href="https://www.saylor.org/site/wp-content/uploads/2011/08/PSYCH202A-3.1.1-Milgram-experiment.pdf">dramatically more likely</a> to behave morally as well.</p>

<p>If the social science research is right, then we can conclude that putting a Pride symbol on your avatar make you behave <em>worse</em> (via moral licensing and moral substitution), but it makes other people behave <em>better</em> (via moral contagion). This leaves a couple of open questions:</p>

<p>First, how do the pros and cons balance out? Perhaps your Pride avatar is net positive if you have a large audience on Facebook, but net negative if you have a small audience. And second, how does moral contagion work with symbolic acts? Does the Pride avatar just make other people add Pride symbols to their avatars? Or does it make them behave more ethically in real and impactful ways?</p>

<p>We are beginning to get <a href="http://smart-meter-analytics.de/downloads/papers/Tiefenbeck%20-%20For%20better%20or%20for%20worse.pdf">some</a> quantitative answers to these questions. Clever research from <a href="https://twitter.com/LindaSkitka">Linda Skitka</a> and others has <a href="https://www.researchgate.net/profile/Linda_Skitka/publication/265606809_Morality_in_everyday_life/links/57457f2608ae9f741b410416.pdf">shown</a> that committing a moral act makes you about 40% less likely to commit another moral act later in the day, whereas hearing about someone else’s moral act makes you about 25% more likely to commit a moral act later in the day, although the latter finding fell short of statistical significance. More research is needed though, particularly when it comes to social media and symbolic virtue signalling.</p>

<h4 id="2-slacktivism-versus-violent-revolution">2. Slacktivism versus Violent Revolution</h4>

<p>This question is more for political scientists.</p>

<p>Many people are concerned that the internet encourages <em><a href="https://en.wikipedia.org/wiki/Slacktivism">slacktivism</a></em>, a phenomenon closely related to <em>moral substitution</em>. It’s easier to slap a Pride symbol on your Facebook than to engage in real activism. In this way, the internet is really a tool of the already powerful.</p>

<div class="wrapper">
  <img src="/assets/2017_three_questions/fig_homer.png" height="250" class="inner" style="position:relative border: #222 2px solid;" />
</div>

<p>On the other hand, some people are concerned that the internet cultivates <em>violent radicalism</em>. Online filter bubbles create anger and online networks create alliances, ultimately leading to violent rhetoric and homegrown terrorism.  Many observers already sense the undercurrents of violent revolution.</p>

<p>How can we be worried that the internet is causing <em>both</em> slacktivism and violent radicalism? One possibility is that we only need to worry about slacktivism, and that the violent rhetoric isn’t actually violent – it’s just rhetoric. But the <a href="http://firstmonday.org/article/view/3336/2767">other</a> possibility is that the internet has made slacktivists out of people who otherwise wouldn’t be doing anything at all, and it has made violent radicals out of people who would otherwise be mere activists. I’m not sure what the answer is, but <a href="https://www.nytimes.com/2017/05/10/opinion/crisis-or-stasis.html?_r=0">it would be useful</a> to understand this more.</p>

<h4 id="3-political-correctness-overton-windows-versus-wolf-crying">3. Political Correctness: Overton Windows versus Wolf Crying</h4>
<p>Perhaps because of filter bubbles on both sides of the political spectrum, the term “political correctness” is back with a vengeance. Leaving aside the question of whether political correctness is good or bad, it would be interesting to understand whether it is effective. On the one hand, political correctness may help define an <a href="https://en.wikipedia.org/wiki/Overton_window">Overton Window</a>, setting useful bounds around opinions that can be aired in polite company. But on the other hand, if the enforcers squeeze the boundaries too much, imposing stricter and stricter controls on the range of acceptable discourse, they risk undermining their own credibility by “crying wolf”. For what it’s worth, many internet trolls credit their success to a perception that so-called social justice warriors overplayed their cards. I’m not sure how much to believe them, but it seems possible.</p>

<p>Just in terms of effectiveness, is there a point at which political correctness starts to backfire? And more broadly, what is the optimal level of political correctness for a society? Neither of these questions seems easy to answer, but I would love to learn more.</p>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2017/06/19/three-questions-for-social-scientists/#disqus_thread" data-disqus-identifier="http://localhost:4000/2017/06/19/three-questions-for-social-scientists/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2017/06/19/three-questions-for-social-scientists/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Three questions for social scientists&#58; Internet virtue edition">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2017/05/22/keyboard-shortcuts-i-couldnt-live-without/">
        Keyboard shortcuts I couldn't live without
      </a>
    </h1>

    <span class="post-date">22 May 2017</span>

    <p>Keyboard shortcuts are interesting. Even though I know they are almost always worth learning, I often find myself shying away from the uncomfortable task of actually learning them. But after years of clumsily reaching for the mouse while my colleagues looked at me with a kindly sense of pity, I have slowly accumulated enough keyboard tricks that I’d like to share them. This set is probably far from optimal, and different people have their own solutions, but it has worked well for me. Before jumping in, here’s a reference table of key symbols and their common names:</p>

<div class="wrapper">
<table style="font-size:.7rem; margin: auto; width: 250px">
    <tr>
        <th>Key Symbol</th>
        <th>Key Name</th>
    </tr>
    <tr>
        <td align="center">⌘</td>
        <td>Command</td>
    </tr>
    <tr>
        <td align="center">⇧</td>
        <td>Shift</td>
    </tr>
    <tr>
        <td align="center">⌃</td>
        <td>Control</td>
    </tr>
    <tr>
        <td align="center">⌥</td>
        <td>Alt/Option</td>
    </tr>
     <tr>
        <td align="center">↵</td>
        <td>Enter</td>
    </tr>
</table>
</div>
<div class="caption"><strong>Table 1.</strong> Key symbol map.</div>

<h3 id="sublime-text">Sublime Text</h3>
<p>Sublime has <a href="http://docs.sublimetext.info/en/latest/reference/keyboard_shortcuts_osx.html">lots</a> of great shortcuts. My favorites is <code class="highlighter-rouge">⌘D</code>, which allows you to sequentially select exact matches of highlighted text. Once the matches are selected, you can simultaneously edit them with a multi-cursor. If you want to select all matches simultaneously, rather than sequentially, you can use <code class="highlighter-rouge">⌃⌘G</code>.</p>

<div class="wrapper">
  <img src="/assets/2017_keyboard_shortcuts/gif_sublime_shining_fast.gif" width="350" class="inner" style="position:relative" />
  <div class="caption">
    <strong>Figure 1.</strong> Demonstration of <code>⌘D</code>, <code>⌘←</code>, <code>⌘→</code>, <code>⌘A</code> and <code>⌘KU</code> in Sublime Text.
  </div>
</div>
<p><br /></p>

<h3 id="chrome">Chrome</h3>
<p>With the exception of scrolling and link clicking, everything you do in Chrome should be done with keyboard only. If you’re new to this, I’d recommend starting with <code class="highlighter-rouge">⌘L</code>, <code class="highlighter-rouge">⌘T</code>, <code class="highlighter-rouge">⌘W</code> and then expanding from there. Special bonus:	 if you ever accidentally close a tab, you can reopen it with <code class="highlighter-rouge">⌘⇧T</code>.</p>

<figure class="image"><img src="/assets/2017_keyboard_shortcuts/no_touching.png" alt="" /></figure>

<div class="caption"><strong>Figure 2.</strong> The "No Touching" Chrome Zone. Your mouse should never come anywhere near here.</div>

<h3 id="mac-os-x">Mac OS X</h3>
<p>On Mac OS X, <code class="highlighter-rouge">⌘ Tab</code> switches between applications, and <code>⌘`</code> switches between windows of the same application. The <code class="highlighter-rouge">⌘+</code> and <code class="highlighter-rouge">⌘-</code> shortcuts change the display size of text and other items. You can jump to the beginning of a line with <code class="highlighter-rouge">⌘←</code> or <code class="highlighter-rouge">⌃A</code>, and to the end of a line with <code class="highlighter-rouge">⌘→</code> or <code class="highlighter-rouge">⌃E</code>. In Terminal, you can delete to the beginning of a line with <code class="highlighter-rouge">⌃U</code>.</p>

<p>For easy window snapping, use the <a href="https://www.spectacleapp.com/">Spectacle</a> app. Because Spectacle’s default mappings conflict with Chrome’s tab switching shortcuts, I’d recommend setting the four main screen position shortcuts to <code class="highlighter-rouge">⌘⌃←</code>, <code class="highlighter-rouge">⌘⌃→</code>, <code class="highlighter-rouge">⌘⌃↑</code>, <code class="highlighter-rouge">⌘⌃↓</code>, and eliminating all the other shortcuts, except the full screen shortcut, which should be set to <code class="highlighter-rouge">⌘⌃F</code>.</p>

<div class="wrapper">
  <img src="/assets/2017_keyboard_shortcuts/gif_spectacle.gif" width="350" class="inner" style="position:relative; border: #666666 2px solid;" />
  <div class="caption">
    <strong>Figure 3.</strong> Arranging windows with custom shortcuts in Spectacle.
  </div>
</div>

<h3 id="gmail">Gmail</h3>
<p>If you’re using a mouse on Gmail, you’re doing it wrong. With the exception of a few word processing operations, literally everything you do in Gmail should be done with keyboard only. Compose, Reply, Reply All, Forward, Send, Search, Navigate, Open, Inbox, Sent, Drafts, Archive. All of these should be done with the keyboard. To enable these shortcuts, you must go into your Settings and navigate to the General tab. Once shortcuts have been enabled, you can see a list of all them by typing <code class="highlighter-rouge">?</code>.</p>
<figure class="image"><img src="/assets/2017_keyboard_shortcuts/fig_gmail_annotated.png" alt="" /></figure>

<div class="caption"><strong>Figure 4.</strong> A small sample of things you can do on Gmail without ever touching your mouse.</div>

<h3 id="twitter">Twitter</h3>
<p>With shortcuts similar to Gmail’s, you can jump to different pages using only the keyboard: <code class="highlighter-rouge">gh</code> brings you to the Home Timeline and <code class="highlighter-rouge">gu</code> lets you jump to another user’s profile. The most useful shortcut is probably <code class="highlighter-rouge">.</code>, which loads any new tweets that are waiting for you at the top of the Timeline. You can see a list of all shortcuts by typing <code class="highlighter-rouge">?</code>.</p>

<h3 id="jetbrains">JetBrains</h3>
<p>JetBrains products like DataGrip, PyCharm, and IntelliJ offer plenty of keyboard shortcuts. My favorites are <code class="highlighter-rouge">⌃G</code>, for sequential highlighting, and <code class="highlighter-rouge">⌥⌥↓</code> and <code class="highlighter-rouge">⌥⌥↑</code> for <a href="https://www.jetbrains.com/help/idea/2017.1/multicursor.html">multi-line cursors</a>.</p>

<h3 id="jupyter">Jupyter</h3>
<p>Jupyter has tons of <a href="http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Notebook%20Basics.html?highlight=keyboard#Keyboard-Navigation">essential</a> keyboard shortcuts that can be found by typing <code class="highlighter-rouge">?</code> while in command mode. In addition, it’s possible to get Sublime-style text editing by following the instructions described <a href="http://blog.rtwilson.com/how-to-get-sublime-text-style-editing-in-the-ipythonjupyter-notebook/">here</a>.</p>

<div class="wrapper">
  <img src="/assets/2017_keyboard_shortcuts/gif_jupyter.gif" width="350" class="inner" style="position:relative; border: #ccc 1px solid;" />
  <div class="caption">
    <strong>Figure 5.</strong> Common Jupyter workflow done entirely with the keyboard, with help from some Sublime-style editing: <code>c</code> and <code>v</code> to copy and paste a cell, <code>⌘D</code> for multiple selection, <code>⌘→</code> to jump to the end of line, <code>dd</code> to delete a cell.
  </div>
</div>
<p><br /></p>



    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2017/05/22/keyboard-shortcuts-i-couldnt-live-without/#disqus_thread" data-disqus-identifier="http://localhost:4000/2017/05/22/keyboard-shortcuts-i-couldnt-live-without/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2017/05/22/keyboard-shortcuts-i-couldnt-live-without/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Keyboard shortcuts I couldn't live without">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2017/05/08/learning-by-flip-flopping/">
        Learning by flip-flopping
      </a>
    </h1>

    <span class="post-date">08 May 2017</span>

    <p>I recently came across <a href="https://nintil.com/">Artir</a>’s <a href="https://nintil.com/2016/03/06/the-pyramid-of-economic-insight-and-virtue/">Pyramid of Economic Insight and Virtue</a>. It’s not actually a pyramid, but is instead a riff on the Expanding Brain meme. Check it out:</p>
<div class="wrapper">
<div class="inner">
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">The Pyramid of Economic Insight and Virtue, now in meme form. From <a href="https://t.co/AT148qXSbc">https://t.co/AT148qXSbc</a> <a href="https://t.co/odmXCcUxiV">pic.twitter.com/odmXCcUxiV</a></p>&mdash; Artir (@ArtirKel) <a href="https://twitter.com/ArtirKel/status/843886928135159819">March 20, 2017</a></blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</div>

<p>What’s interesting about Artir’s Pyramid is that at every step, the position flip-flops from the previous step. This isn’t just a dialogue between two sides. It is a description of the belief sequence that people traverse as they learn more about an issue. We might call this <em>learning by flip-flopping</em>.</p>

<p>This got me thinking: In what other issues do people go through a sequence of flip-flops as they learn more about it? In this blog post, I’d like to suggest a few.</p>

<p>Let me stress that in presenting these I don’t necessarily think that the “highest” levels in these examples are correct, nor do I think I have a strong understanding on many of these issues. It’s just something that’s fun to think about.</p>

<h3 id="increasing-the-minimum-wage">Increasing the minimum wage</h3>
<p>This is arguably a special case of Artir’s Pyramid and is probably the canonical example of learning by flip-flopping.</p>

<div class="wrapper">
  <img src="/assets/2017_learning_by_flip_flopping/fig_minimum_wage.png" width="400" class="inner" style="position:relative" />
</div>
<p>I see this a lot. Memorably, I sometimes see a Stage 2 person talking to someone they believe is at Stage 1 but who is in fact at Stage 3.</p>
<div class="wrapper">
  <img src="/assets/2017_learning_by_flip_flopping/fig_mansplaining.jpg" width="400" class="inner" style="position:relative" />
</div>
<p>Life lesson: When debating someone, don’t make strong claims until you know what stage they are on.</p>

<p><em>Further reading on the minimum wage</em>: <a href="http://www.nber.org/papers/w4509">Card and Krueger</a>, <a href="http://www.uvm.edu/~vlrs/doc/min_wage.htm">criticism</a> of Card and Krueger’s data, another <a href="https://www.forbes.com/sites/timworstall/2015/08/01/why-the-card-and-krueger-paper-on-minimum-wages-rises-and-unemployment-is-wrong">case</a> against Card and Krueger, two <a href="http://cepr.net/documents/publications/min-wage-2013-02.pdf">better</a> <a href="http://irle.berkeley.edu/files/2010/Minimum-Wage-Effects-Across-State-Borders.pdf">studies</a>.</p>

<h3 id="how-to-deal-with-a-recession">How to deal with a recession</h3>
<p>Recession flip-flopping is less related to Artir’s Pyramid, but is still quite common. I may be bungling some of the later stages here, as my macro knowledge is mostly cobbled together from <a href="https://www.youtube.com/watch?v=GTQnarzmTOc">parody rap videos</a>, so I welcome any suggestions for additional further reading.</p>
<div class="wrapper">
  <img src="/assets/2017_learning_by_flip_flopping/fig_recessions.jpg" width="400" class="inner" style="position:relative" />
</div>

<p><em>Further reading on recessions</em>: <a href="https://www.theguardian.com/money/us-money-blog/2013/mar/26/federal-budget-household-finances-fed">The government is not a household</a>, <a href="https://en.wikipedia.org/wiki/Keynesian_economics">Keynesian economics</a>, <a href="https://www.youtube.com/watch?v=GTQnarzmTOc">boom and bust cycles</a>, and a <a href="https://www.amazon.com/Undercover-Economist-Strikes-Back-Ruin/dp/1594631409">wonderful book</a> by <a href="https://twitter.com/TimHarford">Tim Harford</a>.</p>
<h3 id="twitters-140-character-limit">Twitter’s 140 character limit</h3>
<p>I’d like to keep this blog post as value-judgment free as possible, but I’ll make a special exception for this one. The 140 character limit is no longer a good idea, and Stage 3 is the correct stage.</p>
<div class="wrapper">
  <img src="/assets/2017_learning_by_flip_flopping/fig_twitter.jpg" width="400" class="inner" style="position:relative" />
</div>

<h3 id="the-meaning-of-life">The meaning of life</h3>
<p><a href="https://meaningness.com/metablog/stem-fluidity-bridge">David Chapman</a> writes about how STEM-trained people should think about meaning. Extending Robert Kegen’s theory of human development, he believes that most STEM-trained people can find meaning in ideological rationalism (Stage 4) but, upon finding that rationality does not provide any meaning, they become in danger of falling into the Nihilism trap (Stage 4.5). Chapman claims that there is a Stage 5, sometimes called meta-rationality or fluidity, in which meaning can once again be found. You can read more about it on his <a href="https://meaningness.com/">blog</a>.</p>

<div class="wrapper">
  <img src="/assets/2017_learning_by_flip_flopping/fig_meaning.png" width="400" class="inner" style="position:relative" />
</div>

<p>What other examples of learning by flip-flopping are out there?</p>

<p>UPDATE: <a href="https://twitter.com/johnvmcdonnell">John McDonnell</a> points me towards <a href="https://en.wikipedia.org/wiki/Dialectic#Hegelian_dialectic">Hegelian Dialectic</a>.</p>



    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2017/05/08/learning-by-flip-flopping/#disqus_thread" data-disqus-identifier="http://localhost:4000/2017/05/08/learning-by-flip-flopping/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2017/05/08/learning-by-flip-flopping/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Learning by flip-flopping">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2017/05/03/empirical-bayes-for-multiple-sample-sizes/">
        Empirical Bayes for multiple sample sizes
      </a>
    </h1>

    <span class="post-date">03 May 2017</span>

    <p>Here’s a data problem I encounter <em>all the time</em>. Let’s say I’m running a website where users can submit movie ratings on a continuous 1-10 scale. For the sake of argument, let’s say that the users who rate each movie are an unbiased random sample from the population of users. I’d like to compute the average rating for each movie so that I can create a ranked list of the best movies.</p>

<p>Take a look at my data:</p>

<p><img src="/assets/2017_shrinkage/fig_movies.png" /></p>
<div class="caption"><strong>Figure 1.</strong> Each circle represents a movie rating by a user. Diamonds represent sample means for each movie.</div>
<p><br />
I’ve got two big problems here. First, nobody is using my website. And second, I’m not sure if I can trust these averages. The movie at the top of the rankings was only rated by two users, and I’ve never even heard of it! Maybe the movie really is good. But maybe it just got lucky. Maybe just by chance, the two users who gave it ratings happened to be the users who liked it to an unusual extent. It would be great if there were a way to adjust for this.</p>

<p>In particular, I would like a method that will give me an estimate closer to the movie’s <em>true mean</em> (i.e. the mean rating it would get if an infinite number of users rated it). Intuitively, movies with mean ratings at the extremes should be nudged, or “shrunk”, towards the center. And intuitively, movies with low sample sizes should be shrunk more than the movies with large sample sizes.</p>

<p><img src="/assets/2017_shrinkage/fig_movies_shrinkage.png" /></p>
<div class="caption"><strong>Figure 2.</strong> Each circle represents a movie rating by a user. Diamonds represent sample means for each movie. Arrows point towards shrunken estimates of each movie's mean rating. Shrunken estimates are obtained using the MSS James-Stein Estimator, described in more detail below.</div>
<p><br /></p>

<p>As common a problem as this is, there aren’t a lot of accessible resources describing how to solve it. There are tons of <a href="http://varianceexplained.org/r/empirical_bayes_baseball/">excellent blog posts</a> about the Beta-binomial distribution, which is useful if you wish to estimate the true fraction of an occurrence among some events. This works well in the case of Rotten Tomatoes, where one might want to know the true fraction of “thumbs up” judgments. But in my case, I’m dealing with a continuous 1-10 scale, not thumbs up / thumbs down judgments. The Beta-binomial distribution will be of little use.</p>

<p>Many resources mention the James-Stein Estimator, which provides a way to shrink the mean estimates only when the variances of those means can be assumed to be equal. That assumption usually only holds when the <em>sample sizes</em> of each group are equal. But in most real world examples, the sample sizes (and thus the variances of the means) are not equal. When that happens, it’s a lot less clear what to do.</p>

<p>After doing a lot of digging and <a href="https://twitter.com/Chris_Said/status/851211601445240832">asking</a> some very helpful folks on Twitter, I found several solutions. For many of the solutions, I ran simulations to determine which worked best. This blog post is my attempt to summarize what I learned. Along the way, we’ll cover the original James-Stein Estimator, two extensions to the James-Stein Estimator, Markov Chain Monte Carlo (MCMC) methods, and several other strategies.</p>

<p>Before diving in, I want to include a list of symbol definitions I’ll be using because – side rant – it sure would be great if all stats papers did this, given that literally every paper I read used its own idiosyncratic notations! I’ll define everything again in the text, but this is just here for reference:</p>

<div style="margin:40px">
<table style="font-size:.7rem">
    <tr>
        <th>Symbol</th>
        <th>Definition</th>
    </tr>
    <tr>
        <td>$ k $</td>
        <td>The number of groups.</td>
    </tr>
    <tr>
        <td>$ \theta_i $</td>
        <td>The true mean of a group.</td>
    </tr>
    <tr>
        <td>$ x_i $</td>
        <td>The sample mean of a group. The MLE estimate of $ \theta_i $.</td>
    </tr>
    <tr>
        <td>$ \epsilon^{2}_i $</td>
        <td>The true variance of observations within a group.</td>
    </tr>
    <tr>
        <td>$ \epsilon^2 $</td>
        <td>The true variance of observations within a group if we assume all groups have the same variance.</td>
    </tr>
    <tr>
        <td>$ s^{2}_i $</td>
        <td>The sample variance of a group. The MLE estimate of $ \epsilon^{2}_i $.</td>
    </tr>
    <tr>
        <td>$ n_i $</td>
        <td>The number of observations in a group.</td>
    </tr>
    <tr>
        <td>$ n $</td>
        <td>The number of observations in a group, if we assume all groups have the same size.</td>
    </tr>
    <tr>
        <td>$ \sigma^{2}_i $</td>
        <td>The true variance of a group's mean. If each group has the same variance of observations, then $ \sigma^{2}_i  = \epsilon^{2} / n_i $. If each group has different variances of observations, then $ \sigma^{2}_i = \epsilon^{2}_i / n_i $.</td>
    </tr>
    <tr>
        <td>$ \sigma^{2} $</td>
        <td>Like $ \sigma^{2}_i $, but if we assume all groups had the same variance of the mean. Equal to $ \epsilon^{2} / n $.</td>
    </tr>
    <tr>
        <td>$ \hat{\sigma^{2}_i} $</td>
        <td>Estimate of $ \sigma^{2}_i $.</td>
    </tr>
    <tr>
        <td>$ \hat{\sigma^{2}} $</td>
        <td>Estimate of $ \sigma^{2} $. 
        </td>
    </tr>
    <tr>
        <td>$ \mu $</td>
        <td>The true mean of the $ \theta_i $'s (the true group means). The mean of the distribution from which the $ \theta_i $'s are drawn.</td>
    </tr>
    <tr>
        <td>$ \overline{X} $</td>
        <td>The sample mean of the sample means.</td>
    </tr>
    <tr>
        <td>$ \tau^{2} $</td>
        <td>The true variance of the $ \theta_i $'s (the true group means). The variance of the distribution from which the $ \theta_i $'s are drawn.</td>
    </tr>
    <tr>
        <td>$ \hat{\tau^{2}} $</td>
        <td>Estimate of $ \tau^{2} $.</td>
    </tr>
    <tr>
        <td>$ \hat{B} $</td>
        <td>Estimate of the best term for weighting $ x_i $ and $ \overline{X} $ when calculating $ \hat{\theta_i} $. Assumes each group has the same $ \sigma^2 $.</td>
    </tr>
    <tr>
        <td>$ \hat{B_i} $</td>
        <td>Estimate of the best term for weighting $ x_i $ and $ \overline{X} $ when calculating $ \hat{\theta_i} $. Does not assume that all group's have the same $ \sigma^{2}_i $.</td>
    </tr>
    <tr>
        <td>$ \hat{\theta_i} $</td>
        <td>Estimate of a true group means. Its value depends on the method we use.</td>
    </tr>
    <tr>
        <td>$ k_{\Gamma} $</td>
        <td>Shape parameter for the Gamma distribution from which sample sizes are drawn.</td>
    </tr>
    <tr>
        <td>$ \theta_{\Gamma} $</td>
        <td>Scale parameter for the Gamma distribution from which sample sizes are drawn.</td>
    </tr>
    <tr>
        <td>$ \mu_v $</td>
        <td>In simulations in which group observation variances $ \epsilon^{2}_i $ are allowed to vary, this is the mean parameter of the log-normal distribution from which the $ \epsilon^{2}_i $'s are drawn.</td>
    </tr>
    <tr>
        <td>$ \tau^{2}_v $</td>
        <td>In simulations in which group observation variances $ \epsilon^{2}_i $ are allowed to vary, this is the variance parameter of the log-normal distribution from which the $ \epsilon^{2}_i $'s are drawn.</td>
    </tr>
</table>
</div>

<h3 id="quick-analytic-solutions">Quick Analytic Solutions</h3>

<p>Our goal is to find a better way of estimating $ \theta_i $, the true mean of a group. A <a href="http://projecteuclid.org/download/pdf_1/euclid.cbms/1462106062">common</a> <a href="https://mathmodelsblog.wordpress.com/2010/02/02/introduction-to-buhlmann-credibility/">theme</a> <a href="https://www.soa.org/files/pdf/c-24-05.pdf">in</a> <a href="http://www.stat.cmu.edu/~acthomas/724/Efron-Morris.pdf">many</a> <a href="http://www.tandfonline.com/doi/abs/10.1080/09332480.2007.10722861">of</a> <a href="http://intlpress.com/site/pub/files/_fulltext/journals/sii/2010/0003/0004/SII-2010-0003-0004-a011.pdf">the</a> <a href="http://conservancy.umn.edu/bitstream/handle/11299/5852/Staffpaper10.pdf;jsessionid=433FD5983CCE7EE49AE0319D9B9FFA02?sequence=1">papers</a> I read is that good estimates of $ \theta_i $ are usually weighted averages of the group’s sample mean $ x_i $ and the global mean of all group means $ \overline{X} $. Let’s call this weighting factor $ \hat{B_i} $.</p>

<script type="math/tex; mode=display">\hat{\theta_i} = \left(1-\hat{B_i}\right) x_i + \hat{B_i} \overline{X}</script>

<p>This seems very sensible. We want something that is in between the sample mean (which is probably too extreme) and the mean of means. But how do we know what value to use for $ \hat{B_i} $? Different methods exist, and each leads to different results.</p>

<p>Let’s start by defining $ \sigma^{2}_i $, the true variance of a group’s mean. This is equivalent to $ \epsilon^{2}_i / n_i $, where $ \epsilon^{2}_i $ is the true variance of the observations within that group, and $ n_i $ is the sample size of the group. According to the original James-Stein approach, if we assume that all the group means have the same known variance $ \hat{\sigma^2} $, which would usually only happen if the groups all had the same sample size, then we can define a common $ \hat{B} $ for all groups as:</p>

<script type="math/tex; mode=display">\hat{B} = \frac{\left(k-3\right)\hat{\sigma^2}}{\sum{(x_i - \overline{X})^2}}</script>

<p>This formula seems really weird and arbitrary, but it begins to make more sense if we rearrange it a bit and sweep that pesky $ \left(k-3\right) $ under the rug and replace it with a $ (k-1) $. Sorry hardliners!</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray} 
\hat{B} &\approx& \frac{\left(k-1\right)\hat{\sigma^2}}{\sum{(x_i - \overline{X})^2}}\\
  \\
  &=& \frac{\hat{\sigma^2}}{\sum{(x_i - \overline{X})^2}/\left(k-1\right)}  \\
  \\
  &=& \frac{\hat{\sigma^2}}{\hat{\tau^{2}} + \hat{\sigma^2}}

\end{eqnarray} %]]></script>

<p>Before getting to why this makes sense, I should explain the last step above. The denominator $ \sum{(x_i - \overline{X})^2}/\left(k-1\right) $ is the observed variance of the observed sample means. This variance comes from two sources: $ \tau^2 $ is the true variance in the true means and $ \sigma^2 $ is the true variance caused by the fact that each $ x_i $ is computed from a sample. Since variances add, the total variance of the observed means is $ \tau^{2} + \sigma^{2} $.</p>

<p>Anyway, back to the result. This result is actually pretty neat. When we estimate a $ \theta_i $, the weight that we place on the global mean $ \overline{X} $ is the fraction of total variance in the means that is caused by within-group sampling variance. In other words, when the sample mean comes with high uncertainty, we should weight the global mean more. When the sample mean comes with low uncertainty, we should weight the global mean less. At least directionally, this formula makes sense. Later in this blog post, we’ll see how it falls naturally out of Bayes Law.</p>

<p>The James-Stein Estimator is so widely applicable that many other fields have discovered it independently. In the image processing literature, it is a special case of the <a href="http://www.dfmf.uned.es/~daniel/www-imagen-dhp/biblio/adaptive-wiener-noisy.pdf">Wiener Filter</a>, assuming that both the signal and the additive noise are Gaussian. In the insurance world, actuaries call it the <a href="https://en.wikipedia.org/wiki/B%C3%BChlmann_model">Bühlmann model</a>. And in <a href="https://en.wikipedia.org/wiki/Charles_Roy_Henderson">animal breeding</a>, early researchers called it the Best Unbiased Linear Prediction or <a href="http://www.public.iastate.edu/~dnett/S511/27BLUP.pdf">BLUP</a> (technically the Empirical BLUP). The BLUP approach is so useful, in fact, that it has received the highly coveted endorsement of the <a href="http://www.nsif.com/guidel/guidelines.htm">National Swine Improvement Federation</a>.</p>

<p>While the original James-Stein formula is useful, the big limitation is that it only works when we believe that all groups have the same $ \sigma^2 $. In cases where we have different sample sizes, each group will have it’s own $ \sigma^{2}_i $. (Recall that $ \sigma^{2}_i = \epsilon^{2}_i / n_i $.) We’re going to want to shrink some groups more than others, and the original James-Stein estimator does not allow this. In the following sections, we’ll look at a couple of extensions to the James-Stein estimator. These extensions have analogues in the Bühlmann model and BLUP literature.</p>

<h4 id="the-multi-sample-size-james-stein-estimator">The Multi Sample Size James-Stein Estimator</h4>

<p>The most natural extension of James-Stein is to define each group’s $ \hat{\sigma^{2}_i} $ as the squared standard error of the group’s mean. This allows us to estimate a weighting factor $ \hat{B_i} $ tailored to each group. Let’s call this the Multi Sample Size James-Stein Estimator, or MSS James-Stein Estimator.</p>

<script type="math/tex; mode=display">\hat{B_i} = \frac{\hat{\sigma^{2}_i}}{\hat{\tau^{2}} + \hat{\sigma^{2}_i}}</script>

<p>The denominator can just be estimated as the variance across group sample means.</p>

<p>As reasonable as this approach sounds, it somehow didn’t feel totally kosher to me. But when I looked into the literature, it seems like most researchers basically said “<a href="http://projecteuclid.org/download/pdf_1/euclid.cbms/1462106062">yup</a>, <a href="https://mathmodelsblog.wordpress.com/2010/02/02/introduction-to-buhlmann-credibility/">that sounds</a> <a href="https://www.soa.org/files/pdf/c-24-05.pdf">pretty reasonable</a>”.</p>

<p>To test this approach, I ran some <a href="https://github.com/csaid/empirical_bayes/blob/master/simulations.ipynb">simulations</a> on 1000 artificial datasets. Each dataset involved 25 groups with sample sizes drawn from a Gamma distribution $ \Gamma(k_{\Gamma}=1.5,\theta_{\Gamma}=10) $. True group means ($ \theta_i $’s) were sampled from a Normal distribution $ \mathcal{N}(\mu, \tau^2) $. Observations within each group were sampled from $ \mathcal{N}(\theta_i, \epsilon^2) $, where $ \epsilon $ was shared between groups.</p>

<p>For each dataset I computed the Mean Squared Error (MSE) between the vector of true group means and the vector of estimated group means. I then averaged the MSEs across datasets. This process was repeated for a variety of different values of $ \epsilon $ and for two different estimators: The MSS James-Stein Estimator and the Maximum Likelihood Estimator (MLE). To compute the MLE, I just used $ x_i $ as my $ \hat{\theta_i} $ estimate.</p>

<figure class="image"><img src="/assets/2017_shrinkage/fig_shared_2.png" alt="" /></figure>

<div class="caption"><strong>Figure 3.</strong> Mean Squared Error between true group means and estimated group means. In this simulation, the $ \epsilon $ parameter for within-group variance of observations is shared by all groups.</div>
<p><br />
As expected, the MSS James-Stein Estimator outperformed the MLE, with lower MSEs particularly for high values of $ \epsilon $. This make sense. When the raw sample means are noisy, the MLE should be especially untrustworthy and it makes sense to pull extreme estimates back towards the global mean.</p>

<h4 id="the-multi-sample-size-pooled-james-stein-estimator">The Multi Sample Size Pooled James-Stein Estimator</h4>

<p>One thing that’s a little weird about the MSS James-Stein Estimator is that even though we know all the groups should have the same within-group variance $ \epsilon^2 $, we still estimate each group’s standard error separately. Given what we know, it might make more sense to <a href="https://en.wikipedia.org/wiki/Pooled_variance">pool</a> the data from all groups to estimate a common $ \epsilon^2 $. Then we can estimate each group’s $ \sigma^{2}_i $ as $ \epsilon^2 / n_i $. Let’s call this approach the MSS Pooled James-Stein Estimator.</p>

<figure class="image"><img src="/assets/2017_shrinkage/fig_shared_3.png" alt="" /></figure>

<div class="caption"><strong>Figure 4.</strong> Mean Squared Error between true group means and estimated group means. In this simulation, the $ \epsilon $ parameter for within-group variance of observations is shared by all groups.</div>
<p><br />
This works a bit better. By obtaining more accurate estimates of each group’s $ \sigma^{2}_i $, we are able to find a more appropriate shrinking factor $ B_i $ for each group.</p>

<p>Of course, this only works better because we created the simulation data in such a way that all groups have the same $ \epsilon^2 $. But if we run a different set of simulations, in which each group’s $ \epsilon_i $ is drawn from a log-normal distribution $ ln\mathcal{N}\left(\mu_v, \tau^{2}_v\right) $, we obtain the reverse results. The MSS James-Stein Estimator, which estimates a separate $ \hat{\epsilon^{2}_i} $ for each group, does a better job than the  MSS Pooled James-Stein Estimator. This makes sense.</p>

<figure class="image"><img src="/assets/2017_shrinkage/fig_unshared_3.png" alt="" /></figure>

<div class="caption"><strong>Figure 5.</strong> Mean Squared Error between true group means and estimated group means. In this simulation, each group has its own variance parameter $ \epsilon^2 $ for the observations within the group. These parameters are sampled from a log-normal distribution $ ln\mathcal{N}\left(\mu_v, \tau^{2}_v\right) $. For simplicity, the two parameters of this distribution are always set to be identical, and are shown on the horizontal axis. </div>
<p><br /></p>

<p>Which method you choose should depend on whether you think your groups have similar or different variances of their observations. Here’s an interim summary of the methods covered so far.</p>

<div class="box">
<h4 style="margin-top:0rem">Summary of analytic solutions</h4>
All of these estimators define $ \hat{\theta_i} $ as a weighted average of the group sample mean $ x_i $ and the mean of group sample means $ \overline{X} $.
$$ \hat{\theta_i} = \left(1-\hat{B_i}\right) x_i + \hat{B_i} \overline{X} $$

Make sure to clip $ \hat{B_i} $ to the range [0, 1].<br /><br />

<ol>
    <li style="margin-bottom: 10px">
        <div><strong>Maximum Likelihood Estimation (MLE)</strong></div>
        <div style="margin-top: 0px; line-height: 5.7em"> $ \hat{B_i} = 0 $</div>
    </li>

    <li style="margin-bottom: 20px">
        <div><strong>MSS James-Stein Estimator</strong></div>
        <div style="margin-top: 0px; line-height: 5.7em"> $ \hat{B_i} = \frac{s^{2}_i/n_i}{\sum\frac{(x_i - \overline{X})^2}{k-1}} $</div>
        where $ s_i $ is the standard deviation of observations with a group.<br />
    </li>

    <li>
        <div><strong>MSS Pooled James-Stein Estimator</strong></div>
        <div style="margin-top: 0px; line-height: 5.7em"> $ \hat{B_i} = \frac{s^{2}_p/n_i}{\sum\frac{(x_i - \overline{X})^2}{k-1}} $</div>
        where $ s^{2}_p $ is the <a href="https://en.wikipedia.org/wiki/Pooled_variance">pooled</a> estimate of variance.
    </li>
</ol>
Implementations in Python and R are available <a href="https://github.com/csaid/empirical_bayes">here</a>.
</div>

<h4 id="a-bayesian-interpretation-of-the-analytic-solutions">A Bayesian interpretation of the analytic solutions</h4>

<p>So far, the analytic approaches make sense directionally. As described above, our estimate of $ \theta_i $ should be a weighted average of $ x_i $ and $ \overline{X} $, where the weight depends on the ratio of sample mean variance to total variance of the means.</p>

<script type="math/tex; mode=display">\theta_i = \left(1 - \frac{\hat{\sigma^{2}_i}}{\hat{\tau^{2}} + \hat{\sigma^{2}_i}}\right) x_i + \left(\frac{\hat{\sigma^{2}_i}}{\hat{\tau^{2}} + \hat{\sigma^{2}_i}}\right) \overline{X}</script>

<p>But is this really the best weighting? Why use a ratio of variances instead of, say, a ratio of standard deviations? Why not use something else entirely?</p>

<p>It turns out this formula falls out naturally from Bayes Law. Imagine for a moment that we already know the prior distribution $ \mathcal{N}\left(\mu, \tau^2\right) $ over the $ \theta_i $’s. And imagine we know the likelihood function for a group mean is $ \mathcal{N}\left(x_i, \epsilon^{2}_i/n_i\right) $. According to the Wikipedia page on <a href="https://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions">conjugate priors</a>, the posterior distribution for the group mean is itself a Gaussian distribution with mean:</p>

<script type="math/tex; mode=display">\hat{\theta_i} = \frac{\frac{x_i n_i}{\epsilon^{2}_i} + \frac{\mu}{\tau^{2}}}{\frac{1}{\tau^2} + \frac{n_i}{\epsilon^{2}_i}}</script>

<p>(Note that the Wikipedia <a href="https://en.wikipedia.org/wiki/Conjugate_prior#Continuous_distributions">page</a> uses the symbol ‘$ x_i $’ to refer to observations, whereas this blog post will always use the term to refer to the sample mean, including in the equation above. Also note that Wikipedia refers to the variance of observations within a group as ‘$ \sigma^2 $’ whereas this blog post uses $ \epsilon^{2}_i $.)</p>

<p>If we multiply all terms in the numerator and denominator by $ \frac{\tau^2 \epsilon^{2}_i}{n_i} $, we get:</p>

<script type="math/tex; mode=display">\hat{\theta_i} = \frac{\tau^2 x_i + \sigma^{2}_i \mu}{\sigma^{2}_i + \tau^2}</script>

<p>Or equivalently,</p>

<script type="math/tex; mode=display">\hat{\theta_i} = \left(1-B\right) x_i + B \mu</script>

<p>where</p>

<script type="math/tex; mode=display">B = \frac{\sigma^{2}_i}{\sigma^{2}_i + \tau^2}</script>

<p>This looks familiar! It is basically the MSS James-Stein estimator. The only difference is that in the pure Bayesian approach you must somehow know $ \mu $, $ \tau^2 $, and $ \sigma^{2}_i $ in advance. In the MSS James-Stein approach, you estimate those parameters from the data itself. This is the key insight in Empirical Bayes: Use priors to keep your estimates under control, but obtain the priors <em>empirically</em> from the data itself.</p>

<h3 id="hierarchical-modeling-with-mcmc">Hierarchical Modeling with MCMC</h3>

<p>In previous sections we looked at some analytic solutions. While these solutions have the advantage of being quick to calculate, they have the disadvantage of being less accurate than they could be. For more accuracy, we can turn to Hierarchical Model estimation using Markov Chain Monte Carlo (MCMC) methods. MCMC is an iterative process for approximate Bayesian inference. While it is slower than analytical approximations, it tends to be more accurate and has the added benefit of giving you the full posterior distribution. I’m not an expert in how it works internally, but <a href="http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/">this post</a> looks like a good place to start.</p>

<p>To implement this, I first defined a <a href="http://mc-stan.org/documentation/case-studies/radon.html">Hierarchical Model</a> of my data. The model is a description of how I think the data is generated: True means are sampled from a normal distribution, and observations are sampled from a normal distribution centered around the true mean of each group. Of course, I know exactly how my data was generated, because I was the one who generated it! The key thing to understand though is that the Hierarchical Model does not contain any information about the value of the parameters. It’s the MCMC’s job to figure that out. In particular, I used <a href="https://pystan.readthedocs.io/en/latest/">PyStan’s</a> MCMC implementation to fit the parameters of the model based on my data, although I later <a href="https://twitter.com/talyarkoni/status/859837371034062848">learned</a> that it would be even easier to use <a href="https://github.com/bambinos/bambi">bambi</a>.</p>

<figure class="image"><img src="/assets/2017_shrinkage/fig_shared_4.png" alt="" /></figure>

<div class="caption"><strong>Figure 6.</strong> Mean Squared Error between true group means and estimated group means. In this simulation, the $ \epsilon $ parameter for within-group variance of observations is shared by all groups.</div>
<p><br /></p>

<p>For simulated data with shared $ \epsilon^2 $, MCMC did well, outperforming both the MSS James-Stein estimator and the MSS Pooled James-Stein estimator.</p>

<p>If you don’t care about speed and are willing to write the Stan code, then this is probably your best option. It’s also good to learn about MCMC methods, since they can be applied to more complicated models with multiple variables. But if you just want a quick estimate of group means, then one of the analytic solutions above makes more sense.</p>

<h3 id="other-solutions">Other solutions</h3>

<p>There are several other solutions that I did not include in my simulations.</p>

<ol>
  <li>
    <p><a href="https://twitter.com/seanjtaylor/status/851227324796157953">Regularization</a>. Pick the set of $ \hat{\theta_i} $’s that minimize <script type="math/tex">\sum{(\hat{\theta_i} - x_i)^2} + \lambda \sum{(\hat{\theta_i} - \overline{X})^2}</script>. Use cross-validation to choose the best $ \lambda $. This will probably work pretty well, although it takes a bit more work and time than the analytic solutions described above.</p>
  </li>
  <li>
    <p><a href="http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf">Mixed Models</a>. Over in Mixed Models World, there’s a whole ’nother literature on how to shrink estimates depending on sample size. They are especially suited for the movie ratings situation because in addition to shrinking the means, they also can correct for rater bias. I don’t really understand all the math behind Mixed Models, but I was able to use <a href="https://cran.r-project.org/web/packages/lme4/lme4.pdf">lme4</a> to estimate group means in simulated data under the assumption that the group means are a random effect. This gave me slightly different results compared to the James-Stein / Empirical Bayes approach. I would love if some expert who understood this could write an accessible and authoritative blog post on the differences between Mixed Models and Empirical Bayes. The closest I could find was <a href="http://projecteuclid.org/download/pdf_1/euclid.ss/1177011928">this comment</a> by David Harville.</p>
  </li>
  <li>
    <p><a href="http://www.stat.cmu.edu/~acthomas/724/Efron-Morris.pdf">Efron and Morris</a>’ generalization of James-Stein to unequal sample sizes (Section 3 of their paper). I thought this paper was difficult to read. A more accessible presentation can be found at the end of <a href="http://www.tandfonline.com/doi/abs/10.1080/09332480.2007.10722861">this column</a>. The Efron and Morris approach is a numerical solution that seemed to work reasonably well when I played around with it, but I didn’t take it very far. If you want to implement it, be sure to prevent any variance estimates from falling below zero. If one of them does, just set it to zero and then compute your estimates of the means. That being said, I feel like if you’re going to go with a numerical solution, you may as well just go with MCMC.</p>
  </li>
  <li>
    <p><a href="http://intlpress.com/site/pub/files/_fulltext/journals/sii/2010/0003/0004/SII-2010-0003-0004-a011.pdf">Double Shrinkage</a>. When we think that different groups not only have different sample sizes, but also different $ \epsilon_i $’s, we are faced with an interesting conundrum. As shown above, the MSS James-Stein Estimator outperforms the MSS Pooled James-Stein Estimator, because it computes $ \hat{\epsilon_i} $’s specific to each group. However, these estimates of group variances are probably noisy! Just like we don’t trust the raw estimates of group sample means, why should we trust the raw estimates of group sample variances? One way to address this is to use Zhao’s Double Shinkage Estimator, which not only shrinks the means, but also shrinks the variances.</p>
  </li>
  <li>
    <p><a href="https://www.jstor.org/stable/2284137?seq=1#fndtn-page_scan_tab_contents">Kleinman’s weighted moment estimator</a>. Apparently this was motivated by groups of proportions (i.e. the Rotten Tomatoes case), but the estimator can be applied generally.</p>
  </li>
</ol>

<h3 id="conclusion">Conclusion</h3>
<p>Which method you choose depends on your situation. If you want a simple and computationally fast estimate, and if you don’t want to assume that the group variances $ \epsilon^{2}_i $ are identical, I would recommend either the MSS James-Stein Estimator or the Double Shrinkage Estimator if you can get it to work. If you want a fast estimate and can assume all groups share the same $ \epsilon^{2} $, I’d recommend the MSS Pooled James-Stein Estimator. If you don’t care about speed or code complexity, I’d recommend MCMC, Mixed Models, or regularization with a cross-validated penalty term.</p>

<h3 id="acknowledgements">Acknowledgements</h3>
<p>Special thanks to the many people who responded to my <a href="https://twitter.com/Chris_Said/status/851211601445240832">original question on Twitter</a>, including: <a href="https://twitter.com/seanjtaylor">Sean Taylor</a>, <a href="https://twitter.com/johnmyleswhite">John Myles White</a>, <a href="https://twitter.com/drob">David Robinson</a>, <a href="https://twitter.com/stat110">Joe Blitzstein</a>, <a href="https://twitter.com/NeuroStats">Manjari Narayan</a>, <a href="https://twitter.com/oldjacket">Otis Anderson</a>, <a href="https://twitter.com/alex_peys">Alex Peysakhovich</a>, <a href="https://twitter.com/bechhof">Nathaniel Bechhofer</a>, <a href="https://twitter.com/jamneuf">James Neufeld</a>, <a href="https://twitter.com/awmercer">Andrew Mercer</a>, <a href="https://twitter.com/ptrckprry">Patrick Perry</a>, <a href="https://twitter.com/RichmanRonald">Ronald Richman</a>, <a href="https://twitter.com/Hacktuarial">Timothy Sweetster</a>, <a href="https://twitter.com/talyarkoni">Tal Yarkoni</a> and <a href="https://twitter.com/AlxCoventry">Alex Coventry</a>. Special thanks also to Marika Inhoff and Leo Pekelis for many discussions.</p>

<p>All code used in this blog post is available on <a href="https://github.com/csaid/empirical_bayes">GitHub</a>.</p>



    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2017/05/03/empirical-bayes-for-multiple-sample-sizes/#disqus_thread" data-disqus-identifier="http://localhost:4000/2017/05/03/empirical-bayes-for-multiple-sample-sizes/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2017/05/03/empirical-bayes-for-multiple-sample-sizes/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Empirical Bayes for multiple sample sizes">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2016/05/11/optimizing-things-in-the-ussr/">
        Optimizing things in the USSR
      </a>
    </h1>

    <span class="post-date">11 May 2016</span>

    <meta charset="utf-8" />

<style>

.node rect {
  cursor: move;
  fill-opacity: .9;
  shape-rendering: crispEdges;
}

.node text {
  pointer-events: none;
  text-shadow: 0 1px 0 #fff;
}

.link {
  fill: none;
  stroke: #000;
  stroke-opacity: .2;
}

.link:hover {
  stroke-opacity: .5;
}


text {
  font: 10px sans-serif;
}


</style>

<p>As a data scientist, a big part of my job involves picking metrics to optimize and thinking about how to do things as efficiently as possible. With these types of questions on my mind, I recently discovered a totally fascinating book about about economic problems in the USSR and the team of data-driven economists and computer scientists who wanted to solve them. The book is called <a href="http://www.amazon.com/Red-Plenty-Francis-Spufford/dp/1555976042"><em>Red Plenty</em></a>. It’s actually written as a novel, weirdly, but it nevertheless presents an accurate economic history of the USSR. It draws heavily on an earlier book from 1973 called <a href="http://www.amazon.com/Planning-Problems-USSR-Contribution-Mathematical/dp/0521202493"><em>Planning Problems in the USSR</em></a>, which I also picked up. As I read these books, I couldn’t help but notice some parallels with planning in any modern organization. In what will be familiar to any data scientist today, the second book even includes a quote from a researcher who complained that 90% of his time was spent cleaning the data, and only 10% of his time was spent doing actual modeling!</p>

<p>Beyond all the interesting parallels to modern data science and operations research, these books helped me understand a lot of interesting things I previously knew very little about, such as linear programming, price equilibria, and Soviet history. This blog post is about I learned, and ends with some surprising-to-me speculation about whether the technical challenges that the brought down the Soviet Union would be as much of a problem in the future.</p>

<h4 style="text-align: center;">Balance sheets and manual calculation: Kind of a trainwreck</h4>

<p>The main task in the centrally planned Soviet economy was to allocate resources so that a desired assortment of goods and services was produced. Every year, certain target outputs for each good were established. Armed with estimates of the available input resources, central administrators used balance sheets to set plans for every factory, specifying exactly how much input commodities each factory would receive, and how much output it should produce. Up through the 1960s, this was always done by manual calculation. Since there were hundreds of thousands of commodities, and since the supply chains had many dependency steps, it was impossible to compute the full balance sheets for the economy. The administrators therefore decided to make some simplifying assumptions. As a result of these these simplifying assumptions, resource allocation became a bit of a trainwreck. Below are a few of the simplifications and their consequences.</p>

<ul>
  <li><strong>Dimensionality reduction by removing variables.</strong> Because there were too many commodities to track, administrators often limited their analysis to the 10,000 most important commodities in the economy. But when the production of those commodities were planned, there was often a hidden shortage of commodities whose output was not planned centrally but which were used as inputs to one of the 10,000 planned products. Factories that depended on those commodities often sat idle for months as they waited for the shortages to end.</li>
  <li><strong>Dimensionality reduction by aggregation.</strong> Apparently, steel tubes can come in thousands of different types. They can come in different lengths, different shapes, and different compositions. To reduce the dimensionality of the problem, administrators would often track the total tonnage of a few broad classes of steel tubes in the models, rather than using a more detailed classification scheme. While their models successfully balanced the tonnage of tubes for the broad categories (the output in tons of tube-producing factories matched the input requirements in tons of tube-consuming factories), there were constant surpluses of some specific types of tubes, and shortages of other specific types of tubes. In particular, since tonnage was used as a metric, tube-producing factories were overly incentivized to make easy-to-produce thick tubes. As a result, thin tubes were always in short supply.</li>
  <li><strong>Propagating adjustments only a few degrees back.</strong> Let’s say that during balance calculations, the administrators realized they needed to bump up the target output of one commodity. If they did that, it was also necessary to bump up the output targets of commodities that were input into the target commodity. But if they did <em>that</em>, they also needed to bump up the output targets of commodities that fed into those commodities, and so on! This involved a crazy amount of extra hand calculations every time they needed make an adjustment. To simplify things, the administrators typically made adjustments to the first-order suppliers, without making the necessary adjustments to the suppliers of the suppliers. This of course led to critical shortages of input commodities, which again led to idle factories.</li>
</ul>

<!-- The tooltip has absolute positioning, which means it is positioned
"relative" to any parent it has who has either absolute or relative positioning.
The #econ_scatter parent would by default be static, so I have to change it to
relative -->
<div class="wrapper">
  <div id="chart" class="inner" style="position:relative"></div>
  <!-- <div id="econ_scatter" class="inner" style="position:relative"></div> -->
  <div class="caption">
<strong>Figure 1.</strong> Some example inputs and outputs in the Soviet economy in 1951, described in units of weight. This summary shows an extreme dimensionality reduction, more extreme than was ever used in planning. In this diagram, most commodities are excluded and each displayed commodity collapses across multiple different product types. Multiple steps in the supply chain are collapsed into a single step. (Source: <a href="http://www.foia.cia.gov/sites/default/files/document_conversions/89801/DOC_0000380738.pdf">CIA</a>)
  </div>
</div>

<p><br />Even if the administrators could get the accounting correct, which they couldn’t, their attempts to allocate resources would still be far from optimal. In the steel industry, for example, some factories were better at producing some types of tubes whereas others were better at producing other types of tubes. Since there were thousands of different factories and tube types, it was non-trivial to decide how to best distribute resources and output requirements, and it was not immediately obvious which factories should be expanded and which should be closed down.</p>

<h4 style="text-align: center;">Supply chain optimizations</h4>

<p>In the late 1960’s, a group of economists and computer scientists known as the “optimal planners” began to push for a better way of doing things. The group argued that a technique called <a href="https://www.math.ucla.edu/~tom/LP.pdf">linear programming</a>, invented by <a href="https://en.wikipedia.org/wiki/Leonid_Kantorovich">Leonid Kantorovich</a>, could optimally solve the problems with the supply chain. At a minimum, since the process could be computerized, it would be possible to perform more detailed calculations than could be done by hand, with less dimensionality reduction. But more importantly, linear programming allowed you to optimize arbitrary objective functions given certain constraints. In the case of the supply chain, it showed you how to efficiently allocate resources, identifying efficient factories that should get more input commodities, and inefficient factories that should be shut down.</p>

<div class="wrapper">
  <img src="/assets/kantorovich.jpg" height="300" class="inner" style="position:relative" />
  <div class="caption">
    <strong>Figure 2.</strong> Leonid Kantorovich, inventor of linear programming and winner of the 1975 Nobel Prize in Economics.
  </div>
</div>

<p><br />The optimal planners had some success here. For example, in the steel industry, about 60,000 consumers requested 10,000 different types of products from 500 producers. The producers were not equally efficient in their production. Some producers were efficient for some types of steel products, but less efficient for other types of steel products. Given the total amount of each product requested, and given the constraints of how much each factory can produce, the goal was decide how much each factory should produce of each type of product. If we simplify the problem by just asking how much each factory should produce without considering how the products will be distributed to the consuming factories, this becomes a straightforward application of the <a href="https://www.math.ucla.edu/~tom/LP.pdf">Optimal Assignment Problem</a>, a well-studied example in linear programming. If we additionally want to optimize distribution, taking into account the distance-dependent costs of shipments from one factory to another, the problem becomes more complicated but is still doable. The problem becomes similar to the <a href="https://www.math.ucla.edu/~tom/LP.pdf">Transportation Problem</a>, another well-studied example in linear programming, but in this case generalized to multiple commodities instead of just one.</p>

<div class="wrapper">
  <img src="/assets/steel_tubes.jpg" height="200" class="inner" style="position:relative" />
</div>

<p>By introducing linear programming, the optimal planners were modestly successful at improving the efficiency of some industries, but their effect was limited. First, political considerations prevented many of the recommendations surfaced by the model from being implemented. Cement factories that were known to be too inefficient or too far away from consumers were allowed to remain open even though the optimal solution recommended that they be closed. Second, since the planners were only allowed to work in certain narrow parts of the economy, they never had an opportunity to propagate their recommendations back in the supply chain, although one could imagine extending the models to do so. Third, and perhaps most importantly, the value of each commodity was set by old-school administrators in an unprincipled way, and so the optimal planners were forced to optimize objective functions that didn’t even make sense.</p>

<h4 style="text-align: center;">Ideas about optimizing the entire economy</h4>

<p>While the optimal planners were able to improve the efficiency of a few industries, they had more ambitious plans. They believed they could use linear programming to optimize the entire economy and outperform capitalist societies. Doing so involved more than just scaling out the supply chain optimizations adopted by certain industries. It involved shadow prices and interest rates, and a few other things I’ll admit I don’t totally understand. But while I don’t really understand the implementation, I feel like the broader goal of the planners is easier to understand and explain:</p>

<p>Basically, in a completely free market, at least under <a href="https://en.wikipedia.org/wiki/Arrow%E2%80%93Debreu_model">certain assumptions</a>, prices are supposed to converge to what’s called a General Equilibrium. The equilibrium prices have a some nice properties. They balance aggregate supply and demand, so that no commodities are in shortage or surplus. They are also <a href="https://en.wikipedia.org/wiki/Pareto_efficiency">Pareto efficient</a>, which means that nobody in the economy can be made better off without making someone else worse off.</p>

<p>The optimal planners thought that they could do better. In particular, they pointed to two problems with capitalism: First, prices in a capitalist society were determined by individual agents using trial and error to guess the best price. Surely these agents, who had imperfect information, were not picking the exactly optimal prices. In contrast, a central planner using optimal computerized methods could pick prices that hit the equilibrium more exactly. Second, and more importantly, capitalism targeted an objective function that — while Pareto efficient — was not socially optimal. Because of huge differences in wealth, some people were able to obtain far more goods and services than other people. The optimal planners proposed using linear programming to optimize an objective function that would be more socially optimal. For example, it could aim to distribute goods more equitably. It could prioritize certain socially valuable goods (e.g. books) over socially destructive goods (e.g. alcohol). It could prioritize sectors that provide benefits over longer time horizons (e.g. heavy industry). And it could include constraints to ensure full employment.</p>

<h4 style="text-align: center;">What happened</h4>
<p>None of this ever really happened. The ambitious ideas of the optimal planners were never adopted, and by the 1970s it was clear that living standards in the USSR were falling further behind those of the West. Perhaps things would have been better if the optimal planners got their way, but it seems like the consensus is that their plans would have failed even if they were implemented. Below are some of the main problems that would have been encountered.</p>

<ul>
  <li><strong>Computational complexity.</strong> As described in a <a href="http://crookedtimber.org/2012/05/30/in-soviet-union-optimization-problem-solves-you/">wonderful blog post by Cosma Shalizi</a>, the number of calculations needed to solve a linear programming problem is: <script type="math/tex">(m+n)^{3/2} n^2 log(1/h)</script>, where <script type="math/tex">n</script> is the number of products, <script type="math/tex">m</script> is the number of constraints, and <script type="math/tex">h</script> is how much error you are willing to tolerate. Since the number of products, <script type="math/tex">n</script>, was in the millions, and since the complexity was proportional to <script type="math/tex">n^{3.5}</script>, it would have been practically impossible for the Soviets to compute a solution to their planning problem with sufficient detail (although see below). Any attempt to reduce the dimensionality would lead to the same perverse incentives and shortages that bedeviled earlier systems driven by hand calculations.</li>
  <li><strong>Data quality.</strong> The optimal planners thought that optimal computer methods could find prices that more exactly approximated equilibrium than could be done in a market economy, where fallible human actors guessed at prices by trial and error. The reality, however, would have been the exact opposite. Individual actors in a market economy understand their local needs and constraints pretty well, whereas central planners have basically no idea what’s going on. For example, central planners don’t have good information on when a factory fails to receive a shipment and they don’t have an accurate sense for how much more efficient some devices are than others. Even worse, in order to obtain more resources, factory managers in the USSR routinely <em>lied</em> to the central planners about their production capabilities. The situation became so bad that, <a href="https://books.google.com/books?id=Jlmm9GZqxkoC&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false">according to</a> <a href="http://crookedtimber.org/2012/05/30/in-soviet-union-optimization-problem-solves-you/#comment-416126">one of the deep state secrets</a> of the USSR, central planners preferred to use the CIA’s analyses of certain Russian commodities rather than reports from local Party bosses!   This is especially crazy if you consider that the CIA described its own data as being of <a href="http://www.foia.cia.gov/sites/default/files/document_conversions/89801/DOC_0000380738.pdf">“debilitatingly”</a> poor quality.</li>
  <li><strong>Nonlinearities.</strong> The optimal planners assumed linearity, such that the cost for a factory producing its 1000th widget was assumed to be the same as the cost for producing its first widget. In the real world, this is obviously false, as there are increasing returns to scale. It’s possible to model increasing returns to scale, but it becomes harder to solve computationally.</li>
  <li><strong>Choosing an objective function.</strong> Choosing what the society should value is really a political problem, and Cosma Shalizi does a very <a href="http://crookedtimber.org/2012/05/30/in-soviet-union-optimization-problem-solves-you/">nice job</a> describing why it would be so hard to come to agreement.</li>
  <li><strong>Incentives for innovation.</strong> The central planners couldn’t determine resource allocation for products that didn’t exist yet, and more importantly neither they nor the factories had much incentive to invent new products. That’s why the Soviet Union remained so focused on the steel/coal/cement economy while Western nations shifted their focus to plastics and microelectronics.</li>
  <li><strong>Political resistance.</strong> As described in a previous example, the model-based recommendations to shut down certain factories were ignored for political reasons. It is likely that many recommendations for the broader economy would have been ignored as well. For example, if a computer recommended that the price of heating oil should be doubled in the winter, how many politicians would let that happen?</li>
</ul>

<h4 style="text-align: center;">Could this work in the future?</h4>

<p>Had the optimal planners’ ideas been adopted at the time, they would have failed. But what about the future? In a hundred years, could we have the technical capability to pull off a totally planned economy? I did some poking around the internet and found, somewhat to my surprise, that the answer is actually… <em>maybe</em>. It turns out that two of the most serious problems with central planning could have technological solutions that may seem far-fetched but are perhaps not impossible:</p>

<p>Let’s start with <strong>computational complexity</strong>. As described above and in <a href="http://crookedtimber.org/2012/05/30/in-soviet-union-optimization-problem-solves-you/">Cosma Shalizi’s post</a>, the number of steps required to solve a linear programming problem with <script type="math/tex">n</script> products and <script type="math/tex">m</script> constraints is proportional to <script type="math/tex">(m+n)^{3/2} n^2</script>. The USSR had about 12 million types of goods. If you cross them over about 1000 possible locations, that gives you 12 billion variables, which according to Cosma would correspond to an optimization problem that would take a thousand years to solve on a modern desktop computer. However, if Moore’s Law holds up, it would be possible in 100 years to solve this problem reasonably quickly. It’s also worth pointing out that the economy’s input-output matrix is sparse, since not every product depends on every other product as input. It <em>may</em> be possible that someone might develop a faster algorithm that leverages this sparsity, although Cosma is somewhat skeptical that this could happen. <em>[In an earlier version of this post, I discussed a <a href="http://ricardo.ecn.wfu.edu/~cottrell/socialism_book/new_socialism.pdf">sparsity-based proposal</a> that supposedly brought things down to <script type="math/tex">m \times n</script> complexity. This was apparently a <a href="http://bactra.org/weblog/919.html">red herring</a> that doesn’t actually solve the optimization problem.]</em></p>

<p>As described earlier, the second serious issue with a centrally planned economy was <strong>data quality</strong>: Central planners’ knowledge about the input requirements and output capabilities of individual factories was simply not as good as the people actually working in the factory. While this was certainly the case in the Soviet Union, one can’t help but wonder about technological improvements in supply chain management. Imagine if every product had a GPS device to track its location, with other sensors and cameras to determine product quality. Already Amazon is moving in that direction for pretty much all consumer goods, and one could imagine a world where demand could be measured with the <a href="https://en.wikipedia.org/wiki/Internet_of_Things">Internet of Things</a>. Whether a government would be able to harness this data as competently as Amazon is doubtful, and it’s obviously worth asking whether we would ever want a government to be using that type of data. But from a technical point of view it’s interesting to think about how the data quality issues that destroyed the USSR could be much less serious in the future.</p>

<p>All that being said, it’s still unclear to me how an objective function could be chosen in way that would democratically satisfy people, how innovation could be incentivized, or how political freedoms could be preserved. In terms of freedom, things were obviously not-so-hot in the Soviet Union. If you’d like to read more about this, you should definitely check out <a href="http://www.amazon.com/Red-Plenty-Francis-Spufford/dp/1555976042">Red Plenty</a>. It was one of the weirdest and most interesting books I have read.</p>

<div class="wrapper">
  <img src="/assets/red_plenty.jpeg" height="300" class="inner" style="position:relative" />
</div>

<script src="/scripts/sankey.js"></script>

<script>

// Names to Index
var n2i = {
'petro': 0,
'steel': 1,
'copper': 2,
'aluminum': 3,
'ammonia': 4,
'food': 5,
'textiles': 6,
'logging': 7,
'automotive': 8
}

var commodities = {
  "nodes":[
    {'name': 'Petroleum residuals'},
    {'name': 'Crude Steel'},
    {'name': 'Copper'},
    {'name': 'Aluminum'},
    {'name': 'Ammonia'},
    {'name': 'Agriculture and Food'},
    {'name': 'Textiles and Apparel'},
    {'name': 'Logging and Paper Products'},
    {'name': 'Automotive Equipment'}
  ],
  'links':[
    {'source':n2i['petro'],'target':n2i['food'],'value':510},
    {'source':n2i['steel'],'target':n2i['food'],'value':80},
    {'source':n2i['ammonia'],'target':n2i['food'],'value':250},
    {'source':n2i['steel'],'target':n2i['textiles'],'value':10},
    {'source':n2i['ammonia'],'target':n2i['textiles'],'value':10},
    {'source':n2i['petro'],'target':n2i['logging'],'value':30},
    {'source':n2i['steel'],'target':n2i['logging'],'value':40},
    {'source':n2i['ammonia'],'target':n2i['logging'],'value':5},
    {'source':n2i['petro'],'target':n2i['automotive'],'value':60},
    {'source':n2i['steel'],'target':n2i['automotive'],'value':1800},
    {'source':n2i['copper'],'target':n2i['automotive'],'value':14},
    {'source':n2i['aluminum'],'target':n2i['automotive'],'value':3}
  ]
}

if( /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) ) {
  var full_width = 400
  var full_height = 400;
} else {
  var full_width = 620
  var full_height = 500;
}

var margin = {top: 20, right: 40, bottom: 20, left: 40},
    width = full_width - margin.left - margin.right,
    height = full_height - margin.top - margin.bottom;

var formatNumber = d3.format(",.0f"),
    format = function(d) { return formatNumber(d) + " Th MT"; },
    color = d3.scale.category20();

var svg = d3.select("#chart").append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");


var sankey = d3.sankey()
    .nodeWidth(15)
    .nodePadding(10)
    .size([width, height]);

var path = sankey.link();

  sankey
      .nodes(commodities.nodes)
      .links(commodities.links)
      .layout(32);

  var link = svg.append("g").selectAll(".link")
      .data(commodities.links)
    .enter().append("path")
      .attr("class", "link")
      .attr("d", path)
      .style("stroke-width", function(d) { return Math.max(1, d.dy); })
      .sort(function(a, b) { return b.dy - a.dy; });

  link.append("title")
      .text(function(d) { return d.source.name + " → " + d.target.name + "\n" + format(d.value); });

  var node = svg.append("g").selectAll(".node")
      .data(commodities.nodes)
    .enter().append("g")
      .attr("class", "node")
      .attr("transform", function(d) { return "translate(" + d.x + "," + d.y + ")"; })
    .call(d3.behavior.drag()
      .origin(function(d) { return d; })
      .on("dragstart", function() { this.parentNode.appendChild(this); })
      .on("drag", dragmove));

  node.append("rect")
      .attr("height", function(d) { return d.dy; })
      .attr("width", sankey.nodeWidth())
      .style("fill", function(d) { return d.color = color(d.name.replace(/ .*/, "")); })
      .style("stroke", function(d) { return d3.rgb(d.color).darker(2); })
    .append("title")
      .text(function(d) { return d.name + "\n" + format(d.value); });

  node.append("text")
      .attr("x", -6)
      .attr("y", function(d) { return d.dy / 2; })
      .attr("dy", ".35em")
      .attr("text-anchor", "end")
      .attr("transform", null)
      .text(function(d) { return d.name; })
    .filter(function(d) { return d.x < width / 2; })
      .attr("x", 6 + sankey.nodeWidth())
      .attr("text-anchor", "start");


  // Input label
  svg.append("text")
      .attr("transform", "rotate(-90)")
      .attr("x", -height/2)
      .attr("y", -20)
      .attr("dy", ".71em")
      .style("text-anchor", "middle")
      .style("font-size", "16px")
      .text("Inputs");

  // Output label
  svg.append("text")
      .attr("transform", "rotate(-90)")
      .attr("x", -height/2)
      .attr("y", width+10)
      .attr("dy", ".71em")
      .style("text-anchor", "middle")
      .style("font-size", "16px")
      .text("Outputs");



  function dragmove(d) {
    d3.select(this).attr("transform", "translate(" + d.x + "," + (d.y = Math.max(0, Math.min(height - d.dy, d3.event.y))) + ")");
    sankey.relayout();
    link.attr("d", path);
  }
// });
</script>



    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2016/05/11/optimizing-things-in-the-ussr/#disqus_thread" data-disqus-identifier="http://localhost:4000/2016/05/11/optimizing-things-in-the-ussr/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2016/05/11/optimizing-things-in-the-ussr/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Optimizing things in the USSR">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>

<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'csaid81'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function () {
  var s = document.createElement('script'); s.async = true;
  s.type = 'text/javascript';
  s.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
  }());
</script>


    </div>

  </body>
</html>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">


MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


<script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
