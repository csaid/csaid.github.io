<!DOCTYPE html>
<html lang="en-us">

<!--
It's bad to import d3 in every post separately (https://groups.google.com/forum/#!topic/d3-js/bwdNirt2uEU).
Importing it globally here.
Putting this up at the top, according to this controversial stack overflow answer
http://stackoverflow.com/questions/7169370/d3-js-and-document-onready
 -->
<script src="http://d3js.org/d3.v3.min.js"></script>

<link rel="stylesheet" href="/public/font-awesome-4.4.0/css/font-awesome.min.css">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
    <meta name="description" content="My name is Chris Said and I am a data scientist at Opendoor. This blog is mostly about tech, stats, and science.">
  

  <!-- To get a link preview image, just set the image attribute in your _post -->
  

  <!-- twitter standard -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@chris_said" />
  <meta name="twitter:title" content="Home" />
  <meta name="twitter:description" content="" />


  <title>
    
      The File Drawer &middot; A blog by Chris Said
    
  </title>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-67746868-1', 'auto');
    ga('send', 'pageview');
  </script>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

<!--   <body class="theme-base-cps">
 -->
  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          The File Drawer
        </a>
      </h1>
      <p class="lead">My name is Chris Said and I am a data scientist at Opendoor. This blog is mostly about tech, stats, and science.</p>

    </div>

    <nav class="sidebar-nav">

      
        <a class="sidebar-nav-item" href="/">Home</a>
      
        <a class="sidebar-nav-item" href="/archive">Archive</a>
      
        <a class="sidebar-nav-item" href="/atom.xml">Feed</a>
      

    </nav>

    <div class="wrapper">
      <div class="inner">
        <a href = "http://www.twitter.com/Chris_Said" class="contact-button"><i class="fa fa-twitter fa-2x"></i></a>
        <a href = "https://www.linkedin.com/pub/chris-said/6b/86b/979" class="contact-button"><i class="fa fa-linkedin-square fa-2x"></i></a>
        <a href = "mailto:chris.said@gmail.com" class="contact-button"><i class="fa fa-envelope fa-2x"></i></a>
      </div>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2015/05/31/were-getting-better-at-picking-metrics-to-optimize/">
        Across industries, we’re getting better at picking metrics
      </a>
    </h1>

    <span class="post-date">31 May 2015</span>

    <p>Everywhere you look, people are optimizing bad metrics. Sometimes people optimize metrics that aren’t in their self interest, like when startups focus entirely on signup counts while forgetting about retention rates. In other cases, people optimize metrics that serve their immediate short term interest but which are bad for social welfare, like when California corrections officers <a href="http://mic.com/articles/41531/union-of-the-snake-how-california-s-prison-guards-subvert-democracy">lobby for longer prison sentences</a>.</p>

<p>The good news is that as we become a more data-driven society, there seems to be a broad trend — albeit a very slow one — towards better metrics. Take the media economy, for example. A few years ago, media companies optimized for clicks, and companies like Upworthy thrived by producing low quality content with clickbaity headlines. But now, thanks to a more sustainable <a href="https://stratechery.com/2015/buzzfeed-important-news-organization-world/">business model</a>, companies like Buzzfeed are <a href="http://www.buzzfeed.com/bensmith/why-buzzfeed-doesnt-do-clickbait">optimizing for shares rather than clicks</a>. It’s not perfect, but overall it’s better for consumers.</p>

<p>In science, researchers used to optimize for publication counts and citation counts, which biased them towards publishing surprising and interesting results that were unlikely to be true. These metrics <a href="http://www.talyarkoni.org/blog/2013/03/12/the-truth-is-not-optional-five-bad-reasons-and-one-mediocre-one-for-defending-the-status-quo/">still loom large</a>, but increasingly scientists are beginning to optimize for other metrics like <a href="https://twitter.com/lakens/status/603617310298001410">open data badges</a> and <a href="http://sometimesimwrong.typepad.com/wrong/2014/12/why-i-am-optimistic.html">reproducibility</a>, although we still have a long way to go before quality metrics are effectively <a href="http://journal.frontiersin.org/researchtopic/beyond-open-access-visions-for-open-evaluation-of-scientific-papers-by-post-publication-peer-review-137">measured</a> and <a href="/2012/04/17/its-the-incentives-structure-people-why-science-reform-must-come-from-the-granting-agencies/">incentivized</a>.</p>

<p>In health care, hospitals used to profit by maximizing the quantity of care. Perversely, hospitals benefited whenever patients were readmitted due to infections acquired in the hospital or due to lack of adequate follow-up plan. Now, with <a href="http://healthaffairs.org/blog/2014/07/24/examining-medicares-hospital-readmissions-reduction-program/">ACA</a> <a href="http://www.hhs.gov/asl/testify/2013/09/t20130924.html">policies</a> that penalize hospitals for avoidable readmissions, hospitals are taking <a href="https://innovations.ahrq.gov/profiles/statewide-all-payer-financial-incentives-significantly-reduce-hospital-acquired-conditions">real steps</a> to improve follow-up care and to reduce hospital-acquired infections. While the metrics should <a href="http://en.wikipedia.org/wiki/Value-added_modeling">be</a> <a href="https://www.aamc.org/download/382516/data/thehospitalreadmissionsprogramaccuracyandaccountabilityactbills.pdf">adjusted</a> so that they don’t unfairly penalize low income hospitals, the overall emphasis on quality rather than quantity is moving things <a href="http://www.cdc.gov/hai/progress-report/index.html">in the right direction</a>.</p>

<p>We still are light years from where we need to be, and bad incentives continue to plague everything from government to finance to education. But slowly, as we get better at measuring and storing data, I think we are getting at picking the right metrics.</p>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2015/05/31/were-getting-better-at-picking-metrics-to-optimize/#disqus_thread" data-disqus-identifier="http://localhost:4000/2015/05/31/were-getting-better-at-picking-metrics-to-optimize/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2015/05/31/were-getting-better-at-picking-metrics-to-optimize/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Across industries, we’re getting better at picking metrics">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/11/30/independent-t-tests-and-the-83-confidence-interval-a-useful-trick-for-eyeballing-your-data/">
        Independent t-tests and the 83% confidence interval: A useful trick for eyeballing your data.
      </a>
    </h1>

    <span class="post-date">30 Nov 2014</span>

    <p>Like most people who have analyzed data using frequentist statistics, I have often found myself staring at error bars and trying to guess whether my results are significant. When comparing two independent sample means, this practice is confusing and difficult. The conventions that we use for testing differences between sample means are not aligned with the conventions we use for plotting error bars. As a result, it’s fair to say that there’s a lot of confusion about this issue.</p>

<p>Some people believe that two independent samples have significantly different means if and only if their standard error bars (68% confidence intervals for large samples) don’t overlap. This belief is incorrect. Two samples can have nonoverlapping standard error bars and still fail to reach statistical significance at <script type="math/tex">\alpha=.05</script>. Other people believe that two means are significantly different if and only if their 95% confidence intervals overlap. This belief is also incorrect. For one sample t-tests, it is true that significance is reached when the 95% confidence interval crosses the test parameter <script type="math/tex">\mu_0</script>. But for two-sample t-tests, which are more common in research, statistical significance can occur with overlapping 95% confidence intervals.</p>

<p>If neither the 68% confidence interval nor the 95% confidence interval tells us anything about statistical significance, what does? In most situations, the answer is the 83.4% confidence interval. This can be seen in the figure below, which shows two samples with a barely significant difference in means (p=.05). Only the 83.4% confidence intervals shown in the third panel are barely overlapping, reflecting the barely significant results.</p>

<figure class="image"><img src="/assets/fig_errorbars1.png" alt="" /></figure>

<p>To understand why, let’s start by defining the t-statistic for two independent samples:</p>

<script type="math/tex; mode=display">\begin{align}
t = \frac{\overline{X_1} - \overline{X_2}}{\sqrt{se_1^2 + se_2^2}}
\end{align}</script>

<p>where <script type="math/tex">\overline{X_1}</script> and <script type="math/tex">\overline{X_2}</script> are the means of the two samples, and <script type="math/tex">se_1</script> and <script type="math/tex">se_2</script> are their standard errors. By rearranging, we can see that significant results will be barely obtained (<script type="math/tex">p=.05</script>) if the following condition holds:</p>

<script type="math/tex; mode=display">\begin{align}
\overline{X_1} - \overline{X_2} = 1.96\times\sqrt{se_1^2 + se_2^2}
\end{align}</script>

<p>where 1.96 is the large sample <script type="math/tex">t</script> cutoff for significance. Assuming equal standard errors (more on this later), the equation simplifies to:</p>

<script type="math/tex; mode=display">\begin{align}
\overline{X_1} - \overline{X_2} = 1.96\times{\sqrt{2}}\times{se}
\end{align}</script>

<p>On a graph, the quantity <script type="math/tex">\overline{X_1} - \overline{X_2}</script> is the distance between the means. If we want our error bars to just barely touch each other, we should set the length of the half-error bar to be exactly half of this, or:</p>

<script type="math/tex; mode=display">\begin{align}
1.386\times{se}
\end{align}</script>

<p>This corresponds to an 83.4% confidence interval on the normal distribution. While this result assumes a large sample size, it remains quite useful for sample sizes as low as 20. The 83.4% confidence interval can also become slightly less useful when the samples have strongly different standard errors, which can stem from very unequal sample sizes or variances. If you really want a solution that generalizes to this situation, you can set your half-error bar on your first sample to:</p>

<script type="math/tex; mode=display">\begin{align}
\frac{1.96\times{\sqrt{se_1^2 + se_2^2}}\times{se_1}}{se_1^2 + se_2^2}
\end{align}</script>

<p>and make the appropriate substitutions to compute the half-error bar in your second sample. However, this solution has the undesirable property that the error bar for one sample depends on the standard error of the other sample. For most purposes, it’s probably better to just plot the 83% confidence interval. If you are eyeballing data for a project that requires frequentist statistics, it is arguably more useful than plotting the standard error or the 95% confidence interval.</p>

<p><strong>Update</strong>: <a href="http://www.twitter.com/jeffrouder">Jeff Rouder</a> helpfully points me to <a href="http://www.researchgate.net/publication/11578767_Evaluating_statistical_difference_equivalence_and_indeterminacy_using_inferential_confidence_intervals_an_integrated_alternative_method_of_conducting_null_hypothesis_statistical_tests/file/5046351b1018420f84.pdf">Tryon and Lewis (2008)</a>, which presents an error bar that generalizes both to unequal standard errors and small samples. Like the last equation presented above, it has the undesirable property that the size of the error bar around a particular sample depends on both samples. But on the plus side, it’s guaranteed to tell you about significance.</p>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2014/11/30/independent-t-tests-and-the-83-confidence-interval-a-useful-trick-for-eyeballing-your-data/#disqus_thread" data-disqus-identifier="356 http://filedrawer.wordpress.com/?p=356"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2014/11/30/independent-t-tests-and-the-83-confidence-interval-a-useful-trick-for-eyeballing-your-data/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Independent t-tests and the 83% confidence interval: A useful trick for eyeballing your data.">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/10/16/jumping-quickly-between-deep-directories/">
        Jumping quickly between deep directories
      </a>
    </h1>

    <span class="post-date">16 Oct 2014</span>

    <p>I often need to jump between different directories with very deep paths, like this:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="nb">cd </span>some/very/deep/directory/project1
<span class="nv">$ </span><span class="c"># do stuff in Project 1</span>
<span class="nv">$ </span><span class="nb">cd </span>different/very/deep/directory/project2
<span class="nv">$ </span><span class="c"># do stuff in Project 2</span></code></pre></figure>

<p>While it only takes a handful of seconds to switch directories, the extra mental effort often derails my train of thought. Some solutions exist, but they all have their limitations. For example, <code class="highlighter-rouge">pushd</code> and <code class="highlighter-rouge">popd</code> don’t work well for directories you haven’t visited in a while. Aliases require you to manually add a new alias to your .bashrc every time you want to save a new directory.</p>

<p>I recently found a solution, inspired by <a href="http://jeroenjanssens.com/2013/08/16/quickly-navigate-your-filesystem-from-the-command-line.html">this post</a> from <a href="https://twitter.com/jeroenhjanssens">Jeroen Janssens</a>, that works great and feels totally natural. All it takes is a one-time change to your .bashrc that will allow you to easily save directories and switch between them. To save a directory, just use the <code class="highlighter-rouge">mark</code> function:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="nb">pwd
</span>some/very/deep/directory/project1
<span class="nv">$ </span>mark project1</code></pre></figure>

<p>To navigate to a saved directory, just use the <code class="highlighter-rouge">cdd</code> function:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cdd project1
<span class="c"># do stuff in Project 1</span>
<span class="nv">$ </span>cdd project2
<span class="c"># do stuff in Project 2</span></code></pre></figure>

<p>You can display a list of your saved directories with the <code class="highlighter-rouge">marks</code> function, and you can remove a directory from the list with the <code class="highlighter-rouge">unmark</code> function:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>unmark project1</code></pre></figure>

<p>For any of this to work, you’ll need to add this to your .bashrc, assuming you have a Mac and use the bash shell.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">    <span class="k">function </span>cdd <span class="o">{</span>
        <span class="nb">cd</span> <span class="nt">-P</span> <span class="s2">"</span><span class="nv">$MARKPATH</span><span class="s2">/</span><span class="nv">$1</span><span class="s2">"</span> 2&gt;/dev/null <span class="o">||</span> <span class="nb">echo</span> <span class="s2">"No such mark: </span><span class="nv">$1</span><span class="s2">"</span>
    <span class="o">}</span>
    <span class="k">function </span>mark <span class="o">{</span>
        mkdir <span class="nt">-p</span> <span class="s2">"</span><span class="nv">$MARKPATH</span><span class="s2">"</span><span class="p">;</span> ln <span class="nt">-s</span> <span class="s2">"</span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$MARKPATH</span><span class="s2">/</span><span class="nv">$1</span><span class="s2">"</span>
    <span class="o">}</span>
    <span class="k">function </span>unmark <span class="o">{</span>
        rm <span class="nt">-i</span> <span class="s2">"</span><span class="nv">$MARKPATH</span><span class="s2">/</span><span class="nv">$1</span><span class="s2">"</span>
    <span class="o">}</span>
    <span class="k">function </span>marks <span class="o">{</span>
        <span class="se">\l</span>s <span class="nt">-l</span> <span class="s2">"</span><span class="nv">$MARKPATH</span><span class="s2">"</span> | tail <span class="nt">-n</span> +2 | sed <span class="s1">'s/  / /g'</span> | cut <span class="nt">-d</span><span class="s1">' '</span> <span class="nt">-f9-</span> | awk <span class="nt">-F</span> <span class="s1">' -&gt; '</span> <span class="s1">'{printf "%-10s -&gt; %s\n", $1, $2}'</span>
    <span class="o">}</span>

    _cdd<span class="o">()</span>
    <span class="o">{</span>
        <span class="nb">local </span><span class="nv">cur</span><span class="o">=</span><span class="k">${</span><span class="nv">COMP_WORDS</span><span class="p">[COMP_CWORD]</span><span class="k">}</span>
        <span class="nv">COMPREPLY</span><span class="o">=(</span> <span class="k">$(</span><span class="nb">compgen</span> <span class="nt">-W</span> <span class="s2">"</span><span class="k">$(</span> <span class="nb">ls</span> <span class="nv">$MARKPATH</span> <span class="k">)</span><span class="s2">"</span> <span class="nt">--</span> <span class="nv">$cur</span><span class="k">)</span> <span class="o">)</span>
    <span class="o">}</span>
    <span class="nb">complete</span> <span class="nt">-F</span> _cdd cdd</code></pre></figure>

<p>This differs from Jeroen’s <a href="http://jeroenjanssens.com/2013/08/16/quickly-navigate-your-filesystem-from-the-command-line.html">original</a> code in a couple of ways. First, to be more brain-friendly, it names the function “cdd” instead of “jump”. Second, the tab completion works <a href="https://news.ycombinator.com/item?id=6229291">better</a>.</p>

<p>Update: <a href="https://twitter.com/johnvmcdonnell">John McDonnell</a> points me to <a href="https://github.com/joelthelion/autojump">autojump</a>.</p>



    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2014/10/16/jumping-quickly-between-deep-directories/#disqus_thread" data-disqus-identifier="http://localhost:4000/2014/10/16/jumping-quickly-between-deep-directories/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2014/10/16/jumping-quickly-between-deep-directories/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Jumping quickly between deep directories">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/07/10/how-failed-replications-change-our-effect-size-estimates/">
        How failed replications change our effect size estimates
      </a>
    </h1>

    <span class="post-date">10 Jul 2014</span>

    <p>Yesterday I posted a very unscientific <a href="https://www.surveymonkey.com/s/H7NM96W">survey</a> asking researchers to describe how failed replications changed their subjective estimates of effect sizes. The main survey asked for “ballpark estimates” of effect sizes, but an <a href="http://csaid.shinyapps.io/survey/">alternative interactive version</a> allowed researchers to also report their uncertainty by specifying both the mean and variance of their posterior distributions. Thanks to everyone who participated. I won’t be analyzing any new data after this, but it’s never too late to publicly share your estimates!</p>

<p>Here are the questions. </p>

<p><em><strong>Question 1</strong>. A 2009 experiment with 50 subjects (25 per cell) is published in Psych Science. The experiment does not require any special equipment other than a questionnaire. It is not pre-registered. The results show an effect size of d=0.5. Let’s define the true effect size to be the average effect size of an infinite number of replications that the original experimenter would deem “reasonably exact” in advance. Based on this information alone, what is your ballpark subjective estimate of the true effect size?</em></p>

<p><em><strong>Question 2</strong>. What if the experiment had been pre-registered? </em></p>

<p><em><strong>Question 3</strong>. Assume again that the experiment was not pre-registered. Now imagine that a pre-registered replication attempt with the same sample size estimated the effect size to be d=0.0. At the time of pre-registration, the original experimenter deemed it “reasonably exact”. Based on this replication and the original experiment, what is your ballpark subjective estimate of the true effect size?</em></p>

<p><em><strong>Question 4</strong>. What if the replication attempt had 300 subjects per cell?</em></p>

<p>Here are the results.</p>

<figure class="image"><img src="/assets/results.png" alt="" /></figure>

<p>Keeping in mind all the caveats about sampling bias and other issues, here are a few observations:</p>

<ul>
  <li>
    <p>The original study reported an effect size of d=0.5, but the results for Question 1 tell us that most researchers believed the true effect size was closer to d=0.2, which is roughly in line with my own estimate. Had I allowed researchers to <a href="http://csaid.shinyapps.io/survey/">state their uncertainty</a>, I suspect that many would find it quite possible that even the <em>sign</em> of the effect was wrong. This isn’t really surprising to me, but I think we should take a moment to reflect on what this means. When a scientist reports a result, most other researchers believe it is massively overstated. I know that there are still some researchers who want little or no changes to the status quo, but I’d like to live in a world where people actually believe the claims that scientists make. That’s why I’m a strong supporter of all the attempts to <a href="http://centerforopenscience.org/">fundamentally</a> <a href="https://pubpeer.com/">change</a> <a href="http://www.talyarkoni.org/blog/2013/03/12/the-truth-is-not-optional-five-bad-reasons-and-one-mediocre-one-for-defending-the-status-quo/">how</a> scientists do research.</p>
  </li>
  <li>
    <p>If you want people to have more confidence in your findings, pre-registration can make a big difference.</p>
  </li>
  <li>
    <p>While it’s not apparent from the plot, almost all respondents reduced their effect size estimate upon hearing about failed replications (Question 3 and 4 compared to Question 1).</p>
  </li>
  <li>
    <p>As some have pointed out, the original experiment falls a bit short of statistical significance. This was an oversight, as I forgot to check the p-value after changing some of the values. I don’t think this is a huge deal, since posterior estimates shouldn’t really depend too much on whether the results cross an arbitrary threshold. But apologies for the error.</p>
  </li>
  <li>
    <p>My estimates <a href="https://twitter.com/Chris_Said/status/487041593422016512">were</a> .25, .40, .10, .05.</p>
  </li>
  <li>
    <p>I wish I included another question asking what people would have thought of the original study if it was conducted in 2014.</p>
  </li>
</ul>



    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2014/07/10/how-failed-replications-change-our-effect-size-estimates/#disqus_thread" data-disqus-identifier="http://localhost:4000/2014/07/10/how-failed-replications-change-our-effect-size-estimates/"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2014/07/10/how-failed-replications-change-our-effect-size-estimates/"
              data-via="Chris_Said"
              data-count="none"
              data-text="How failed replications change our effect size estimates">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/07/07/jason-mitchells-essay/">
        Jason Mitchell's essay
      </a>
    </h1>

    <span class="post-date">07 Jul 2014</span>

    <p>As of yesterday I thought the debate about replication in psychology was converging on consensus in at least one respect. While there was still some disagreement about tone, <a href="http://www.spspblog.org/simone-schnall-on-her-experience-with-a-registered-replication-project/">basically</a> <a href="http://www.talyarkoni.org/blog/2013/03/12/the-truth-is-not-optional-five-bad-reasons-and-one-mediocre-one-for-defending-the-status-quo/">everyone</a> agreed that there was value in failed replications. But then this morning, Jason Mitchell posted <a href="http://wjh.harvard.edu/~jmitchel/writing/failed_science.htm">this essay</a>, in which he describes his belief that failed replication attempts can contain errors and therefore “cannot contribute to a cumulative understanding of scientific phenomena”. It’s hard to know where to begin when someone comes from a worldview so different from one’s own. Since there’s clearly a communication problem here, I’ll just give two examples to illustrate how I think about science.</p>

<ul>
  <li>
    <p><strong>Example 1</strong>. A rigorous lab conducts an experiment using a measurement device that requires special care. The effect size is d=0.5. Later, a different lab with no experience using the device tries to quickly replicate the experiment and computes an effect size of d=0.0.</p>
  </li>
  <li>
    <p><strong>Example 2</strong>. A small sample experiment in a field with a history of p-hacking shows an effect size of d=0.5. Another lab tries to replicate the study with a much larger sample and computes an effect size of d=0.0.</p>
  </li>
</ul>

<p>In both cases, I’d have subjective beliefs about the true effect size. For the first example, my posterior distribution might peak around d=0.4. For the second example, my posterior distribution might peak around d=0.1. In both cases, the replication would influence my posterior, but to varying degrees. In the first example, it would cause a small shift. In the second, it would cause a big shift. Reasonable people can disagree on the exact positions of the posteriors, but basically everyone ought to agree that our posteriors should incrementally adjust as we acquire new information, and that the size of these shifts should depend on a variety of factors, including the possibility of errors in either the original experiment or in the replication attempt. Maybe it’s because I’m stuck in a worldview, but none of this even seems very hard to understand. </p>

<p>Jason Mitchell <a href="http://wjh.harvard.edu/~jmitchel/writing/failed_science.htm">sees</a> things differently. For him, all failed replications contain “no meaningful evidentiary value” and “do not constitute scientific output”. I don’t doubt the sincerity of his beliefs, but I suspect that most scientists and nonscientists alike will find these assertions to be pretty bizarre. NHST isn’t the only thing causing the crisis in psychology, but it’s pretty clear that this is what happens when people get too immersed in it. </p>


    <div>
        <i class="fa fa-comment"></i> <a href="http://localhost:4000/2014/07/07/jason-mitchells-essay/#disqus_thread" data-disqus-identifier="273 http://filedrawer.wordpress.com/?p=273"></a>
        <span>&nbsp; &nbsp;</span>
        <span>
            <a class="twitter-share-button"
              href="https://twitter.com/share"
              data-url="http://localhost:4000/2014/07/07/jason-mitchells-essay/"
              data-via="Chris_Said"
              data-count="none"
              data-text="Jason Mitchell's essay">
            Tweet
            </a>
        </span>

    </div>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page5">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page3">Newer</a>
    
  
</div>

<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'csaid81'; // required: replace example with your forum shortname

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function () {
  var s = document.createElement('script'); s.async = true;
  s.type = 'text/javascript';
  s.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
  }());
</script>

    </div>

  </body>
</html>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>


<script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
